<!DOCTYPE html>
<html lang="en">

<head>
    <title>Arxiv Speech Papers</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <a href="https://github.com/Shayne-Ada/arxivpaper" style="text-decoration: none;">
                <div class="header-title">
                    <span class="header-title-preffix">Arxiv Speech Papers</span>
                </div>
            </a> 
            <div class="header-title">
                Arxiv Speech Papers
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Soundscapes: Leveraging Text-to-Audio Models for
  Environmental Sound Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Fabio Antonacci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past few years, text-to-audio models have emerged as a significant
advancement in automatic audio generation. Although they represent impressive
technological progress, the effectiveness of their use in the development of
audio applications remains uncertain. This paper aims to investigate these
aspects, specifically focusing on the task of classification of environmental
sounds. This study analyzes the performance of two different environmental
classification systems when data generated from text-to-audio models is used
for training. Two cases are considered: a) when the training dataset is
augmented by data coming from two different text-to-audio models; and b) when
the training dataset consists solely of synthetic audio generated. In both
cases, the performance of the classification task is tested on real data.
Results indicate that text-to-audio models are effective for dataset
augmentation, whereas the performance of the models drops when relying on only
generated audio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep functional multiple index models with an application to SER 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Saumard, Abir El Haj, Thibault Napoleon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Emotion Recognition (SER) plays a crucial role in advancing
human-computer interaction and speech processing capabilities. We introduce a
novel deep-learning architecture designed specifically for the functional data
model known as the multiple-index functional model. Our key innovation lies in
integrating adaptive basis layers and an automated data transformation search
within the deep learning framework. Simulations for this new model show good
performances. This allows us to extract features tailored for chunk-level SER,
based on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the
effectiveness of our approach on the benchmark IEMOCAP database, achieving good
performance compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of Deepfake Environmental Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafsa Ouajdi, Oussama Hadder, Modan Tailleur, Mathieu Lagrange, Laurie M. Heller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the ever-rising quality of deep generative models, it is increasingly
important to be able to discern whether the audio data at hand have been
recorded or synthesized. Although the detection of fake speech signals has been
studied extensively, this is not the case for the detection of fake
environmental audio.
  We propose a simple and efficient pipeline for detecting fake environmental
sounds based on the CLAP audio embedding. We evaluate this detector using audio
data from the 2023 DCASE challenge task on Foley sound synthesis.
  Our experiments show that fake sounds generated by 44 state-of-the-art
synthesizers can be detected on average with 98% accuracy. We show that using
an audio embedding learned on environmental audio is beneficial over a standard
VGGish one as it provides a 10% increase in detection performance. Informal
listening to Incorrect Negative examples demonstrates audible features of fake
sounds missed by the detector such as distortion and implausible background
noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaker Distance Estimation in Enclosures from Single-Channel Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Neri, Archontis Politis, Daniel Krause, Marco Carli, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distance estimation from audio plays a crucial role in various applications,
such as acoustic scene analysis, sound source localization, and room modeling.
Most studies predominantly center on employing a classification approach, where
distances are discretized into distinct categories, enabling smoother model
training and achieving higher accuracy but imposing restrictions on the
precision of the obtained sound source position. Towards this direction, in
this paper we propose a novel approach for continuous distance estimation from
audio signals using a convolutional recurrent neural network with an attention
module. The attention mechanism enables the model to focus on relevant temporal
and spectral features, enhancing its ability to capture fine-grained
distance-related information. To evaluate the effectiveness of our proposed
method, we conduct extensive experiments using audio recordings in controlled
environments with three levels of realism (synthetic room impulse response,
measured response with convolved speech, and real recordings) on four datasets
(our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23). Experimental
results show that the model achieves an absolute error of 0.11 meters in a
noiseless synthetic scenario. Moreover, the results showed an absolute error of
about 1.30 meters in the hybrid scenario. The algorithm's performance in the
real scenario, where unpredictable environmental factors and noise are
prevalent, yields an absolute error of approximately 0.50 meters. For
reproducible research purposes we make model, code, and synthetic datasets
available at https://github.com/michaelneri/audio-distance-estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE/ACM Transactions on Audio, Speech,
  and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlation of Fréchet Audio Distance With Human Perception of
  Environmental Audio Is Embedding Dependant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Modan Tailleur, Junwon Lee, Mathieu Lagrange, Keunwoo Choi, Laurie M. Heller, Keisuke Imoto, Yuki Okamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores whether considering alternative domain-specific
embeddings to calculate the Fr\'echet Audio Distance (FAD) metric can help the
FAD to correlate better with perceptual ratings of environmental sounds. We
used embeddings from VGGish, PANNs, MS-CLAP, L-CLAP, and MERT, which are
tailored for either music or environmental sound evaluation. The FAD scores
were calculated for sounds from the DCASE 2023 Task 7 dataset. Using perceptual
data from the same task, we find that PANNs-WGM-LogMel produces the best
correlation between FAD scores and perceptual ratings of both audio quality and
perceived fit with a Spearman correlation higher than 0.5. We also find that
music-specific embeddings resulted in significantly lower results.
Interestingly, VGGish, the embedding used for the original Fr\'echet
calculation, yielded a correlation below 0.1. These results underscore the
critical importance of the choice of embedding for the FAD metric design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Visually Localize Sound Sources from Mixtures without Prior
  Source Knowledge <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of the multi-sound source localization task is to localize sound
sources from the mixture individually. While recent multi-sound source
localization methods have shown improved performance, they face challenges due
to their reliance on prior information about the number of objects to be
separated. In this paper, to overcome this limitation, we present a novel
multi-sound source localization method that can perform localization without
prior knowledge of the number of sound sources. To achieve this goal, we
propose an iterative object identification (IOI) module, which can recognize
sound-making objects in an iterative manner. After finding the regions of
sound-making objects, we devise object similarity-aware clustering (OSC) loss
to guide the IOI module to effectively combine regions of the same object but
also distinguish between different objects and backgrounds. It enables our
method to perform accurate localization of sound-making objects without any
prior knowledge. Extensive experimental results on the MUSIC and VGGSound
benchmarks show the significant performance improvements of the proposed method
over the existing methods for both single and multi-source. Our code is
available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infrastructure-less Localization from Indoor Environmental Sounds Based
  on Spectral Decomposition and Spatial Likelihood Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoki Ogiso, Yoshiaki Bando, Takeshi Kurata, Takashi Okuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human and/or asset tracking using an attached sensor units helps understand
their activities. Most common indoor localization methods for human tracking
technologies require expensive infrastructures, deployment and maintenance. To
overcome this problem, environmental sounds have been used for
infrastructure-free localization. While they achieve room-level classification,
they suffer from two problems: low signal-to-noise-ratio (SNR) condition and
non-uniqueness of sound over the coverage area. A microphone localization
method was proposed using supervised spectral decomposition and spatial
likelihood to solve these problems. The proposed method was evaluated with
actual recordings in an experimental room with a size of 12 x 30 m. The results
showed that the proposed method with supervised NMF was robust under low-SNR
condition compared to a simple feature (mel frequency cepstrum coefficient:
MFCC). Additionally, the proposed method could be easily integrated with prior
distribution, which is available from other Bayesian localizations. The
proposed method can be used to evaluate the spatial likelihood from
environmental sounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, accepted to IEEE/SICE SII 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Latency Neural Speech Phase Prediction based on Parallel Estimation
  Architecture and Anti-Wrapping Losses for Speech <span class="highlight-title">Generation</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Ai, Zhen-Hua Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel neural speech phase prediction model which
predicts wrapped phase spectra directly from amplitude spectra. The proposed
model is a cascade of a residual convolutional network and a parallel
estimation architecture. The parallel estimation architecture is a core module
for direct wrapped phase prediction. This architecture consists of two parallel
linear convolutional layers and a phase calculation formula, imitating the
process of calculating the phase spectra from the real and imaginary parts of
complex spectra and strictly restricting the predicted phase values to the
principal value interval. To avoid the error expansion issue caused by phase
wrapping, we design anti-wrapping training losses defined between the predicted
wrapped phase spectra and natural ones by activating the instantaneous phase
error, group delay error and instantaneous angular frequency error using an
anti-wrapping function. We mathematically demonstrate that the anti-wrapping
function should possess three properties, namely parity, periodicity and
monotonicity. We also achieve low-latency streamable phase prediction by
combining causal convolutions and knowledge distillation training strategies.
For both analysis-synthesis and specific speech generation tasks, experimental
results show that our proposed neural speech phase prediction model outperforms
the iterative phase estimation algorithms and neural network-based phase
prediction methods in terms of phase prediction precision, efficiency and
robustness. Compared with HiFi-GAN-based waveform reconstruction method, our
proposed model also shows outstanding efficiency advantages while ensuring the
quality of synthesized speech. To the best of our knowledge, we are the first
to directly predict speech phase spectra from amplitude spectra only via neural
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Audio, Speech and Language
  Processing. arXiv admin note: substantial text overlap with arXiv:2211.15974</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy enhancement method for speech emotion recognition from
  spectrogram using temporal frequency correlation and positional information
  learning through knowledge transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeong-Yoon Kim, Seung-Ho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a method to improve the accuracy of speech emotion
recognition (SER) by using vision transformer (ViT) to attend to the
correlation of frequency (y-axis) with time (x-axis) in spectrogram and
transferring positional information between ViT through knowledge transfer. The
proposed method has the following originality i) We use vertically segmented
patches of log-Mel spectrogram to analyze the correlation of frequencies over
time. This type of patch allows us to correlate the most relevant frequencies
for a particular emotion with the time they were uttered. ii) We propose the
use of image coordinate encoding, an absolute positional encoding suitable for
ViT. By normalizing the x, y coordinates of the image to -1 to 1 and
concatenating them to the image, we can effectively provide valid absolute
positional information for ViT. iii) Through feature map matching, the locality
and location information of the teacher network is effectively transmitted to
the student network. Teacher network is a ViT that contains locality of
convolutional stem and absolute position information through image coordinate
encoding, and student network is a structure that lacks positional encoding in
the basic ViT structure. In feature map matching stage, we train through the
mean absolute error (L1 loss) to minimize the difference between the feature
maps of the two networks. To validate the proposed method, three emotion
datasets (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into
log-Mel spectrograms for comparison experiments. The experimental results show
that the proposed method significantly outperforms the state-of-the-art methods
in terms of weighted accuracy while requiring significantly fewer floating
point operations (FLOPs). Overall, the proposed method offers an promising
solution for SER by providing improved efficiency and performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ As Good As A Coin Toss: Human detection of AI-generated images, videos,
  audio, and audiovisual stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Cooke, Abigail Edwards, Sophia Barkoff, Kathryn Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As synthetic media becomes progressively more realistic and barriers to using
it continue to lower, the technology has been increasingly utilized for
malicious purposes, from financial fraud to nonconsensual pornography. Today,
the principal defense against being misled by synthetic media relies on the
ability of the human observer to visually and auditorily discern between real
and fake. However, it remains unclear just how vulnerable people actually are
to deceptive synthetic media in the course of their day to day lives. We
conducted a perceptual study with 1276 participants to assess how accurate
people were at distinguishing synthetic images, audio only, video only, and
audiovisual stimuli from authentic. To reflect the circumstances under which
people would likely encounter synthetic media in the wild, testing conditions
and stimuli emulated a typical online platform, while all synthetic media used
in the survey was sourced from publicly accessible generative AI technology.
  We find that overall, participants struggled to meaningfully discern between
synthetic and authentic content. We also find that detection performance
worsens when the stimuli contains synthetic content as compared to authentic
content, images featuring human faces as compared to non face objects, a single
modality as compared to multimodal stimuli, mixed authenticity as compared to
being fully synthetic for audiovisual stimuli, and features foreign languages
as compared to languages the observer is fluent in. Finally, we also find that
prior knowledge of synthetic media does not meaningfully impact their detection
performance. Collectively, these results indicate that people are highly
susceptible to being tricked by synthetic media in their daily lives and that
human perceptual detection capabilities can no longer be relied upon as an
effective counterdefense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For study pre-registration, see https://osf.io/fnhr3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lodge: A Coarse to Fine <span class="highlight-title">Diffusion</span> Network for Long Dance <span class="highlight-title">Generation</span>
  Guided by the Characteristic Dance Primitives <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lodge, a network capable of generating extremely long dance
sequences conditioned on given music. We design Lodge as a two-stage coarse to
fine diffusion architecture, and propose the characteristic dance primitives
that possess significant expressiveness as intermediate representations between
two diffusion models. The first stage is global diffusion, which focuses on
comprehending the coarse-level music-dance correlation and production
characteristic dance primitives. In contrast, the second-stage is the local
diffusion, which parallelly generates detailed motion sequences under the
guidance of the dance primitives and choreographic rules. In addition, we
propose a Foot Refine Block to optimize the contact between the feet and the
ground, enhancing the physical realism of the motion. Our approach can
parallelly generate dance sequences of extremely long length, striking a
balance between global choreographic patterns and local motion quality and
expressiveness. Extensive experiments validate the efficacy of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024, Project page:
  https://li-ronghui.github.io/lodge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Room Transfer Function Reconstruction Using Complex-valued Neural
  Networks and Irregularly Distributed Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several important real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of measurements at scattered points in the room. In this paper, we employ
complex-valued neural networks to estimate room transfer functions in the
frequency range of the first room resonances, using a few irregularly
distributed microphones. To the best of our knowledge, this is the first time
that complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex-valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
kernel-based signal processing approach for sound field reconstruction, showing
that the proposed technique exhibits relevant advantages in terms of phase
accuracy and overall quality of the reconstructed sound field. For informative
purposes, we also compare the model with a similarly-structured data-driven
approach that, however, applies a real-valued neural network to reconstruct
only the magnitude of the sound field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Entrainment in Spontaneous Code-switched Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasmita Bhattacharya, Siying Ding, Alayna Nguyen, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that speakers who entrain to one another have more
successful conversations than those who do not. Previous research has shown
that interlocutors entrain on linguistic features in both written and spoken
monolingual domains. More recent work on code-switched communication has also
shown preliminary evidence of entrainment on certain aspects of code-switching
(CSW). However, such studies of entrainment in code-switched domains have been
extremely few and restricted to human-machine textual interactions. Our work
studies code-switched spontaneous speech between humans, finding that (1)
patterns of written and spoken entrainment in monolingual settings largely
generalize to code-switched settings, and (2) some patterns of entrainment on
code-switching in dialogue agent-generated text generalize to spontaneous
code-switched speech. Our findings give rise to important implications for the
potentially "universal" nature of entrainment as a communication phenomenon,
and potential applications in inclusive and interactive speech technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Edits: camera-ready manuscript for NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Soundscapes: Leveraging Text-to-Audio Models for
  Environmental Sound Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Fabio Antonacci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past few years, text-to-audio models have emerged as a significant
advancement in automatic audio generation. Although they represent impressive
technological progress, the effectiveness of their use in the development of
audio applications remains uncertain. This paper aims to investigate these
aspects, specifically focusing on the task of classification of environmental
sounds. This study analyzes the performance of two different environmental
classification systems when data generated from text-to-audio models is used
for training. Two cases are considered: a) when the training dataset is
augmented by data coming from two different text-to-audio models; and b) when
the training dataset consists solely of synthetic audio generated. In both
cases, the performance of the classification task is tested on real data.
Results indicate that text-to-audio models are effective for dataset
augmentation, whereas the performance of the models drops when relying on only
generated audio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep functional multiple index models with an application to SER 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Saumard, Abir El Haj, Thibault Napoleon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Emotion Recognition (SER) plays a crucial role in advancing
human-computer interaction and speech processing capabilities. We introduce a
novel deep-learning architecture designed specifically for the functional data
model known as the multiple-index functional model. Our key innovation lies in
integrating adaptive basis layers and an automated data transformation search
within the deep learning framework. Simulations for this new model show good
performances. This allows us to extract features tailored for chunk-level SER,
based on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the
effectiveness of our approach on the benchmark IEMOCAP database, achieving good
performance compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of Deepfake Environmental Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafsa Ouajdi, Oussama Hadder, Modan Tailleur, Mathieu Lagrange, Laurie M. Heller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the ever-rising quality of deep generative models, it is increasingly
important to be able to discern whether the audio data at hand have been
recorded or synthesized. Although the detection of fake speech signals has been
studied extensively, this is not the case for the detection of fake
environmental audio.
  We propose a simple and efficient pipeline for detecting fake environmental
sounds based on the CLAP audio embedding. We evaluate this detector using audio
data from the 2023 DCASE challenge task on Foley sound synthesis.
  Our experiments show that fake sounds generated by 44 state-of-the-art
synthesizers can be detected on average with 98% accuracy. We show that using
an audio embedding learned on environmental audio is beneficial over a standard
VGGish one as it provides a 10% increase in detection performance. Informal
listening to Incorrect Negative examples demonstrates audible features of fake
sounds missed by the detector such as distortion and implausible background
noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaker Distance Estimation in Enclosures from Single-Channel Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Neri, Archontis Politis, Daniel Krause, Marco Carli, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distance estimation from audio plays a crucial role in various applications,
such as acoustic scene analysis, sound source localization, and room modeling.
Most studies predominantly center on employing a classification approach, where
distances are discretized into distinct categories, enabling smoother model
training and achieving higher accuracy but imposing restrictions on the
precision of the obtained sound source position. Towards this direction, in
this paper we propose a novel approach for continuous distance estimation from
audio signals using a convolutional recurrent neural network with an attention
module. The attention mechanism enables the model to focus on relevant temporal
and spectral features, enhancing its ability to capture fine-grained
distance-related information. To evaluate the effectiveness of our proposed
method, we conduct extensive experiments using audio recordings in controlled
environments with three levels of realism (synthetic room impulse response,
measured response with convolved speech, and real recordings) on four datasets
(our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23). Experimental
results show that the model achieves an absolute error of 0.11 meters in a
noiseless synthetic scenario. Moreover, the results showed an absolute error of
about 1.30 meters in the hybrid scenario. The algorithm's performance in the
real scenario, where unpredictable environmental factors and noise are
prevalent, yields an absolute error of approximately 0.50 meters. For
reproducible research purposes we make model, code, and synthetic datasets
available at https://github.com/michaelneri/audio-distance-estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE/ACM Transactions on Audio, Speech,
  and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlation of Fréchet Audio Distance With Human Perception of
  Environmental Audio Is Embedding Dependant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Modan Tailleur, Junwon Lee, Mathieu Lagrange, Keunwoo Choi, Laurie M. Heller, Keisuke Imoto, Yuki Okamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores whether considering alternative domain-specific
embeddings to calculate the Fr\'echet Audio Distance (FAD) metric can help the
FAD to correlate better with perceptual ratings of environmental sounds. We
used embeddings from VGGish, PANNs, MS-CLAP, L-CLAP, and MERT, which are
tailored for either music or environmental sound evaluation. The FAD scores
were calculated for sounds from the DCASE 2023 Task 7 dataset. Using perceptual
data from the same task, we find that PANNs-WGM-LogMel produces the best
correlation between FAD scores and perceptual ratings of both audio quality and
perceived fit with a Spearman correlation higher than 0.5. We also find that
music-specific embeddings resulted in significantly lower results.
Interestingly, VGGish, the embedding used for the original Fr\'echet
calculation, yielded a correlation below 0.1. These results underscore the
critical importance of the choice of embedding for the FAD metric design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Visually Localize Sound Sources from Mixtures without Prior
  Source Knowledge <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of the multi-sound source localization task is to localize sound
sources from the mixture individually. While recent multi-sound source
localization methods have shown improved performance, they face challenges due
to their reliance on prior information about the number of objects to be
separated. In this paper, to overcome this limitation, we present a novel
multi-sound source localization method that can perform localization without
prior knowledge of the number of sound sources. To achieve this goal, we
propose an iterative object identification (IOI) module, which can recognize
sound-making objects in an iterative manner. After finding the regions of
sound-making objects, we devise object similarity-aware clustering (OSC) loss
to guide the IOI module to effectively combine regions of the same object but
also distinguish between different objects and backgrounds. It enables our
method to perform accurate localization of sound-making objects without any
prior knowledge. Extensive experimental results on the MUSIC and VGGSound
benchmarks show the significant performance improvements of the proposed method
over the existing methods for both single and multi-source. Our code is
available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infrastructure-less Localization from Indoor Environmental Sounds Based
  on Spectral Decomposition and Spatial Likelihood Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoki Ogiso, Yoshiaki Bando, Takeshi Kurata, Takashi Okuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human and/or asset tracking using an attached sensor units helps understand
their activities. Most common indoor localization methods for human tracking
technologies require expensive infrastructures, deployment and maintenance. To
overcome this problem, environmental sounds have been used for
infrastructure-free localization. While they achieve room-level classification,
they suffer from two problems: low signal-to-noise-ratio (SNR) condition and
non-uniqueness of sound over the coverage area. A microphone localization
method was proposed using supervised spectral decomposition and spatial
likelihood to solve these problems. The proposed method was evaluated with
actual recordings in an experimental room with a size of 12 x 30 m. The results
showed that the proposed method with supervised NMF was robust under low-SNR
condition compared to a simple feature (mel frequency cepstrum coefficient:
MFCC). Additionally, the proposed method could be easily integrated with prior
distribution, which is available from other Bayesian localizations. The
proposed method can be used to evaluate the spatial likelihood from
environmental sounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, accepted to IEEE/SICE SII 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Latency Neural Speech Phase Prediction based on Parallel Estimation
  Architecture and Anti-Wrapping Losses for Speech <span class="highlight-title">Generation</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Ai, Zhen-Hua Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel neural speech phase prediction model which
predicts wrapped phase spectra directly from amplitude spectra. The proposed
model is a cascade of a residual convolutional network and a parallel
estimation architecture. The parallel estimation architecture is a core module
for direct wrapped phase prediction. This architecture consists of two parallel
linear convolutional layers and a phase calculation formula, imitating the
process of calculating the phase spectra from the real and imaginary parts of
complex spectra and strictly restricting the predicted phase values to the
principal value interval. To avoid the error expansion issue caused by phase
wrapping, we design anti-wrapping training losses defined between the predicted
wrapped phase spectra and natural ones by activating the instantaneous phase
error, group delay error and instantaneous angular frequency error using an
anti-wrapping function. We mathematically demonstrate that the anti-wrapping
function should possess three properties, namely parity, periodicity and
monotonicity. We also achieve low-latency streamable phase prediction by
combining causal convolutions and knowledge distillation training strategies.
For both analysis-synthesis and specific speech generation tasks, experimental
results show that our proposed neural speech phase prediction model outperforms
the iterative phase estimation algorithms and neural network-based phase
prediction methods in terms of phase prediction precision, efficiency and
robustness. Compared with HiFi-GAN-based waveform reconstruction method, our
proposed model also shows outstanding efficiency advantages while ensuring the
quality of synthesized speech. To the best of our knowledge, we are the first
to directly predict speech phase spectra from amplitude spectra only via neural
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Audio, Speech and Language
  Processing. arXiv admin note: substantial text overlap with arXiv:2211.15974</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy enhancement method for speech emotion recognition from
  spectrogram using temporal frequency correlation and positional information
  learning through knowledge transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeong-Yoon Kim, Seung-Ho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a method to improve the accuracy of speech emotion
recognition (SER) by using vision transformer (ViT) to attend to the
correlation of frequency (y-axis) with time (x-axis) in spectrogram and
transferring positional information between ViT through knowledge transfer. The
proposed method has the following originality i) We use vertically segmented
patches of log-Mel spectrogram to analyze the correlation of frequencies over
time. This type of patch allows us to correlate the most relevant frequencies
for a particular emotion with the time they were uttered. ii) We propose the
use of image coordinate encoding, an absolute positional encoding suitable for
ViT. By normalizing the x, y coordinates of the image to -1 to 1 and
concatenating them to the image, we can effectively provide valid absolute
positional information for ViT. iii) Through feature map matching, the locality
and location information of the teacher network is effectively transmitted to
the student network. Teacher network is a ViT that contains locality of
convolutional stem and absolute position information through image coordinate
encoding, and student network is a structure that lacks positional encoding in
the basic ViT structure. In feature map matching stage, we train through the
mean absolute error (L1 loss) to minimize the difference between the feature
maps of the two networks. To validate the proposed method, three emotion
datasets (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into
log-Mel spectrograms for comparison experiments. The experimental results show
that the proposed method significantly outperforms the state-of-the-art methods
in terms of weighted accuracy while requiring significantly fewer floating
point operations (FLOPs). Overall, the proposed method offers an promising
solution for SER by providing improved efficiency and performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lodge: A Coarse to Fine <span class="highlight-title">Diffusion</span> Network for Long Dance <span class="highlight-title">Generation</span>
  Guided by the Characteristic Dance Primitives <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lodge, a network capable of generating extremely long dance
sequences conditioned on given music. We design Lodge as a two-stage coarse to
fine diffusion architecture, and propose the characteristic dance primitives
that possess significant expressiveness as intermediate representations between
two diffusion models. The first stage is global diffusion, which focuses on
comprehending the coarse-level music-dance correlation and production
characteristic dance primitives. In contrast, the second-stage is the local
diffusion, which parallelly generates detailed motion sequences under the
guidance of the dance primitives and choreographic rules. In addition, we
propose a Foot Refine Block to optimize the contact between the feet and the
ground, enhancing the physical realism of the motion. Our approach can
parallelly generate dance sequences of extremely long length, striking a
balance between global choreographic patterns and local motion quality and
expressiveness. Extensive experiments validate the efficacy of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024, Project page:
  https://li-ronghui.github.io/lodge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Room Transfer Function Reconstruction Using Complex-valued Neural
  Networks and Irregularly Distributed Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several important real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of measurements at scattered points in the room. In this paper, we employ
complex-valued neural networks to estimate room transfer functions in the
frequency range of the first room resonances, using a few irregularly
distributed microphones. To the best of our knowledge, this is the first time
that complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex-valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
kernel-based signal processing approach for sound field reconstruction, showing
that the proposed technique exhibits relevant advantages in terms of phase
accuracy and overall quality of the reconstructed sound field. For informative
purposes, we also compare the model with a similarly-structured data-driven
approach that, however, applies a real-valued neural network to reconstruct
only the magnitude of the sound field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Entrainment in Spontaneous Code-switched Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasmita Bhattacharya, Siying Ding, Alayna Nguyen, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that speakers who entrain to one another have more
successful conversations than those who do not. Previous research has shown
that interlocutors entrain on linguistic features in both written and spoken
monolingual domains. More recent work on code-switched communication has also
shown preliminary evidence of entrainment on certain aspects of code-switching
(CSW). However, such studies of entrainment in code-switched domains have been
extremely few and restricted to human-machine textual interactions. Our work
studies code-switched spontaneous speech between humans, finding that (1)
patterns of written and spoken entrainment in monolingual settings largely
generalize to code-switched settings, and (2) some patterns of entrainment on
code-switching in dialogue agent-generated text generalize to spontaneous
code-switched speech. Our findings give rise to important implications for the
potentially "universal" nature of entrainment as a communication phenomenon,
and potential applications in inclusive and interactive speech technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Edits: camera-ready manuscript for NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ As Good As A Coin Toss: Human detection of AI-generated images, videos,
  audio, and audiovisual stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Cooke, Abigail Edwards, Sophia Barkoff, Kathryn Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As synthetic media becomes progressively more realistic and barriers to using
it continue to lower, the technology has been increasingly utilized for
malicious purposes, from financial fraud to nonconsensual pornography. Today,
the principal defense against being misled by synthetic media relies on the
ability of the human observer to visually and auditorily discern between real
and fake. However, it remains unclear just how vulnerable people actually are
to deceptive synthetic media in the course of their day to day lives. We
conducted a perceptual study with 1276 participants to assess how accurate
people were at distinguishing synthetic images, audio only, video only, and
audiovisual stimuli from authentic. To reflect the circumstances under which
people would likely encounter synthetic media in the wild, testing conditions
and stimuli emulated a typical online platform, while all synthetic media used
in the survey was sourced from publicly accessible generative AI technology.
  We find that overall, participants struggled to meaningfully discern between
synthetic and authentic content. We also find that detection performance
worsens when the stimuli contains synthetic content as compared to authentic
content, images featuring human faces as compared to non face objects, a single
modality as compared to multimodal stimuli, mixed authenticity as compared to
being fully synthetic for audiovisual stimuli, and features foreign languages
as compared to languages the observer is fluent in. Finally, we also find that
prior knowledge of synthetic media does not meaningfully impact their detection
performance. Collectively, these results indicate that people are highly
susceptible to being tricked by synthetic media in their daily lives and that
human perceptual detection capabilities can no longer be relied upon as an
effective counterdefense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For study pre-registration, see https://osf.io/fnhr3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation
  with Unified Audio-Visual Speech Representation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. Demo page is available on
https://choijeongsoo.github.io/av2av.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed collaborative anomalous sound detection by embedding sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Dohi, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To develop a machine sound monitoring system, a method for detecting
anomalous sound is proposed. In this paper, we explore a method for multiple
clients to collaboratively learn an anomalous sound detection model while
keeping their raw data private from each other. In the context of industrial
machine anomalous sound detection, each client possesses data from different
machines or different operational states, making it challenging to learn
through federated learning or split learning. In our proposed method, each
client calculates embeddings using a common pre-trained model developed for
sound data classification, and these calculated embeddings are aggregated on
the server to perform anomalous sound detection through outlier exposure.
Experiments showed that our proposed method improves the AUC of anomalous sound
detection by an average of 6.8%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Generative Adversarial Network-Based Vocoder with Limited Data
  Using Augmentation-Conditional Discriminator <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generative adversarial network (GAN)-based vocoder trained with an
adversarial discriminator is commonly used for speech synthesis because of its
fast, lightweight, and high-quality characteristics. However, this data-driven
model requires a large amount of training data incurring high data-collection
costs. This fact motivates us to train a GAN-based vocoder on limited data. A
promising solution is to augment the training data to avoid overfitting.
However, a standard discriminator is unconditional and insensitive to
distributional changes caused by data augmentation. Thus, augmented speech
(which can be extraordinary) may be considered real speech. To address this
issue, we propose an augmentation-conditional discriminator (AugCondD) that
receives the augmentation state as input in addition to speech, thereby
assessing the input speech according to the augmentation state, without
inhibiting the learning of the original non-augmented distribution.
Experimental results indicate that AugCondD improves speech quality under
limited data conditions while achieving comparable speech quality under
sufficient data conditions. Audio samples are available at
https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoiceCraft: <span class="highlight-title">Zero-Shot</span> Speech Editing and <span class="highlight-title">Text-to-Speech</span> in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puyuan Peng, Po-Yao Huang, Daniel Li, Abdelrahman Mohamed, David Harwath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VoiceCraft, a token infilling neural codec language model, that
achieves state-of-the-art performance on both speech editing and zero-shot
text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft
employs a Transformer decoder architecture and introduces a token rearrangement
procedure that combines causal masking and delayed stacking to enable
generation within an existing sequence. On speech editing tasks, VoiceCraft
produces edited speech that is nearly indistinguishable from unedited
recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS,
our model outperforms prior SotA models including VALLE and the popular
commercial model XTTS-v2. Crucially, the models are evaluated on challenging
and realistic datasets, that consist of diverse accents, speaking styles,
recording conditions, and background noise and music, and our model performs
consistently well compared to other models and real recordings. In particular,
for speech editing evaluation, we introduce a high quality, challenging, and
realistic dataset named RealEdit. We encourage readers to listen to the demos
at https://jasonppy.github.io/VoiceCraft_web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data, code, and model weights are available at
  https://github.com/jasonppy/VoiceCraft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial
  Network <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Shibuya, Yuhta Takida, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative adversarial network (GAN)-based vocoders have been intensively
studied because they can synthesize high-fidelity audio waveforms faster than
real-time. However, it has been reported that most GANs fail to obtain the
optimal projection for discriminating between real and fake data in the feature
space. In the literature, it has been demonstrated that slicing adversarial
network (SAN), an improved GAN training framework that can find the optimal
projection, is effective in the image generation task. In this paper, we
investigate the effectiveness of SAN in the vocoding task. For this purpose, we
propose a scheme to modify least-squares GAN, which most GAN-based vocoders
adopt, so that their loss functions satisfy the requirements of SAN. Through
our experiments, we demonstrate that SAN can improve the performance of
GAN-based vocoders, including BigVGAN, with small modifications. Our code is
available at https://github.com/sony/bigvsan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024. Equation (5) in the previous version is
  wrong. We modified it</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A unified front-end framework for English <span class="highlight-title">text-to-speech</span> <span class="highlight-title">synthesis</span> <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10666v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10666v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Ying, Chen Li, Yu Dong, Qiuqiang Kong, Qiao Tian, Yuanyuan Huo, Yuxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The front-end is a critical component of English text-to-speech (TTS)
systems, responsible for extracting linguistic features that are essential for
a text-to-speech model to synthesize speech, such as prosodies and phonemes.
The English TTS front-end typically consists of a text normalization (TN)
module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme
(G2P) module. However, current research on the English TTS front-end focuses
solely on individual modules, neglecting the interdependence between them and
resulting in sub-optimal performance for each module. Therefore, this paper
proposes a unified front-end framework that captures the dependencies among the
English TTS front-end modules. Extensive experiments have demonstrated that the
proposed method achieves state-of-the-art (SOTA) performance in all modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pre-train</span>ing for Speech with Flow Matching <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, Wei-Ning Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have gained more and more attention in recent years for
their remarkable success in tasks that required estimating and sampling data
distribution to generate high-fidelity synthetic data. In speech,
text-to-speech synthesis and neural vocoder are good examples where generative
models have shined. While generative models have been applied to different
applications in speech, there exists no general-purpose generative model that
models speech directly. In this work, we take a step toward this direction by
showing a single pre-trained generative model can be adapted to different
downstream tasks with strong performance. Specifically, we pre-trained a
generative model, named SpeechFlow, on 60k hours of untranscribed speech with
Flow Matching and masked conditions. Experiment results show the pre-trained
generative model can be fine-tuned with task-specific data to match or surpass
existing expert models on speech enhancement, separation, and synthesis. Our
work suggested a foundational model for generation tasks in speech can be built
with generative pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed collaborative anomalous sound detection by embedding sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Dohi, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To develop a machine sound monitoring system, a method for detecting
anomalous sound is proposed. In this paper, we explore a method for multiple
clients to collaboratively learn an anomalous sound detection model while
keeping their raw data private from each other. In the context of industrial
machine anomalous sound detection, each client possesses data from different
machines or different operational states, making it challenging to learn
through federated learning or split learning. In our proposed method, each
client calculates embeddings using a common pre-trained model developed for
sound data classification, and these calculated embeddings are aggregated on
the server to perform anomalous sound detection through outlier exposure.
Experiments showed that our proposed method improves the AUC of anomalous sound
detection by an average of 6.8%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Generative Adversarial Network-Based Vocoder with Limited Data
  Using Augmentation-Conditional Discriminator <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generative adversarial network (GAN)-based vocoder trained with an
adversarial discriminator is commonly used for speech synthesis because of its
fast, lightweight, and high-quality characteristics. However, this data-driven
model requires a large amount of training data incurring high data-collection
costs. This fact motivates us to train a GAN-based vocoder on limited data. A
promising solution is to augment the training data to avoid overfitting.
However, a standard discriminator is unconditional and insensitive to
distributional changes caused by data augmentation. Thus, augmented speech
(which can be extraordinary) may be considered real speech. To address this
issue, we propose an augmentation-conditional discriminator (AugCondD) that
receives the augmentation state as input in addition to speech, thereby
assessing the input speech according to the augmentation state, without
inhibiting the learning of the original non-augmented distribution.
Experimental results indicate that AugCondD improves speech quality under
limited data conditions while achieving comparable speech quality under
sufficient data conditions. Audio samples are available at
https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoiceCraft: <span class="highlight-title">Zero-Shot</span> Speech Editing and <span class="highlight-title">Text-to-Speech</span> in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puyuan Peng, Po-Yao Huang, Daniel Li, Abdelrahman Mohamed, David Harwath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VoiceCraft, a token infilling neural codec language model, that
achieves state-of-the-art performance on both speech editing and zero-shot
text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft
employs a Transformer decoder architecture and introduces a token rearrangement
procedure that combines causal masking and delayed stacking to enable
generation within an existing sequence. On speech editing tasks, VoiceCraft
produces edited speech that is nearly indistinguishable from unedited
recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS,
our model outperforms prior SotA models including VALLE and the popular
commercial model XTTS-v2. Crucially, the models are evaluated on challenging
and realistic datasets, that consist of diverse accents, speaking styles,
recording conditions, and background noise and music, and our model performs
consistently well compared to other models and real recordings. In particular,
for speech editing evaluation, we introduce a high quality, challenging, and
realistic dataset named RealEdit. We encourage readers to listen to the demos
at https://jasonppy.github.io/VoiceCraft_web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data, code, and model weights are available at
  https://github.com/jasonppy/VoiceCraft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoding of lexical tone in <span class="highlight-title">self-supervised</span> models of spoken language <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaofei Shen, Michaela Watkins, Afra Alishahi, Arianna Bisazza, Grzegorz Chrupała
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability research has shown that self-supervised Spoken Language
Models (SLMs) encode a wide variety of features in human speech from the
acoustic, phonetic, phonological, syntactic and semantic levels, to speaker
characteristics. The bulk of prior research on representations of phonology has
focused on segmental features such as phonemes; the encoding of suprasegmental
phonology (such as tone and stress patterns) in SLMs is not yet well
understood. Tone is a suprasegmental feature that is present in more than half
of the world's languages. This paper aims to analyze the tone encoding
capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show
that SLMs encode lexical tone to a significant degree even when they are
trained on data from non-tonal languages. We further find that SLMs behave
similarly to native and non-native human participants in tone and consonant
perception studies, but they do not follow the same developmental trajectory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial
  Network <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Shibuya, Yuhta Takida, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative adversarial network (GAN)-based vocoders have been intensively
studied because they can synthesize high-fidelity audio waveforms faster than
real-time. However, it has been reported that most GANs fail to obtain the
optimal projection for discriminating between real and fake data in the feature
space. In the literature, it has been demonstrated that slicing adversarial
network (SAN), an improved GAN training framework that can find the optimal
projection, is effective in the image generation task. In this paper, we
investigate the effectiveness of SAN in the vocoding task. For this purpose, we
propose a scheme to modify least-squares GAN, which most GAN-based vocoders
adopt, so that their loss functions satisfy the requirements of SAN. Through
our experiments, we demonstrate that SAN can improve the performance of
GAN-based vocoders, including BigVGAN, with small modifications. Our code is
available at https://github.com/sony/bigvsan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024. Equation (5) in the previous version is
  wrong. We modified it</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A unified front-end framework for English <span class="highlight-title">text-to-speech</span> <span class="highlight-title">synthesis</span> <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10666v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10666v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Ying, Chen Li, Yu Dong, Qiuqiang Kong, Qiao Tian, Yuanyuan Huo, Yuxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The front-end is a critical component of English text-to-speech (TTS)
systems, responsible for extracting linguistic features that are essential for
a text-to-speech model to synthesize speech, such as prosodies and phonemes.
The English TTS front-end typically consists of a text normalization (TN)
module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme
(G2P) module. However, current research on the English TTS front-end focuses
solely on individual modules, neglecting the interdependence between them and
resulting in sub-optimal performance for each module. Therefore, this paper
proposes a unified front-end framework that captures the dependencies among the
English TTS front-end modules. Extensive experiments have demonstrated that the
proposed method achieves state-of-the-art (SOTA) performance in all modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pre-train</span>ing for Speech with Flow Matching <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, Wei-Ning Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have gained more and more attention in recent years for
their remarkable success in tasks that required estimating and sampling data
distribution to generate high-fidelity synthetic data. In speech,
text-to-speech synthesis and neural vocoder are good examples where generative
models have shined. While generative models have been applied to different
applications in speech, there exists no general-purpose generative model that
models speech directly. In this work, we take a step toward this direction by
showing a single pre-trained generative model can be adapted to different
downstream tasks with strong performance. Specifically, we pre-trained a
generative model, named SpeechFlow, on 60k hours of untranscribed speech with
Flow Matching and masked conditions. Experiment results show the pre-trained
generative model can be fine-tuned with task-specific data to match or surpass
existing expert models on speech enhancement, separation, and synthesis. Our
work suggested a foundational model for generation tasks in speech can be built
with generative pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Analog Dynamic Range Compressors using Deep Learning and
  State-space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhi Yin, Gang Cheng, Christian J. Steinmetz, Ruibin Yuan, Richard M. Stern, Roger B. Dannenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a novel approach for developing realistic digital models of
dynamic range compressors for digital audio production by analyzing their
analog prototypes. While realistic digital dynamic compressors are potentially
useful for many applications, the design process is challenging because the
compressors operate nonlinearly over long time scales. Our approach is based on
the structured state space sequence model (S4), as implementing the state-space
model (SSM) has proven to be efficient at learning long-range dependencies and
is promising for modeling dynamic range compressors. We present in this paper a
deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic
range compressor. The model is causal, executes efficiently in real time, and
achieves roughly the same quality as previous deep-learning models but with
fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target Speech Extraction with <span class="highlight-title">Pre-train</span>ed AV-Hu<span class="highlight-title">BERT</span> and Mask-And-Recover
  Strategy <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wu, Xueyuan Chen, Xixin Wu, Haizhou Li, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual target speech extraction (AV-TSE) is one of the enabling
technologies in robotics and many audio-visual applications. One of the
challenges of AV-TSE is how to effectively utilize audio-visual synchronization
information in the process. AV-HuBERT can be a useful pre-trained model for
lip-reading, which has not been adopted by AV-TSE. In this paper, we would like
to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system.
We have good reasons to expect an improved performance. To benefit from the
inter and intra-modality correlations, we also propose a novel Mask-And-Recover
(MAR) strategy for self-supervised learning. The experimental results on the
VoxCeleb2 dataset show that our proposed model outperforms the baselines both
in terms of subjective and objective metrics, suggesting that the pre-trained
AV-HuBERT model provides more informative visual cues for target speech
extraction. Furthermore, through a comparative study, we confirm that the
proposed Mask-And-Recover strategy is significantly effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Analysis of Quality of Conventional Beamforming for Phased
  Microphone Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dheepak Khatri, Kenneth Granlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A theoretical study is performed to analyze the directional response of
different types of microphone array designs. 1-D (linear) and 2-D (planar)
microphone array types are considered, and the delay and sum beamforming and
conventional beamforming techniques are employed to localize the sound source.
A non-dimensional parameter, G, is characterized to simplify and standardize
the rejection performance of both 1-D and 2-D microphone arrays as a function
of array geometry and sound source parameters. This parameter G is then used to
determine an improved design of a 2-D microphone array for far-field sound
localization. One such design, termed the Equi-area array is introduced and
analyzed in detail. The design is shown to have an advantageous rejection
performance compared to other conventionally used 2-D planar microphone arrays.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Analog Dynamic Range Compressors using Deep Learning and
  State-space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhi Yin, Gang Cheng, Christian J. Steinmetz, Ruibin Yuan, Richard M. Stern, Roger B. Dannenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a novel approach for developing realistic digital models of
dynamic range compressors for digital audio production by analyzing their
analog prototypes. While realistic digital dynamic compressors are potentially
useful for many applications, the design process is challenging because the
compressors operate nonlinearly over long time scales. Our approach is based on
the structured state space sequence model (S4), as implementing the state-space
model (SSM) has proven to be efficient at learning long-range dependencies and
is promising for modeling dynamic range compressors. We present in this paper a
deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic
range compressor. The model is causal, executes efficiently in real time, and
achieves roughly the same quality as previous deep-learning models but with
fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Analysis of Quality of Conventional Beamforming for Phased
  Microphone Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dheepak Khatri, Kenneth Granlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A theoretical study is performed to analyze the directional response of
different types of microphone array designs. 1-D (linear) and 2-D (planar)
microphone array types are considered, and the delay and sum beamforming and
conventional beamforming techniques are employed to localize the sound source.
A non-dimensional parameter, G, is characterized to simplify and standardize
the rejection performance of both 1-D and 2-D microphone arrays as a function
of array geometry and sound source parameters. This parameter G is then used to
determine an improved design of a 2-D microphone array for far-field sound
localization. One such design, termed the Equi-area array is introduced and
analyzed in detail. The design is shown to have an advantageous rejection
performance compared to other conventionally used 2-D planar microphone arrays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target Speech Extraction with <span class="highlight-title">Pre-train</span>ed AV-Hu<span class="highlight-title">BERT</span> and Mask-And-Recover
  Strategy <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wu, Xueyuan Chen, Xixin Wu, Haizhou Li, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual target speech extraction (AV-TSE) is one of the enabling
technologies in robotics and many audio-visual applications. One of the
challenges of AV-TSE is how to effectively utilize audio-visual synchronization
information in the process. AV-HuBERT can be a useful pre-trained model for
lip-reading, which has not been adopted by AV-TSE. In this paper, we would like
to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system.
We have good reasons to expect an improved performance. To benefit from the
inter and intra-modality correlations, we also propose a novel Mask-And-Recover
(MAR) strategy for self-supervised learning. The experimental results on the
VoxCeleb2 dataset show that our proposed model outperforms the baselines both
in terms of subjective and objective metrics, suggesting that the pre-trained
AV-HuBERT model provides more informative visual cues for target speech
extraction. Furthermore, through a comparative study, we confirm that the
proposed Mask-And-Recover strategy is significantly effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyang Song, Jibin Wu, Malu Zhang, Mike Zheng Shou, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-inspired spiking neural networks (SNNs) have demonstrated great
potential for temporal signal processing. However, their performance in speech
processing remains limited due to the lack of an effective auditory front-end.
To address this limitation, we introduce Spiking-LEAF, a learnable auditory
front-end meticulously designed for SNN-based speech processing. Spiking-LEAF
combines a learnable filter bank with a novel two-compartment spiking neuron
model called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure
of inner hair cells (IHC) and they leverage segregated dendritic and somatic
compartments to effectively capture multi-scale temporal dynamics of speech
signals. Additionally, the IHC-LIF neurons incorporate the lateral feedback
mechanism along with spike regularization loss to enhance spike encoding
efficiency. On keyword spotting and speaker identification tasks, the proposed
Spiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional
real-valued acoustic features in terms of classification accuracy, noise
robustness, and encoding efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arbitrary Discrete Fourier Analysis and Its Application in Replayed
  Speech Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Kuang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a group of finite sequences and its variants were proposed to
use in conducting signal analysis; we called the developed signal analysis
methods arbitrary discrete Fourier analysis (ADFA), Mel-scale discrete Fourier
analysis (MDFA) and constant Q analysis (CQA). The effectiveness of three
signal analysis methods were then validated by testing their performance on a
replayed speech detection benchmark (i.e., the ASVspoof 2019 Physical Access)
along with a state-of-the-art model. Comparable performance to the best
reported systems were shown by the experimental results with three signal
analysis methods. Furthermore, the CQA method shown its efficiency with less
computation time in compared to the convention method constant Q transform
(CQT), which is commonly used in spoofed and fake speech detection and music
processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/shihkuanglee/ADFA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyang Song, Jibin Wu, Malu Zhang, Mike Zheng Shou, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-inspired spiking neural networks (SNNs) have demonstrated great
potential for temporal signal processing. However, their performance in speech
processing remains limited due to the lack of an effective auditory front-end.
To address this limitation, we introduce Spiking-LEAF, a learnable auditory
front-end meticulously designed for SNN-based speech processing. Spiking-LEAF
combines a learnable filter bank with a novel two-compartment spiking neuron
model called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure
of inner hair cells (IHC) and they leverage segregated dendritic and somatic
compartments to effectively capture multi-scale temporal dynamics of speech
signals. Additionally, the IHC-LIF neurons incorporate the lateral feedback
mechanism along with spike regularization loss to enhance spike encoding
efficiency. On keyword spotting and speaker identification tasks, the proposed
Spiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional
real-valued acoustic features in terms of classification accuracy, noise
robustness, and encoding efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music to Dance as Language Translation using Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Correia, Luís A. Alexandre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesising appropriate choreographies from music remains an open problem.
We introduce MDLT, a novel approach that frames the choreography generation
problem as a translation task. Our method leverages an existing data set to
learn to translate sequences of audio into corresponding dance poses. We
present two variants of MDLT: one utilising the Transformer architecture and
the other employing the Mamba architecture. We train our method on AIST++ and
PhantomDance data sets to teach a robotic arm to dance, but our method can be
applied to a full humanoid robot. Evaluation metrics, including Average Joint
Error and Frechet Inception Distance, consistently demonstrate that, when given
a piece of music, MDLT excels at producing realistic and high-quality
choreography. The code can be found at github.com/meowatthemoon/MDLT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards auditory attention decoding with noise-tagging: A pilot study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. A. Scheppink, S. Ahmadi, P. Desain, M. Tangermann, J. Thielen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auditory attention decoding (AAD) aims to extract from brain activity the
attended speaker amidst candidate speakers, offering promising applications for
neuro-steered hearing devices and brain-computer interfacing. This pilot study
makes a first step towards AAD using the noise-tagging stimulus protocol, which
evokes reliable code-modulated evoked potentials, but is minimally explored in
the auditory modality. Participants were sequentially presented with two Dutch
speech stimuli that were amplitude modulated with a unique binary pseudo-random
noise-code, effectively tagging these with additional decodable information. We
compared the decoding of unmodulated audio against audio modulated with various
modulation depths, and a conventional AAD method against a standard method to
decode noise-codes. Our pilot study revealed higher performances for the
conventional method with 70 to 100 percent modulation depths compared to
unmodulated audio. The noise-code decoder did not further improve these
results. These fundamental insights highlight the potential of integrating
noise-codes in speech to enhance auditory speaker detection when multiple
speakers are presented simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Multi-Task Fusion for Emotional Mimicry Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11879v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11879v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Multi-channel Separation and Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Han, Kevin Wilson, Scott Wisdom, John R. Hershey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key challenge in machine learning is to generalize from training data to an
application domain of interest. This work generalizes the recently-proposed
mixture invariant training (MixIT) algorithm to perform unsupervised learning
in the multi-channel setting. We use MixIT to train a model on far-field
microphone array recordings of overlapping reverberant and noisy speech from
the AMI Corpus. The models are trained on both supervised and unsupervised
training data, and are tested on real AMI recordings containing overlapping
speech. To objectively evaluate our models, we also use a synthetic
multi-channel AMI test set. Holding network architectures constant, we find
that a fine-tuned semi-supervised model yields the largest improvement to
SI-SNR and to human listening ratings across synthetic and real datasets,
outperforming supervised models trained on well-matched synthetic data. Our
results demonstrate that unsupervised learning through MixIT enables model
adaptation on both single- and multi-channel real-world speech recordings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive
  Instruction-Tuning Benchmark for Speech <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text language models have shown remarkable zero-shot capability in
generalizing to unseen tasks when provided with well-formulated instructions.
However, existing studies in speech processing primarily focus on limited or
specific tasks. Moreover, the lack of standardized benchmarks hinders a fair
comparison across different approaches. Thus, we present Dynamic-SUPERB, a
benchmark designed for building universal speech models capable of leveraging
instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve
comprehensive coverage of diverse speech tasks and harness instruction tuning,
we invite the community to collaborate and contribute, facilitating the dynamic
growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation
instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of
dimensions, providing a comprehensive platform for evaluation. Additionally, we
propose several approaches to establish benchmark baselines. These include the
utilization of speech models, text language models, and the multimodal encoder.
Evaluation results indicate that while these baselines perform reasonably on
seen tasks, they struggle with unseen ones. We release all materials to the
public and welcome researchers to collaborate on the project, advancing
technologies in the field together.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the proceedings of ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSAC: Multiple Speech Attribute Control Method for Reliable Speech
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04025v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04025v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yuguang Yang, Yuheng Huang, Jixun Yao, Jingjing Yin, Yanni Hu, Heng Lu, Lei Ma, Jianjun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite notable progress, speech emotion recognition (SER) remains
challenging due to the intricate and ambiguous nature of speech emotion,
particularly in wild world. While current studies primarily focus on
recognition and generalization abilities, our research pioneers an
investigation into the reliability of SER methods in the presence of semantic
data shifts and explores how to exert fine-grained control over various
attributes inherent in speech signals to enhance speech emotion modeling. In
this paper, we first introduce MSAC-SERNet, a novel unified SER framework
capable of simultaneously handling both single-corpus and cross-corpus SER.
Specifically, concentrating exclusively on the speech emotion attribute, a
novel CNN-based SER model is presented to extract discriminative emotional
representations, guided by additive margin softmax loss. Considering
information overlap between various speech attributes, we propose a novel
learning paradigm based on correlations of different speech attributes, termed
Multiple Speech Attribute Control (MSAC), which empowers the proposed SER model
to simultaneously capture fine-grained emotion-related features while
mitigating the negative impact of emotion-agnostic representations.
Furthermore, we make a first attempt to examine the reliability of the
MSAC-SERNet framework using out-of-distribution detection methods. Experiments
on both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet
not only consistently outperforms the baseline in all aspects, but achieves
superior performance compared to state-of-the-art SER approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Audio-Visual Speech Separation Model Inspired by
  Cortico-Thalamo-Cortical Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Li, Fenghua Xie, Hang Chen, Kexin Yuan, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual approaches involving visual inputs have laid the foundation for
recent progress in speech separation. However, the optimization of the
concurrent usage of auditory and visual inputs is still an active research
area. Inspired by the cortico-thalamo-cortical circuit, in which the sensory
processing mechanisms of different modalities modulate one another via the
non-lemniscal sensory thalamus, we propose a novel cortico-thalamo-cortical
neural network (CTCNet) for audio-visual speech separation (AVSS). First, the
CTCNet learns hierarchical auditory and visual representations in a bottom-up
manner in separate auditory and visual subnetworks, mimicking the functions of
the auditory and visual cortical areas. Then, inspired by the large number of
connections between cortical regions and the thalamus, the model fuses the
auditory and visual information in a thalamic subnetwork through top-down
connections. Finally, the model transmits this fused information back to the
auditory and visual subnetworks, and the above process is repeated several
times. The results of experiments on three speech separation benchmark datasets
show that CTCNet remarkably outperforms existing AVSS methods with considerably
fewer parameters. These results suggest that mimicking the anatomical
connectome of the mammalian brain has great potential for advancing the
development of deep neural networks. Project repo is
https://github.com/JusperLee/CTCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling of Speech-dependent Own Voice Transfer Characteristics for
  Hearables with In-ear Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattes Ohlenbusch, Christian Rollwage, Simon Doclo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many hearables contain an in-ear microphone, which may be used to capture the
own voice of its user. However, due to the hearable occluding the ear canal,
the in-ear microphone mostly records body-conducted speech, typically suffering
from band-limitation effects and amplification at low frequencies. Since the
occlusion effect is determined by the ratio between the air-conducted and
body-conducted components of own voice, the own voice transfer characteristics
between the outer face of the hearable and the in-ear microphone depend on the
speech content and the individual talker. In this paper, we propose a
speech-dependent model of the own voice transfer characteristics based on
phoneme recognition, assuming a linear time-invariant relative transfer
function for each phoneme. We consider both individual models as well as models
averaged over several talkers. Experimental results based on recordings with a
prototype hearable show that the proposed speech-dependent model enables to
simulate in-ear signals more accurately than a speech-independent model in
terms of technical measures, especially under utterance mismatch and talker
mismatch. Additionally, simulation results show that talker-averaged models
generalize better to different talkers than individual models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures; Extended version of arXiv:2309.08294 (more
  detailed description of the problem, additional models considered, more
  systematic evaluation conducted on a different, larger dataset) -> Updated
  version (20th march 2024): major changes after internal review; in submission</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music to Dance as Language Translation using Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Correia, Luís A. Alexandre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesising appropriate choreographies from music remains an open problem.
We introduce MDLT, a novel approach that frames the choreography generation
problem as a translation task. Our method leverages an existing data set to
learn to translate sequences of audio into corresponding dance poses. We
present two variants of MDLT: one utilising the Transformer architecture and
the other employing the Mamba architecture. We train our method on AIST++ and
PhantomDance data sets to teach a robotic arm to dance, but our method can be
applied to a full humanoid robot. Evaluation metrics, including Average Joint
Error and Frechet Inception Distance, consistently demonstrate that, when given
a piece of music, MDLT excels at producing realistic and high-quality
choreography. The code can be found at github.com/meowatthemoon/MDLT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogue Understandability: Why are we streaming movies with subtitles? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helard Becerra, Alessandro Ragano, Diptasree Debnath, Asad Ullah, Crisron Rudolf Lucas, Martin Walsh, Andrew Hines
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watching movies and TV shows with subtitles enabled is not simply down to
audibility or speech intelligibility. A variety of evolving factors related to
technological advances, cinema production and social behaviour challenge our
perception and understanding. This study seeks to formalise and give context to
these influential factors under a wider and novel term referred to as Dialogue
Understandability. We propose a working definition for Dialogue
Understandability being a listener's capacity to follow the story without undue
cognitive effort or concentration being required that impacts their Quality of
Experience (QoE). The paper identifies, describes and categorises the factors
that influence Dialogue Understandability mapping them over the QoE framework,
a media streaming lifecycle, and the stakeholders involved. We then explore
available measurement tools in the literature and link them to the factors they
could potentially be used for. The maturity and suitability of these tools is
evaluated over a set of pilot experiments. Finally, we reflect on the gaps that
still need to be filled, what we can measure and what not, future subjective
experiments, and new research trends that could help us to fully characterise
Dialogue Understandability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards auditory attention decoding with noise-tagging: A pilot study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. A. Scheppink, S. Ahmadi, P. Desain, M. Tangermann, J. Thielen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auditory attention decoding (AAD) aims to extract from brain activity the
attended speaker amidst candidate speakers, offering promising applications for
neuro-steered hearing devices and brain-computer interfacing. This pilot study
makes a first step towards AAD using the noise-tagging stimulus protocol, which
evokes reliable code-modulated evoked potentials, but is minimally explored in
the auditory modality. Participants were sequentially presented with two Dutch
speech stimuli that were amplitude modulated with a unique binary pseudo-random
noise-code, effectively tagging these with additional decodable information. We
compared the decoding of unmodulated audio against audio modulated with various
modulation depths, and a conventional AAD method against a standard method to
decode noise-codes. Our pilot study revealed higher performances for the
conventional method with 70 to 100 percent modulation depths compared to
unmodulated audio. The noise-code decoder did not further improve these
results. These fundamental insights highlight the potential of integrating
noise-codes in speech to enhance auditory speaker detection when multiple
speakers are presented simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving End-to-End Spoken Language Understanding <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinggui Wang, Wei Huang, Le Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken language understanding (SLU), one of the key enabling technologies for
human-computer interaction in IoT devices, provides an easy-to-use user
interface. Human speech can contain a lot of user-sensitive information, such
as gender, identity, and sensitive content. New types of security and privacy
breaches have thus emerged. Users do not want to expose their personal
sensitive information to malicious attacks by untrusted third parties. Thus,
the SLU system needs to ensure that a potential malicious attacker cannot
deduce the sensitive attributes of the users, while it should avoid greatly
compromising the SLU accuracy. To address the above challenge, this paper
proposes a novel SLU multi-task privacy-preserving model to prevent both the
speech recognition (ASR) and identity recognition (IR) attacks. The model uses
the hidden layer separation technique so that SLU information is distributed
only in a specific portion of the hidden layer, and the other two types of
information are removed to obtain a privacy-secure hidden layer. In order to
achieve good balance between efficiency and privacy, we introduce a new
mechanism of model pre-training, namely joint adversarial training, to further
enhance the user privacy. Experiments over two SLU datasets show that the
proposed method can reduce the accuracy of both the ASR and IR attacks close to
that of a random guess, while leaving the SLU performance largely unaffected.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Multi-Task Fusion for Emotional Mimicry Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11879v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11879v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Multi-channel Separation and Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Han, Kevin Wilson, Scott Wisdom, John R. Hershey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key challenge in machine learning is to generalize from training data to an
application domain of interest. This work generalizes the recently-proposed
mixture invariant training (MixIT) algorithm to perform unsupervised learning
in the multi-channel setting. We use MixIT to train a model on far-field
microphone array recordings of overlapping reverberant and noisy speech from
the AMI Corpus. The models are trained on both supervised and unsupervised
training data, and are tested on real AMI recordings containing overlapping
speech. To objectively evaluate our models, we also use a synthetic
multi-channel AMI test set. Holding network architectures constant, we find
that a fine-tuned semi-supervised model yields the largest improvement to
SI-SNR and to human listening ratings across synthetic and real datasets,
outperforming supervised models trained on well-matched synthetic data. Our
results demonstrate that unsupervised learning through MixIT enables model
adaptation on both single- and multi-channel real-world speech recordings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive
  Instruction-Tuning Benchmark for Speech <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text language models have shown remarkable zero-shot capability in
generalizing to unseen tasks when provided with well-formulated instructions.
However, existing studies in speech processing primarily focus on limited or
specific tasks. Moreover, the lack of standardized benchmarks hinders a fair
comparison across different approaches. Thus, we present Dynamic-SUPERB, a
benchmark designed for building universal speech models capable of leveraging
instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve
comprehensive coverage of diverse speech tasks and harness instruction tuning,
we invite the community to collaborate and contribute, facilitating the dynamic
growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation
instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of
dimensions, providing a comprehensive platform for evaluation. Additionally, we
propose several approaches to establish benchmark baselines. These include the
utilization of speech models, text language models, and the multimodal encoder.
Evaluation results indicate that while these baselines perform reasonably on
seen tasks, they struggle with unseen ones. We release all materials to the
public and welcome researchers to collaborate on the project, advancing
technologies in the field together.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the proceedings of ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSAC: Multiple Speech Attribute Control Method for Reliable Speech
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04025v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04025v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yuguang Yang, Yuheng Huang, Jixun Yao, Jingjing Yin, Yanni Hu, Heng Lu, Lei Ma, Jianjun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite notable progress, speech emotion recognition (SER) remains
challenging due to the intricate and ambiguous nature of speech emotion,
particularly in wild world. While current studies primarily focus on
recognition and generalization abilities, our research pioneers an
investigation into the reliability of SER methods in the presence of semantic
data shifts and explores how to exert fine-grained control over various
attributes inherent in speech signals to enhance speech emotion modeling. In
this paper, we first introduce MSAC-SERNet, a novel unified SER framework
capable of simultaneously handling both single-corpus and cross-corpus SER.
Specifically, concentrating exclusively on the speech emotion attribute, a
novel CNN-based SER model is presented to extract discriminative emotional
representations, guided by additive margin softmax loss. Considering
information overlap between various speech attributes, we propose a novel
learning paradigm based on correlations of different speech attributes, termed
Multiple Speech Attribute Control (MSAC), which empowers the proposed SER model
to simultaneously capture fine-grained emotion-related features while
mitigating the negative impact of emotion-agnostic representations.
Furthermore, we make a first attempt to examine the reliability of the
MSAC-SERNet framework using out-of-distribution detection methods. Experiments
on both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet
not only consistently outperforms the baseline in all aspects, but achieves
superior performance compared to state-of-the-art SER approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling of Speech-dependent Own Voice Transfer Characteristics for
  Hearables with In-ear Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattes Ohlenbusch, Christian Rollwage, Simon Doclo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many hearables contain an in-ear microphone, which may be used to capture the
own voice of its user. However, due to the hearable occluding the ear canal,
the in-ear microphone mostly records body-conducted speech, typically suffering
from band-limitation effects and amplification at low frequencies. Since the
occlusion effect is determined by the ratio between the air-conducted and
body-conducted components of own voice, the own voice transfer characteristics
between the outer face of the hearable and the in-ear microphone depend on the
speech content and the individual talker. In this paper, we propose a
speech-dependent model of the own voice transfer characteristics based on
phoneme recognition, assuming a linear time-invariant relative transfer
function for each phoneme. We consider both individual models as well as models
averaged over several talkers. Experimental results based on recordings with a
prototype hearable show that the proposed speech-dependent model enables to
simulate in-ear signals more accurately than a speech-independent model in
terms of technical measures, especially under utterance mismatch and talker
mismatch. Additionally, simulation results show that talker-averaged models
generalize better to different talkers than individual models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures; Extended version of arXiv:2309.08294 (more
  detailed description of the problem, additional models considered, more
  systematic evaluation conducted on a different, larger dataset) -> Updated
  version (20th march 2024): major changes after internal review; in submission</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for
  Noise-Robust Speech Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyoJung Han, Mohamed Anwar, Juan Pino, Wei-Ning Hsu, Marine Carpuat, Bowen Shi, Changhan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech recognition and translation systems perform poorly on noisy inputs,
which are frequent in realistic environments. Augmenting these systems with
visual signals has the potential to improve robustness to noise. However,
audio-visual (AV) data is only available in limited amounts and for fewer
languages than audio-only resources. To address this gap, we present XLAVS-R, a
cross-lingual audio-visual speech representation model for noise-robust speech
recognition and translation in over 100 languages. It is designed to maximize
the benefits of limited multilingual AV pre-training data, by building on top
of audio-only multilingual pre-training and simplifying existing pre-training
schemes. Extensive evaluation on the MuAViC benchmark shows the strength of
XLAVS-R on downstream audio-visual speech recognition and translation tasks,
where it outperforms the previous state of the art by up to 18.5% WER and 4.7
BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability
with audio-only fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Green AI for Audio Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhajit Saha, Md Sahidullah, Swagatam Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state-of-the-art audio deepfake detectors leveraging deep neural networks
exhibit impressive recognition performance. Nonetheless, this advantage is
accompanied by a significant carbon footprint. This is mainly due to the use of
high-performance computing with accelerators and high training time. Studies
show that average deep NLP model produces around 626k lbs of
CO\textsubscript{2} which is equivalent to five times of average US car
emission at its lifetime. This is certainly a massive threat to the
environment. To tackle this challenge, this study presents a novel framework
for audio deepfake detection that can be seamlessly trained using standard CPU
resources. Our proposed framework utilizes off-the-shelve self-supervised
learning (SSL) based models which are pre-trained and available in public
repositories. In contrast to existing methods that fine-tune SSL models and
employ additional deep neural networks for downstream tasks, we exploit
classical machine learning algorithms such as logistic regression and shallow
neural networks using the SSL embeddings extracted using the pre-trained model.
Our approach shows competitive results compared to the commonly used
high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA
dataset, we achieve a 0.90\% equal error rate (EER) with less than 1k trainable
model parameters. To encourage further research in this direction and support
reproducible results, the Python code will be made publicly accessible
following acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is under review in a conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Robustness of Spectral Clustering for Deep Speaker
  Diarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Raghav, Md Sahidullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering speaker embeddings is crucial in speaker diarization but hasn't
received as much focus as other components. Moreover, the robustness of speaker
diarization across various datasets hasn't been explored when the development
and evaluation data are from different domains. To bridge this gap, this study
thoroughly examines spectral clustering for both same-domain and cross-domain
speaker diarization. Our extensive experiments on two widely used corpora, AMI
and DIHARD, reveal the performance trend of speaker diarization in the presence
of domain mismatch. We observe that the performance difference between two
different domain conditions can be attributed to the role of spectral
clustering. In particular, keeping other modules unchanged, we show that
differences in optimal tuning parameters as well as speaker count estimation
originates due to the mismatch. This study opens several future directions for
speaker diarization research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by
  Attention Constraints <span class="chip">TAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        PeiYing Lee, HauYun Guo, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-End Neural Diarization with Encoder-Decoder based Attractor (EEND-EDA)
is an end-to-end neural model for automatic speaker segmentation and labeling.
It achieves the capability to handle flexible number of speakers by estimating
the number of attractors. EEND-EDA, however, struggles to accurately capture
local speaker dynamics. This work proposes an auxiliary loss that aims to guide
the Transformer encoders at the lower layer of EEND-EDA model to enhance the
effect of self-attention modules using speaker activity information. The
results evaluated on public dataset Mini LibriSpeech, demonstrates the
effectiveness of the work, reducing Diarization Error Rate from 30.95% to
28.17%. We will release the source code on GitHub to allow further research and
reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The 28th International Conference on Technologies and
  Applications of Artificial Intelligence (TAAI), in Chinese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaProj: Adaptively Scaled Angular Margin Subspace Projections for
  Anomalous Sound Detection with Auxiliary Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Wilkinghoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state-of-the-art approach for semi-supervised anomalous sound detection
is to first learn an embedding space by using auxiliary classification tasks
based on meta information or self-supervised learning and then estimate the
distribution of normal data. In this work, AdaProj a novel loss function is
presented. In contrast to commonly used angular margin losses, which project
data of each class as close as possible to their corresponding class centers,
AdaProj learns to project data onto class-specific subspaces. By doing so, the
resulting distributions of embeddings belonging to normal data are not required
to be as restrictive as other loss functions allowing a more detailed view on
the data. In experiments conducted on the DCASE2022 and DCASE2023 datasets, it
is shown that using AdaProj to learn an embedding space significantly
outperforms other commonly used loss functions and results in a
state-of-the-art performance on the DCASE2023 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ emoDARTS: Joint Optimisation of CNN & Sequential Neural Network
  Architectures for Superior Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thejan Rajapakshe, Rajib Rana, Sara Khalifa, Berrak Sisman, Bjorn W. Schuller, Carlos Busso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Emotion Recognition (SER) is crucial for enabling computers to
understand the emotions conveyed in human communication. With recent
advancements in Deep Learning (DL), the performance of SER models has
significantly improved. However, designing an optimal DL architecture requires
specialised knowledge and experimental assessments. Fortunately, Neural
Architecture Search (NAS) provides a potential solution for automatically
determining the best DL model. The Differentiable Architecture Search (DARTS)
is a particularly efficient method for discovering optimal models. This study
presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network
(SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature
supports the selection of CNN and LSTM coupling to improve performance.
  While DARTS has previously been used to choose CNN and LSTM operations
independently, our technique adds a novel mechanism for selecting CNN and SeqNN
operations in conjunction using DARTS. Unlike earlier work, we do not impose
limits on the layer order of the CNN. Instead, we let DARTS choose the best
layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms
conventionally designed CNN-LSTM models and surpasses the best-reported SER
results achieved through DARTS on CNN-LSTM by evaluating our approach on the
IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Affective Computing on February 19,
  2024. arXiv admin note: text overlap with arXiv:2305.14402</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio
  Benchmarks and Novel Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Baird, Rachel Manzelli, Panagiotis Tzirakis, Chris Gagne, Haoqi Li, Sadie Allen, Sander Dieleman, Brian Kulis, Shrikanth S. Narayanan, Alan Cowen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine
learning (ML) experts from various audio domains. There are several valuable
audio-driven ML tasks, from speech emotion recognition to audio event
detection, but the community is sparse compared to other ML areas, e.g.,
computer vision or natural language processing. A major limitation with audio
is the available data; with audio being a time-dependent modality, high-quality
data collection is time-consuming and costly, making it challenging for
academic groups to apply their often state-of-the-art strategies to a larger,
more generalizable dataset. In this short white paper, to encourage researchers
with limited access to large-datasets, the organizers first outline several
open-source datasets that are available to the community, and for the duration
of the workshop are making several propriety datasets available. Namely, three
vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech
dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We
outline the current baselines on these datasets but encourage researchers from
across audio to utilize them outside of the initial baseline tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual
  Speech Separation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17189v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17189v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Pegg, Kai Li, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speech separation methods aim to integrate different modalities
to generate high-quality separated speech, thereby enhancing the performance of
downstream tasks such as speech recognition. Most existing state-of-the-art
(SOTA) models operate in the time domain. However, their overly simplistic
approach to modeling acoustic features often necessitates larger and more
computationally intensive models in order to achieve SOTA performance. In this
paper, we present a novel time-frequency domain audio-visual speech separation
method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies
its algorithms on the complex time-frequency bins yielded by the Short-Time
Fourier Transform. We model and capture the time and frequency dimensions of
the audio independently using a multi-layered RNN along each dimension.
Furthermore, we introduce a unique attention-based fusion technique for the
efficient integration of audio and visual information, and a new mask
separation approach that takes advantage of the intrinsic spectral nature of
the acoustic features for a clearer separation. RTFS-Net outperforms the prior
SOTA method in both inference speed and separation quality while reducing the
number of parameters by 90% and MACs by 83%. This is the first time-frequency
domain audio-visual speech separation method to outperform all contemporary
time-domain counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Twelfth International Conference on Learning
  Representations (ICLR) 2024, see https://openreview.net/forum?id=PEuDO2EiDr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00969v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00969v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Budaghyan, Charles C. Onu, Arsenii Gorin, Cem Subakan, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries - and the accompanying CryCeleb 2023 task, which is a public
speaker verification challenge based on cry sounds. We released more than 6
hours of manually segmented cry sounds from 786 newborns for academic use,
aiming to encourage research in infant cry analysis. The inaugural public
competition attracted 59 participants, 11 of whom improved the baseline
performance. The top-performing system achieved a significant improvement
scoring 25.8% equal error rate, which is still far from the performance of
state-of-the-art adult speaker verification systems. Therefore, we believe
there is room for further research on this dataset, potentially extending
beyond the verification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-scale Contrastive Language-Audio <span class="highlight-title">Pretrain</span>ing with Feature Fusion
  and Keyword-to-Caption Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06687v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06687v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has shown remarkable success in the field of multimodal
representation learning. In this paper, we propose a pipeline of contrastive
language-audio pretraining to develop an audio representation by combining
audio data with natural language descriptions. To accomplish this target, we
first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs
from different data sources. Second, we construct a contrastive language-audio
pretraining model by considering different audio encoders and text encoders. We
incorporate the feature fusion mechanism and keyword-to-caption augmentation
into the model design to further enable the model to process audio inputs of
variable lengths and enhance the performance. Third, we perform comprehensive
experiments to evaluate our model across three tasks: text-to-audio retrieval,
zero-shot audio classification, and supervised audio classification. The
results demonstrate that our model achieves superior performance in
text-to-audio retrieval task. In audio classification tasks, the model achieves
state-of-the-art performance in the zero-shot setting and is able to obtain
performance comparable to models' results in the non-zero-shot setting.
LAION-Audio-630K and the proposed model are both available to the public.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wager, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for
  Noise-Robust Speech Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyoJung Han, Mohamed Anwar, Juan Pino, Wei-Ning Hsu, Marine Carpuat, Bowen Shi, Changhan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech recognition and translation systems perform poorly on noisy inputs,
which are frequent in realistic environments. Augmenting these systems with
visual signals has the potential to improve robustness to noise. However,
audio-visual (AV) data is only available in limited amounts and for fewer
languages than audio-only resources. To address this gap, we present XLAVS-R, a
cross-lingual audio-visual speech representation model for noise-robust speech
recognition and translation in over 100 languages. It is designed to maximize
the benefits of limited multilingual AV pre-training data, by building on top
of audio-only multilingual pre-training and simplifying existing pre-training
schemes. Extensive evaluation on the MuAViC benchmark shows the strength of
XLAVS-R on downstream audio-visual speech recognition and translation tasks,
where it outperforms the previous state of the art by up to 18.5% WER and 4.7
BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability
with audio-only fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by
  Attention Constraints <span class="chip">TAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        PeiYing Lee, HauYun Guo, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-End Neural Diarization with Encoder-Decoder based Attractor (EEND-EDA)
is an end-to-end neural model for automatic speaker segmentation and labeling.
It achieves the capability to handle flexible number of speakers by estimating
the number of attractors. EEND-EDA, however, struggles to accurately capture
local speaker dynamics. This work proposes an auxiliary loss that aims to guide
the Transformer encoders at the lower layer of EEND-EDA model to enhance the
effect of self-attention modules using speaker activity information. The
results evaluated on public dataset Mini LibriSpeech, demonstrates the
effectiveness of the work, reducing Diarization Error Rate from 30.95% to
28.17%. We will release the source code on GitHub to allow further research and
reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The 28th International Conference on Technologies and
  Applications of Artificial Intelligence (TAAI), in Chinese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CATSE: A Context-Aware Framework for Causal Target Sound Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrishail Baligar, Mikolaj Kegler, Bryce Irvin, Marko Stamenovic, Shawn Newsam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Target Sound Extraction (TSE) focuses on the problem of separating sources of
interest, indicated by a user's cue, from the input mixture. Most existing
solutions operate in an offline fashion and are not suited to the low-latency
causal processing constraints imposed by applications in live-streamed content
such as augmented hearing. We introduce a family of context-aware low-latency
causal TSE models suitable for real-time processing. First, we explore the
utility of context by providing the TSE model with oracle information about
what sound classes make up the input mixture, where the objective of the model
is to extract one or more sources of interest indicated by the user. Since the
practical applications of oracle models are limited due to their assumptions,
we introduce a composite multi-task training objective involving separation and
classification losses. Our evaluation involving single- and multi-source
extraction shows the benefit of using context information in the model either
by means of providing full context or via the proposed multi-task training loss
without the need for full context information. Specifically, we show that our
proposed model outperforms size- and latency-matched Waveformer, a
state-of-the-art model for real-time TSE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaProj: Adaptively Scaled Angular Margin Subspace Projections for
  Anomalous Sound Detection with Auxiliary Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Wilkinghoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state-of-the-art approach for semi-supervised anomalous sound detection
is to first learn an embedding space by using auxiliary classification tasks
based on meta information or self-supervised learning and then estimate the
distribution of normal data. In this work, AdaProj a novel loss function is
presented. In contrast to commonly used angular margin losses, which project
data of each class as close as possible to their corresponding class centers,
AdaProj learns to project data onto class-specific subspaces. By doing so, the
resulting distributions of embeddings belonging to normal data are not required
to be as restrictive as other loss functions allowing a more detailed view on
the data. In experiments conducted on the DCASE2022 and DCASE2023 datasets, it
is shown that using AdaProj to learn an embedding space significantly
outperforms other commonly used loss functions and results in a
state-of-the-art performance on the DCASE2023 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ emoDARTS: Joint Optimisation of CNN & Sequential Neural Network
  Architectures for Superior Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thejan Rajapakshe, Rajib Rana, Sara Khalifa, Berrak Sisman, Bjorn W. Schuller, Carlos Busso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Emotion Recognition (SER) is crucial for enabling computers to
understand the emotions conveyed in human communication. With recent
advancements in Deep Learning (DL), the performance of SER models has
significantly improved. However, designing an optimal DL architecture requires
specialised knowledge and experimental assessments. Fortunately, Neural
Architecture Search (NAS) provides a potential solution for automatically
determining the best DL model. The Differentiable Architecture Search (DARTS)
is a particularly efficient method for discovering optimal models. This study
presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network
(SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature
supports the selection of CNN and LSTM coupling to improve performance.
  While DARTS has previously been used to choose CNN and LSTM operations
independently, our technique adds a novel mechanism for selecting CNN and SeqNN
operations in conjunction using DARTS. Unlike earlier work, we do not impose
limits on the layer order of the CNN. Instead, we let DARTS choose the best
layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms
conventionally designed CNN-LSTM models and surpasses the best-reported SER
results achieved through DARTS on CNN-LSTM by evaluating our approach on the
IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Affective Computing on February 19,
  2024. arXiv admin note: text overlap with arXiv:2305.14402</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio
  Benchmarks and Novel Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Baird, Rachel Manzelli, Panagiotis Tzirakis, Chris Gagne, Haoqi Li, Sadie Allen, Sander Dieleman, Brian Kulis, Shrikanth S. Narayanan, Alan Cowen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine
learning (ML) experts from various audio domains. There are several valuable
audio-driven ML tasks, from speech emotion recognition to audio event
detection, but the community is sparse compared to other ML areas, e.g.,
computer vision or natural language processing. A major limitation with audio
is the available data; with audio being a time-dependent modality, high-quality
data collection is time-consuming and costly, making it challenging for
academic groups to apply their often state-of-the-art strategies to a larger,
more generalizable dataset. In this short white paper, to encourage researchers
with limited access to large-datasets, the organizers first outline several
open-source datasets that are available to the community, and for the duration
of the workshop are making several propriety datasets available. Namely, three
vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech
dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We
outline the current baselines on these datasets but encourage researchers from
across audio to utilize them outside of the initial baseline tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Green AI for Audio Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhajit Saha, Md Sahidullah, Swagatam Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state-of-the-art audio deepfake detectors leveraging deep neural networks
exhibit impressive recognition performance. Nonetheless, this advantage is
accompanied by a significant carbon footprint. This is mainly due to the use of
high-performance computing with accelerators and high training time. Studies
show that average deep NLP model produces around 626k lbs of
CO\textsubscript{2} which is equivalent to five times of average US car
emission at its lifetime. This is certainly a massive threat to the
environment. To tackle this challenge, this study presents a novel framework
for audio deepfake detection that can be seamlessly trained using standard CPU
resources. Our proposed framework utilizes off-the-shelve self-supervised
learning (SSL) based models which are pre-trained and available in public
repositories. In contrast to existing methods that fine-tune SSL models and
employ additional deep neural networks for downstream tasks, we exploit
classical machine learning algorithms such as logistic regression and shallow
neural networks using the SSL embeddings extracted using the pre-trained model.
Our approach shows competitive results compared to the commonly used
high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA
dataset, we achieve a 0.90\% equal error rate (EER) with less than 1k trainable
model parameters. To encourage further research in this direction and support
reproducible results, the Python code will be made publicly accessible
following acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is under review in a conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Robustness of Spectral Clustering for Deep Speaker
  Diarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Raghav, Md Sahidullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering speaker embeddings is crucial in speaker diarization but hasn't
received as much focus as other components. Moreover, the robustness of speaker
diarization across various datasets hasn't been explored when the development
and evaluation data are from different domains. To bridge this gap, this study
thoroughly examines spectral clustering for both same-domain and cross-domain
speaker diarization. Our extensive experiments on two widely used corpora, AMI
and DIHARD, reveal the performance trend of speaker diarization in the presence
of domain mismatch. We observe that the performance difference between two
different domain conditions can be attributed to the role of spectral
clustering. In particular, keeping other modules unchanged, we show that
differences in optimal tuning parameters as well as speaker count estimation
originates due to the mismatch. This study opens several future directions for
speaker diarization research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourced Multilingual Speech Intelligibility Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Lechler, Kamil Wojcicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of generative audio features, there is an increasing need for
rapid evaluation of their impact on speech intelligibility. Beyond the existing
laboratory measures, which are expensive and do not scale well, there has been
comparatively little work on crowdsourced assessment of intelligibility.
Standards and recommendations are yet to be defined, and publicly available
multilingual test materials are lacking. In response to this challenge, we
propose an approach for a crowdsourced intelligibility assessment. We detail
the test design, the collection and public release of the multilingual speech
data, and the results of our early experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual
  Speech Separation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17189v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17189v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Pegg, Kai Li, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speech separation methods aim to integrate different modalities
to generate high-quality separated speech, thereby enhancing the performance of
downstream tasks such as speech recognition. Most existing state-of-the-art
(SOTA) models operate in the time domain. However, their overly simplistic
approach to modeling acoustic features often necessitates larger and more
computationally intensive models in order to achieve SOTA performance. In this
paper, we present a novel time-frequency domain audio-visual speech separation
method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies
its algorithms on the complex time-frequency bins yielded by the Short-Time
Fourier Transform. We model and capture the time and frequency dimensions of
the audio independently using a multi-layered RNN along each dimension.
Furthermore, we introduce a unique attention-based fusion technique for the
efficient integration of audio and visual information, and a new mask
separation approach that takes advantage of the intrinsic spectral nature of
the acoustic features for a clearer separation. RTFS-Net outperforms the prior
SOTA method in both inference speed and separation quality while reducing the
number of parameters by 90% and MACs by 83%. This is the first time-frequency
domain audio-visual speech separation method to outperform all contemporary
time-domain counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Twelfth International Conference on Learning
  Representations (ICLR) 2024, see https://openreview.net/forum?id=PEuDO2EiDr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00969v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00969v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Budaghyan, Charles C. Onu, Arsenii Gorin, Cem Subakan, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries - and the accompanying CryCeleb 2023 task, which is a public
speaker verification challenge based on cry sounds. We released more than 6
hours of manually segmented cry sounds from 786 newborns for academic use,
aiming to encourage research in infant cry analysis. The inaugural public
competition attracted 59 participants, 11 of whom improved the baseline
performance. The top-performing system achieved a significant improvement
scoring 25.8% equal error rate, which is still far from the performance of
state-of-the-art adult speaker verification systems. Therefore, we believe
there is room for further research on this dataset, potentially extending
beyond the verification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-scale Contrastive Language-Audio <span class="highlight-title">Pretrain</span>ing with Feature Fusion
  and Keyword-to-Caption Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06687v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06687v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has shown remarkable success in the field of multimodal
representation learning. In this paper, we propose a pipeline of contrastive
language-audio pretraining to develop an audio representation by combining
audio data with natural language descriptions. To accomplish this target, we
first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs
from different data sources. Second, we construct a contrastive language-audio
pretraining model by considering different audio encoders and text encoders. We
incorporate the feature fusion mechanism and keyword-to-caption augmentation
into the model design to further enable the model to process audio inputs of
variable lengths and enhance the performance. Third, we perform comprehensive
experiments to evaluate our model across three tasks: text-to-audio retrieval,
zero-shot audio classification, and supervised audio classification. The
results demonstrate that our model achieves superior performance in
text-to-audio retrieval task. In audio classification tasks, the model achieves
state-of-the-art performance in the zero-shot setting and is able to obtain
performance comparable to models' results in the non-zero-shot setting.
LAION-Audio-630K and the proposed model are both available to the public.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing
  Using Discrete Speech Unit Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wataru Nakata, Kazuki Yamauchi, Dong Yang, Hiroaki Hyodo, Yuki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present UTDUSS, the UTokyo-SaruLab system submitted to Interspeech2024
Speech Processing Using Discrete Speech Unit Challenge. The challenge focuses
on using discrete speech unit learned from large speech corpora for some tasks.
We submitted our UTDUSS system to two text-to-speech tracks: Vocoder and
Acoustic+Vocoder. Our system incorporates neural audio codec (NAC) pre-trained
on only speech corpora, which makes the learned codec represent rich acoustic
features that are necessary for high-fidelity speech reconstruction. For the
acoustic+vocoder track, we trained an acoustic model based on Transformer
encoder-decoder that predicted the pre-trained NAC tokens from text input. We
describe our strategies to build these models, such as data selection,
downsampling, and hyper-parameter tuning. Our system ranked in second and first
for the Vocoder and Acoustic+Vocoder tracks, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Gnana Praveen, Jahangir Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal emotion recognition has recently gained a lot of attention since
it can leverage diverse and complementary relationships over multiple
modalities, such as audio, visual, and text. Most state-of-the-art methods for
multimodal fusion rely on recurrent networks or conventional attention
mechanisms that do not effectively leverage the complementary nature of the
modalities. In this paper, we focus on dimensional emotion recognition based on
the fusion of facial, vocal, and text modalities extracted from videos.
Specifically, we propose a recursive cross-modal attention (RCMA) to
effectively capture the complementary relationships across the modalities in a
recursive fashion. The proposed model is able to effectively capture the
inter-modal relationships by computing the cross-attention weights across the
individual modalities and the joint representation of the other two modalities.
To further improve the inter-modal relationships, the obtained attended
features of the individual modalities are again fed as input to the cross-modal
attention to refine the feature representations of the individual modalities.
In addition to that, we have used Temporal convolution networks (TCNs) to
capture the temporal modeling (intra-modal relationships) of the individual
modalities. By deploying the TCNs as well cross-modal attention in a recursive
fashion, we are able to effectively capture both intra- and inter-modal
relationships across the audio, visual, and text modalities. Experimental
results on validation-set videos from the AffWild2 dataset indicate that our
proposed fusion model is able to achieve significant improvement over the
baseline for the sixth challenge of Affective Behavior Analysis in-the-Wild
2024 (ABAW6) competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.09068;
  text overlap with arXiv:2203.14779 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vibration Sensitivity of one-port and two-port MEMS microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francis Doyon-D'Amour, Carly Stalder, Timothy Hodges, Michel Stephan, Lixiue Wu, Triantafillos Koukoulas, Stephane Leahy, Raphael St-Gelais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro-electro-mechanical system (MEMS) microphones (mics) with two acoustic
ports are currently receiving considerable interest, with the promise of
achieving higher directional sensitivity compared to traditional one-port
architectures. However, measuring pressure differences in two-port microphones
typically commands sensing elements that are softer than in one-port mics, and
are therefore presumably more prone to interference from external vibration.
Here we derive a universal expression for microphone sensitivity to vibration
and we experimentally demonstrate its validity for several emerging two-port
microphone technologies. We also perform vibration measurements on a one-port
mic, thus providing a one-stop direct comparison between one-port and two-port
sensing approaches. We find that the acoustically-referred vibration
sensitivity of two-port MEMS mics, in units of measured acoustic pressure per
external acceleration (i.e., Pascals per g), does not depend on the sensing
element stiffness nor on its natural frequency. We also show that this
vibration sensitivity in two-port mics is inversely proportional to frequency
as opposed to the frequency independent behavior observed in one-port mics.
This is confirmed experimentally for several types of microphone packages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Long-Content Speech Recognition With Factorized Neural
  Transducer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Gong, Yu Wu, Jinyu Li, Shujie Liu, Rui Zhao, Xie Chen, Yanmin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose two novel approaches, which integrate long-content
information into the factorized neural transducer (FNT) based architecture in
both non-streaming (referred to as LongFNT ) and streaming (referred to as
SLongFNT ) scenarios. We first investigate whether long-content transcriptions
can improve the vanilla conformer transducer (C-T) models. Our experiments
indicate that the vanilla C-T models do not exhibit improved performance when
utilizing long-content transcriptions, possibly due to the predictor network of
C-T models not functioning as a pure language model. Instead, FNT shows its
potential in utilizing long-content information, where we propose the LongFNT
model and explore the impact of long-content information in both text
(LongFNT-Text) and speech (LongFNT-Speech). The proposed LongFNT-Text and
LongFNT-Speech models further complement each other to achieve better
performance, with transcription history proving more valuable to the model. The
effectiveness of our LongFNT approach is evaluated on LibriSpeech and
GigaSpeech corpora, and obtains relative 19% and 12% word error rate reduction,
respectively. Furthermore, we extend the LongFNT model to the streaming
scenario, which is named SLongFNT , consisting of SLongFNT-Text and
SLongFNT-Speech approaches to utilize long-content text and speech information.
Experiments show that the proposed SLongFNT model achieves relative 26% and 17%
WER reduction on LibriSpeech and GigaSpeech respectively while keeping a good
latency, compared to the FNT baseline. Overall, our proposed LongFNT and
SLongFNT highlight the significance of considering long-content speech and
transcription knowledge for improving both non-streaming and streaming speech
recognition systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TASLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KunquDB: An Attempt for Speaker Verification in the Chinese Opera
  Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huali Zhou, Yuke Lin, Dong Liu, Ming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work aims to promote Chinese opera research in both musical and speech
domains, with a primary focus on overcoming the data limitations. We introduce
KunquDB, a relatively large-scale, well-annotated audio-visual dataset
comprising 339 speakers and 128 hours of content. Originating from the Kunqu
Opera Art Canon (Kunqu yishu dadian), KunquDB is meticulously structured by
dialogue lines, providing explicit annotations including character names,
speaker names, gender information, vocal manner classifications, and
accompanied by preliminary text transcriptions. KunquDB provides a versatile
foundation for role-centric acoustic studies and advancements in speech-related
research, including Automatic Speaker Verification (ASV). Beyond enriching
opera research, this dataset bridges the gap between artistic expression and
technological innovation. Pioneering the exploration of ASV in Chinese opera,
we construct four test trials considering two distinct vocal manners in opera
voices: stage speech (ST) and singing (S). Implementing domain adaptation
methods effectively mitigates domain mismatches induced by these vocal manner
variations while there is still room for further improvement as a benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building speech corpus with diverse voice characteristics for its
  prompt-based representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aya Watanabe, Shinnosuke Takamichi, Yuki Saito, Wataru Nakata, Detai Xin, Hiroshi Saruwatari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In text-to-speech synthesis, the ability to control voice characteristics is
vital for various applications. By leveraging thriving text prompt-based
generation techniques, it should be possible to enhance the nuanced control of
voice characteristics. While previous research has explored the prompt-based
manipulation of voice characteristics, most studies have used pre-recorded
speech, which limits the diversity of voice characteristics available. Thus, we
aim to address this gap by creating a novel corpus and developing a model for
prompt-based manipulation of voice characteristics in text-to-speech synthesis,
facilitating a broader range of voice characteristics. Specifically, we propose
a method to build a sizable corpus pairing voice characteristics descriptions
with corresponding speech samples. This involves automatically gathering
voice-related speech data from the Internet, ensuring its quality, and manually
annotating it using crowdsourcing. We implement this method with Japanese
language data and analyze the results to validate its effectiveness.
Subsequently, we propose a construction method of the model to retrieve speech
from voice characteristics descriptions based on a contrastive learning method.
We train the model using not only conservative contrastive learning but also
feature prediction learning to predict quantitative speech features
corresponding to voice characteristics. We evaluate the model performance via
experiments with the corpus we constructed above.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing. arXiv admin note: text overlap with arXiv:2309.13509</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration
  Transducer <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xi, Hao Li, Baochen Yang, Haoyu Li, Hainan Xu, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing an efficient keyword spotting (KWS) system that delivers
exceptional performance on resource-constrained edge devices has long been a
subject of significant attention. Existing KWS search algorithms typically
follow a frame-synchronous approach, where search decisions are made repeatedly
at each frame despite the fact that most frames are keyword-irrelevant. In this
paper, we propose TDT-KWS, which leverages token-and-duration Transducers (TDT)
for KWS tasks. We also propose a novel KWS task-specific decoding algorithm for
Transducer-based models, which supports highly effective frame-asynchronous
keyword search in streaming speech scenarios. With evaluations conducted on
both the public Hey Snips and self-constructed LibriKWS-20 datasets, our
proposed KWS-decoding algorithm produces more accurate results than
conventional ASR decoding algorithms. Additionally, TDT-KWS achieves on-par or
better wake word detection performance than both RNN-T and traditional TDT-ASR
systems while achieving significant inference speed-up. Furthermore,
experiments show that TDT-KWS is more robust to noisy environments compared to
RNN-T KWS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Onset and offset weighted loss function for sound event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a typical sound event detection (SED) system, the existence of a sound
event is detected at a frame level, and consecutive frames with the same event
detected are combined as one sound event. The median filter is applied as a
post-processing step to remove detection errors as much as possible. However,
detection errors occurring around the onset and offset of a sound event are
beyond the capacity of the median filter. To address this issue, an onset and
offset weighted binary cross-entropy (OWBCE) loss function is proposed in this
paper, which trains the DNN model to be more robust on frames around (a) onsets
and offsets. Experiments are carried out in the context of DCASE 2022 task 4.
Results show that OWBCE outperforms BCE when different models are considered.
For a basic CRNN, relative improvements of 6.43% in event-F1, 1.96% in PSDS1,
and 2.43% in PSDS2 can be achieved by OWBCE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-aware convolution for sound event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In sound event detection (SED), convolution neural networks (CNNs) are widely
used to extract time-frequency patterns from the input spectrogram. However,
features extracted by CNN can be insensitive to the shift of time-frequency
patterns along the frequency axis. To address this issue, frequency dynamic
convolution (FDY) has been proposed, which applies different kernels to
different frequency components. Compared to the vannila CNN, FDY requires
several times more parameters. In this paper, a more efficient solution named
frequency-aware convolution (FAC) is proposed. In FAC, frequency-positional
information is encoded in a vector and added to the input spectrogram. To match
the amplitude of input, the encoding vector is scaled adaptively and
channel-independently. Experiments are carried out in the context of DCASE 2022
task 4, and the results demonstrate that FAC can achieve comparable performance
to that of FDY with only 515 additional parameters, while FDY requires 8.02
million additional parameters. The ablation study shows that scaling the
encoding vector adaptively and channel-independently is critical to the
performance of FAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fusion Method with Spatiotemporal Sequences and Relationship
  Learning for Valence-Arousal Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Gongpeng Zhao, Yongqi Wang, Zhihong Wei, Yang Zheng, Zerui Zhang, Zhongpeng Cai, Guochen Xie, Jichao Zhu, Wangyuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our approach for the VA (Valence-Arousal) estimation task
in the ABAW6 competition. We devised a comprehensive model by preprocessing
video frames and audio segments to extract visual and audio features. Through
the utilization of Temporal Convolutional Network (TCN) modules, we effectively
captured the temporal and spatial correlations between these features.
Subsequently, we employed a Transformer encoder structure to learn long-range
dependencies, thereby enhancing the model's performance and generalization
ability. Our method leverages a multimodal data fusion approach, integrating
pre-trained audio and video backbones for feature extraction, followed by
TCN-based spatiotemporal encoding and Transformer-based temporal information
capture. Experimental results demonstrate the effectiveness of our approach,
achieving competitive performance in VA estimation on the AffWild2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MusicHiFi: Fast High-Fidelity Stereo Vocoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Juan-Pablo Caceres, Zhiyao Duan, Nicholas J. Bryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based audio and music generation models commonly generate music by
constructing an image representation of audio (e.g., a mel-spectrogram) and
then converting it to audio using a phase reconstruction model or vocoder.
Typical vocoders, however, produce monophonic audio at lower resolutions (e.g.,
16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an
efficient high-fidelity stereophonic vocoder. Our method employs a cascade of
three generative adversarial networks (GANs) that convert low-resolution
mel-spectrograms to audio, upsamples to high-resolution audio via bandwidth
expansion, and upmixes to stereophonic audio. Compared to previous work, we
propose 1) a unified GAN-based generator and discriminator architecture and
training procedure for each stage of our cascade, 2) a new fast, near
downsampling-compatible bandwidth extension module, and 3) a new fast
downmix-compatible mono-to-stereo upmixer that ensures the preservation of
monophonic content in the output. We evaluate our approach using both objective
and subjective listening tests and find our approach yields comparable or
better audio quality, better spatialization control, and significantly faster
inference speed compared to past work. Sound examples are at
https://MusicHiFi.github.io/web/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Whisper perform speech-based in-context learning? <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the in-context learning abilities of the Whisper
automatic speech recognition (ASR) models released by OpenAI. A novel
speech-based in-context learning (SICL) approach is proposed for test-time
adaptation, which can reduce the word error rates (WERs) with only a small
number of labelled speech samples without gradient descent. Language-level
adaptation experiments using Chinese dialects showed that when applying SICL to
isolated word ASR, consistent and considerable relative WER reductions can be
achieved using Whisper models of any size on two dialects, which is on average
32.3%. A k-nearest-neighbours-based in-context example selection technique can
be applied to further improve the efficiency of SICL, which can increase the
average relative WER reduction to 36.4%. The findings are verified using
speaker adaptation or continuous speech recognition tasks, and both achieved
considerable relative WER reductions. Detailed quantitative analyses are also
provided to shed light on SICL's adaptability to phonological variances and
dialect-specific lexical nuances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Aggregation for CTC-based Speech Recognition <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Fang, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper works on non-autoregressive automatic speech recognition. A
unimodal aggregation (UMA) is proposed to segment and integrate the feature
frames that belong to the same text token, and thus to learn better feature
representations for text tokens. The frame-wise features and weights are both
derived from an encoder. Then, the feature frames with unimodal weights are
integrated and further processed by a decoder. Connectionist temporal
classification (CTC) loss is applied for training. Compared to the regular CTC,
the proposed method learns better feature representations and shortens the
sequence length, resulting in lower recognition error and computational
complexity. Experiments on three Mandarin datasets show that UMA demonstrates
superior or comparable performance to other advanced non-autoregressive
methods, such as self-conditioned CTC. Moreover, by integrating
self-conditioned CTC into the proposed framework, the performance can be
further noticeably improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoDia: A New Dataset for Romanian Dialect Identification from Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RoDia, the first dataset for Romanian dialect identification
from speech. The RoDia dataset includes a varied compilation of speech samples
from five distinct regions of Romania, covering both urban and rural
environments, totaling 2 hours of manually annotated speech data. Along with
our dataset, we introduce a set of competitive models to be used as baselines
for future research. The top scoring model achieves a macro F1 score of 59.83%
and a micro F1 score of 62.08%, indicating that the task is challenging. We
thus believe that RoDia is a valuable resource that will stimulate research
aiming to address the challenges of Romanian dialect identification. We release
our dataset at https://github.com/codrut2/RoDia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gammatonegram Representation for End-to-End Dysarthric Speech Processing
  Tasks: Speech Recognition, Speaker Identification, and Intelligibility
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Farhadipour, Hadi Veisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dysarthria is a disability that causes a disturbance in the human speech
system and reduces the quality and intelligibility of a person's speech.
Because of this effect, the normal speech processing systems can not work
properly on impaired speech. This disability is usually associated with
physical disabilities. Therefore, designing a system that can perform some
tasks by receiving voice commands in the smart home can be a significant
achievement. In this work, we introduce gammatonegram as an effective method to
represent audio files with discriminative details, which is used as input for
the convolutional neural network. On the other word, we convert each speech
file into an image and propose image recognition system to classify speech in
different scenarios. Proposed CNN is based on the transfer learning method on
the pre-trained Alexnet. In this research, the efficiency of the proposed
system for speech recognition, speaker identification, and intelligibility
assessment is evaluated. According to the results on the UA dataset, the
proposed speech recognition system achieved 91.29% accuracy in
speaker-dependent mode, the speaker identification system acquired 87.74%
accuracy in text-dependent mode, and the intelligibility assessment system
achieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network
speech recognition system that works fully automatically. This system is
located in a cascade arrangement with the two-class intelligibility assessment
system, and the output of this system activates each one of the speech
recognition networks. This architecture achieves an accuracy of 92.3% WRR. The
source code of this paper is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures. Iran J Comput Sci (2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing
  Using Discrete Speech Unit Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wataru Nakata, Kazuki Yamauchi, Dong Yang, Hiroaki Hyodo, Yuki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present UTDUSS, the UTokyo-SaruLab system submitted to Interspeech2024
Speech Processing Using Discrete Speech Unit Challenge. The challenge focuses
on using discrete speech unit learned from large speech corpora for some tasks.
We submitted our UTDUSS system to two text-to-speech tracks: Vocoder and
Acoustic+Vocoder. Our system incorporates neural audio codec (NAC) pre-trained
on only speech corpora, which makes the learned codec represent rich acoustic
features that are necessary for high-fidelity speech reconstruction. For the
acoustic+vocoder track, we trained an acoustic model based on Transformer
encoder-decoder that predicted the pre-trained NAC tokens from text input. We
describe our strategies to build these models, such as data selection,
downsampling, and hyper-parameter tuning. Our system ranked in second and first
for the Vocoder and Acoustic+Vocoder tracks, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Gnana Praveen, Jahangir Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal emotion recognition has recently gained a lot of attention since
it can leverage diverse and complementary relationships over multiple
modalities, such as audio, visual, and text. Most state-of-the-art methods for
multimodal fusion rely on recurrent networks or conventional attention
mechanisms that do not effectively leverage the complementary nature of the
modalities. In this paper, we focus on dimensional emotion recognition based on
the fusion of facial, vocal, and text modalities extracted from videos.
Specifically, we propose a recursive cross-modal attention (RCMA) to
effectively capture the complementary relationships across the modalities in a
recursive fashion. The proposed model is able to effectively capture the
inter-modal relationships by computing the cross-attention weights across the
individual modalities and the joint representation of the other two modalities.
To further improve the inter-modal relationships, the obtained attended
features of the individual modalities are again fed as input to the cross-modal
attention to refine the feature representations of the individual modalities.
In addition to that, we have used Temporal convolution networks (TCNs) to
capture the temporal modeling (intra-modal relationships) of the individual
modalities. By deploying the TCNs as well cross-modal attention in a recursive
fashion, we are able to effectively capture both intra- and inter-modal
relationships across the audio, visual, and text modalities. Experimental
results on validation-set videos from the AffWild2 dataset indicate that our
proposed fusion model is able to achieve significant improvement over the
baseline for the sixth challenge of Affective Behavior Analysis in-the-Wild
2024 (ABAW6) competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.09068;
  text overlap with arXiv:2203.14779 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vibration Sensitivity of one-port and two-port MEMS microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francis Doyon-D'Amour, Carly Stalder, Timothy Hodges, Michel Stephan, Lixiue Wu, Triantafillos Koukoulas, Stephane Leahy, Raphael St-Gelais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro-electro-mechanical system (MEMS) microphones (mics) with two acoustic
ports are currently receiving considerable interest, with the promise of
achieving higher directional sensitivity compared to traditional one-port
architectures. However, measuring pressure differences in two-port microphones
typically commands sensing elements that are softer than in one-port mics, and
are therefore presumably more prone to interference from external vibration.
Here we derive a universal expression for microphone sensitivity to vibration
and we experimentally demonstrate its validity for several emerging two-port
microphone technologies. We also perform vibration measurements on a one-port
mic, thus providing a one-stop direct comparison between one-port and two-port
sensing approaches. We find that the acoustically-referred vibration
sensitivity of two-port MEMS mics, in units of measured acoustic pressure per
external acceleration (i.e., Pascals per g), does not depend on the sensing
element stiffness nor on its natural frequency. We also show that this
vibration sensitivity in two-port mics is inversely proportional to frequency
as opposed to the frequency independent behavior observed in one-port mics.
This is confirmed experimentally for several types of microphone packages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BanglaNum -- A Public Dataset for Bengali Digit Recognition from Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mir Sayeed Mohammad, Azizul Zahid, Md Asif Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) converts the human voice into readily
understandable and categorized text or words. Although Bengali is one of the
most widely spoken languages in the world, there have been very few studies on
Bengali ASR, particularly on Bangladeshi-accented Bengali. In this study, audio
recordings of spoken digits (0-9) from university students were used to create
a Bengali speech digits dataset that may be employed to train artificial neural
networks for voice-based digital input systems. This paper also compares the
Bengali digit recognition accuracy of several Convolutional Neural Networks
(CNNs) using spectrograms and shows that a test accuracy of 98.23% is
achievable using parameter-efficient models such as SqueezeNet on our dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Long-Content Speech Recognition With Factorized Neural
  Transducer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Gong, Yu Wu, Jinyu Li, Shujie Liu, Rui Zhao, Xie Chen, Yanmin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose two novel approaches, which integrate long-content
information into the factorized neural transducer (FNT) based architecture in
both non-streaming (referred to as LongFNT ) and streaming (referred to as
SLongFNT ) scenarios. We first investigate whether long-content transcriptions
can improve the vanilla conformer transducer (C-T) models. Our experiments
indicate that the vanilla C-T models do not exhibit improved performance when
utilizing long-content transcriptions, possibly due to the predictor network of
C-T models not functioning as a pure language model. Instead, FNT shows its
potential in utilizing long-content information, where we propose the LongFNT
model and explore the impact of long-content information in both text
(LongFNT-Text) and speech (LongFNT-Speech). The proposed LongFNT-Text and
LongFNT-Speech models further complement each other to achieve better
performance, with transcription history proving more valuable to the model. The
effectiveness of our LongFNT approach is evaluated on LibriSpeech and
GigaSpeech corpora, and obtains relative 19% and 12% word error rate reduction,
respectively. Furthermore, we extend the LongFNT model to the streaming
scenario, which is named SLongFNT , consisting of SLongFNT-Text and
SLongFNT-Speech approaches to utilize long-content text and speech information.
Experiments show that the proposed SLongFNT model achieves relative 26% and 17%
WER reduction on LibriSpeech and GigaSpeech respectively while keeping a good
latency, compared to the FNT baseline. Overall, our proposed LongFNT and
SLongFNT highlight the significance of considering long-content speech and
transcription knowledge for improving both non-streaming and streaming speech
recognition systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TASLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KunquDB: An Attempt for Speaker Verification in the Chinese Opera
  Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huali Zhou, Yuke Lin, Dong Liu, Ming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work aims to promote Chinese opera research in both musical and speech
domains, with a primary focus on overcoming the data limitations. We introduce
KunquDB, a relatively large-scale, well-annotated audio-visual dataset
comprising 339 speakers and 128 hours of content. Originating from the Kunqu
Opera Art Canon (Kunqu yishu dadian), KunquDB is meticulously structured by
dialogue lines, providing explicit annotations including character names,
speaker names, gender information, vocal manner classifications, and
accompanied by preliminary text transcriptions. KunquDB provides a versatile
foundation for role-centric acoustic studies and advancements in speech-related
research, including Automatic Speaker Verification (ASV). Beyond enriching
opera research, this dataset bridges the gap between artistic expression and
technological innovation. Pioneering the exploration of ASV in Chinese opera,
we construct four test trials considering two distinct vocal manners in opera
voices: stage speech (ST) and singing (S). Implementing domain adaptation
methods effectively mitigates domain mismatches induced by these vocal manner
variations while there is still room for further improvement as a benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building speech corpus with diverse voice characteristics for its
  prompt-based representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aya Watanabe, Shinnosuke Takamichi, Yuki Saito, Wataru Nakata, Detai Xin, Hiroshi Saruwatari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In text-to-speech synthesis, the ability to control voice characteristics is
vital for various applications. By leveraging thriving text prompt-based
generation techniques, it should be possible to enhance the nuanced control of
voice characteristics. While previous research has explored the prompt-based
manipulation of voice characteristics, most studies have used pre-recorded
speech, which limits the diversity of voice characteristics available. Thus, we
aim to address this gap by creating a novel corpus and developing a model for
prompt-based manipulation of voice characteristics in text-to-speech synthesis,
facilitating a broader range of voice characteristics. Specifically, we propose
a method to build a sizable corpus pairing voice characteristics descriptions
with corresponding speech samples. This involves automatically gathering
voice-related speech data from the Internet, ensuring its quality, and manually
annotating it using crowdsourcing. We implement this method with Japanese
language data and analyze the results to validate its effectiveness.
Subsequently, we propose a construction method of the model to retrieve speech
from voice characteristics descriptions based on a contrastive learning method.
We train the model using not only conservative contrastive learning but also
feature prediction learning to predict quantitative speech features
corresponding to voice characteristics. We evaluate the model performance via
experiments with the corpus we constructed above.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing. arXiv admin note: text overlap with arXiv:2309.13509</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration
  Transducer <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xi, Hao Li, Baochen Yang, Haoyu Li, Hainan Xu, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing an efficient keyword spotting (KWS) system that delivers
exceptional performance on resource-constrained edge devices has long been a
subject of significant attention. Existing KWS search algorithms typically
follow a frame-synchronous approach, where search decisions are made repeatedly
at each frame despite the fact that most frames are keyword-irrelevant. In this
paper, we propose TDT-KWS, which leverages token-and-duration Transducers (TDT)
for KWS tasks. We also propose a novel KWS task-specific decoding algorithm for
Transducer-based models, which supports highly effective frame-asynchronous
keyword search in streaming speech scenarios. With evaluations conducted on
both the public Hey Snips and self-constructed LibriKWS-20 datasets, our
proposed KWS-decoding algorithm produces more accurate results than
conventional ASR decoding algorithms. Additionally, TDT-KWS achieves on-par or
better wake word detection performance than both RNN-T and traditional TDT-ASR
systems while achieving significant inference speed-up. Furthermore,
experiments show that TDT-KWS is more robust to noisy environments compared to
RNN-T KWS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Onset and offset weighted loss function for sound event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a typical sound event detection (SED) system, the existence of a sound
event is detected at a frame level, and consecutive frames with the same event
detected are combined as one sound event. The median filter is applied as a
post-processing step to remove detection errors as much as possible. However,
detection errors occurring around the onset and offset of a sound event are
beyond the capacity of the median filter. To address this issue, an onset and
offset weighted binary cross-entropy (OWBCE) loss function is proposed in this
paper, which trains the DNN model to be more robust on frames around (a) onsets
and offsets. Experiments are carried out in the context of DCASE 2022 task 4.
Results show that OWBCE outperforms BCE when different models are considered.
For a basic CRNN, relative improvements of 6.43% in event-F1, 1.96% in PSDS1,
and 2.43% in PSDS2 can be achieved by OWBCE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Author Classification Using Parsed Language Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Todd K Moon, Jacob H. Gunther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years there has been ongoing interest in detecting authorship of a
text based on statistical properties of the text, such as by using occurrence
rates of noncontextual words. In previous work, these techniques have been
used, for example, to determine authorship of all of \emph{The Federalist
Papers}. Such methods may be useful in more modern times to detect fake or AI
authorship. Progress in statistical natural language parsers introduces the
possibility of using grammatical structure to detect authorship. In this paper
we explore a new possibility for detecting authorship using grammatical
structural information extracted using a statistical natural language parser.
This paper provides a proof of concept, testing author classification based on
grammatical structure on a set of "proof texts," The Federalist Papers and
Sanditon which have been as test cases in previous authorship detection
studies. Several features extracted from the statistical natural language
parser were explored: all subtrees of some depth from any level; rooted
subtrees of some depth, part of speech, and part of speech by level in the
parse tree. It was found to be helpful to project the features into a lower
dimensional space. Statistical experiments on these documents demonstrate that
information from a statistical parser can, in fact, assist in distinguishing
authors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-aware convolution for sound event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In sound event detection (SED), convolution neural networks (CNNs) are widely
used to extract time-frequency patterns from the input spectrogram. However,
features extracted by CNN can be insensitive to the shift of time-frequency
patterns along the frequency axis. To address this issue, frequency dynamic
convolution (FDY) has been proposed, which applies different kernels to
different frequency components. Compared to the vannila CNN, FDY requires
several times more parameters. In this paper, a more efficient solution named
frequency-aware convolution (FAC) is proposed. In FAC, frequency-positional
information is encoded in a vector and added to the input spectrogram. To match
the amplitude of input, the encoding vector is scaled adaptively and
channel-independently. Experiments are carried out in the context of DCASE 2022
task 4, and the results demonstrate that FAC can achieve comparable performance
to that of FDY with only 515 additional parameters, while FDY requires 8.02
million additional parameters. The ablation study shows that scaling the
encoding vector adaptively and channel-independently is critical to the
performance of FAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually Grounded Speech Models have a Mutual Exclusivity Bias <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leanne Nortje, Dan Oneaţă, Yevgen Matusevych, Herman Kamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When children learn new words, they employ constraints such as the mutual
exclusivity (ME) bias: a novel word is mapped to a novel object rather than a
familiar one. This bias has been studied computationally, but only in models
that use discrete word representations as input, ignoring the high variability
of spoken words. We investigate the ME bias in the context of visually grounded
speech models that learn from natural images and continuous speech audio.
Concretely, we train a model on familiar words and test its ME bias by asking
it to select between a novel and a familiar object when queried with a novel
word. To simulate prior acoustic and visual knowledge, we experiment with
several initialisation strategies using pretrained speech and vision networks.
Our findings reveal the ME bias across the different initialisation approaches,
with a stronger bias in models with more prior (in particular, visual)
knowledge. Additional tests confirm the robustness of our results, even when
different loss functions are considered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL, pre-MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Isometric Neural Machine Translation using Phoneme Count Ratio
  Reward-based Reinforcement Learning <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Ratnakant Mhaskar, Nirmesh J. Shah, Mohammadi Zaki, Ashishkumar P. Gudmalwar, Pankaj Wasnik, Rajiv Ratn Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Automatic Video Dubbing (AVD) pipeline consists of three key
modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation
(NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms
are employed to regulate the length of the synthesized output text. This is
done to guarantee synchronization with respect to the alignment of video and
audio subsequent to the dubbing process. Previous approaches have focused on
aligning the number of characters and words in the source and target language
texts of Machine Translation models. However, our approach aims to align the
number of phonemes instead, as they are closely associated with speech
duration. In this paper, we present the development of an isometric NMT system
using Reinforcement Learning (RL), with a focus on optimizing the alignment of
phoneme counts in the source and target language sentence pairs. To evaluate
our models, we propose the Phoneme Count Compliance (PCC) score, which is a
measure of length compliance. Our approach demonstrates a substantial
improvement of approximately 36% in the PCC score compared to the
state-of-the-art models when applied to English-Hindi language pairs. Moreover,
we propose a student-teacher architecture within the framework of our RL
approach to maintain a trade-off between the phoneme count and translation
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NAACL2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fusion Method with Spatiotemporal Sequences and Relationship
  Learning for Valence-Arousal Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Gongpeng Zhao, Yongqi Wang, Zhihong Wei, Yang Zheng, Zerui Zhang, Zhongpeng Cai, Guochen Xie, Jichao Zhu, Wangyuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our approach for the VA (Valence-Arousal) estimation task
in the ABAW6 competition. We devised a comprehensive model by preprocessing
video frames and audio segments to extract visual and audio features. Through
the utilization of Temporal Convolutional Network (TCN) modules, we effectively
captured the temporal and spatial correlations between these features.
Subsequently, we employed a Transformer encoder structure to learn long-range
dependencies, thereby enhancing the model's performance and generalization
ability. Our method leverages a multimodal data fusion approach, integrating
pre-trained audio and video backbones for feature extraction, followed by
TCN-based spatiotemporal encoding and Transformer-based temporal information
capture. Experimental results demonstrate the effectiveness of our approach,
achieving competitive performance in VA estimation on the AffWild2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SwG-former: A Sliding-Window Graph Convolutional Network for
  Simultaneous Spatial-Temporal Information Extraction in Sound Event
  Localization and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Huang, Qinghua Huang, Liyan Ma, Chuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound event localization and detection (SELD) involves sound event detection
(SED) and direction of arrival (DoA) estimation tasks. SED mainly relies on
temporal dependencies to distinguish different sound classes, while DoA
estimation depends on spatial correlations to estimate source directions. This
paper addresses the need to simultaneously extract spatial-temporal information
in audio signals to improve SELD performance. A novel block, the sliding-window
graph-former (SwG-former), is designed to learn temporal context information of
sound events based on their spatial correlations. The SwG-former block
transforms audio signals into a graph representation and constructs graph
vertices to capture higher abstraction levels for spatial correlations. It uses
different-sized sliding windows to adapt various sound event durations and
aggregates temporal features with similar spatial information while
incorporating multi-head self-attention (MHSA) to model global information.
Furthermore, as the cornerstone of message passing, a robust Conv2dAgg function
is proposed and embedded into the block to aggregate the features of neighbor
vertices. As a result, a SwG-former model, which stacks the SwG-former blocks,
demonstrates superior performance compared to recent advanced SELD models. The
SwG-former block is also integrated into the event-independent network version
2 (EINV2), called SwG-EINV2, which surpasses the state-of-the-art (SOTA)
methods under the same acoustic environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MusicHiFi: Fast High-Fidelity Stereo Vocoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Juan-Pablo Caceres, Zhiyao Duan, Nicholas J. Bryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based audio and music generation models commonly generate music by
constructing an image representation of audio (e.g., a mel-spectrogram) and
then converting it to audio using a phase reconstruction model or vocoder.
Typical vocoders, however, produce monophonic audio at lower resolutions (e.g.,
16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an
efficient high-fidelity stereophonic vocoder. Our method employs a cascade of
three generative adversarial networks (GANs) that convert low-resolution
mel-spectrograms to audio, upsamples to high-resolution audio via bandwidth
expansion, and upmixes to stereophonic audio. Compared to previous work, we
propose 1) a unified GAN-based generator and discriminator architecture and
training procedure for each stage of our cascade, 2) a new fast, near
downsampling-compatible bandwidth extension module, and 3) a new fast
downmix-compatible mono-to-stereo upmixer that ensures the preservation of
monophonic content in the output. We evaluate our approach using both objective
and subjective listening tests and find our approach yields comparable or
better audio quality, better spatialization control, and significantly faster
inference speed compared to past work. Sound examples are at
https://MusicHiFi.github.io/web/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Whisper perform speech-based in-context learning? <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the in-context learning abilities of the Whisper
automatic speech recognition (ASR) models released by OpenAI. A novel
speech-based in-context learning (SICL) approach is proposed for test-time
adaptation, which can reduce the word error rates (WERs) with only a small
number of labelled speech samples without gradient descent. Language-level
adaptation experiments using Chinese dialects showed that when applying SICL to
isolated word ASR, consistent and considerable relative WER reductions can be
achieved using Whisper models of any size on two dialects, which is on average
32.3%. A k-nearest-neighbours-based in-context example selection technique can
be applied to further improve the efficiency of SICL, which can increase the
average relative WER reduction to 36.4%. The findings are verified using
speaker adaptation or continuous speech recognition tasks, and both achieved
considerable relative WER reductions. Detailed quantitative analyses are also
provided to shed light on SICL's adaptability to phonological variances and
dialect-specific lexical nuances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Aggregation for CTC-based Speech Recognition <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Fang, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper works on non-autoregressive automatic speech recognition. A
unimodal aggregation (UMA) is proposed to segment and integrate the feature
frames that belong to the same text token, and thus to learn better feature
representations for text tokens. The frame-wise features and weights are both
derived from an encoder. Then, the feature frames with unimodal weights are
integrated and further processed by a decoder. Connectionist temporal
classification (CTC) loss is applied for training. Compared to the regular CTC,
the proposed method learns better feature representations and shortens the
sequence length, resulting in lower recognition error and computational
complexity. Experiments on three Mandarin datasets show that UMA demonstrates
superior or comparable performance to other advanced non-autoregressive
methods, such as self-conditioned CTC. Moreover, by integrating
self-conditioned CTC into the proposed framework, the performance can be
further noticeably improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoDia: A New Dataset for Romanian Dialect Identification from Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RoDia, the first dataset for Romanian dialect identification
from speech. The RoDia dataset includes a varied compilation of speech samples
from five distinct regions of Romania, covering both urban and rural
environments, totaling 2 hours of manually annotated speech data. Along with
our dataset, we introduce a set of competitive models to be used as baselines
for future research. The top scoring model achieves a macro F1 score of 59.83%
and a micro F1 score of 62.08%, indicating that the task is challenging. We
thus believe that RoDia is a valuable resource that will stimulate research
aiming to address the challenges of Romanian dialect identification. We release
our dataset at https://github.com/codrut2/RoDia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gammatonegram Representation for End-to-End Dysarthric Speech Processing
  Tasks: Speech Recognition, Speaker Identification, and Intelligibility
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Farhadipour, Hadi Veisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dysarthria is a disability that causes a disturbance in the human speech
system and reduces the quality and intelligibility of a person's speech.
Because of this effect, the normal speech processing systems can not work
properly on impaired speech. This disability is usually associated with
physical disabilities. Therefore, designing a system that can perform some
tasks by receiving voice commands in the smart home can be a significant
achievement. In this work, we introduce gammatonegram as an effective method to
represent audio files with discriminative details, which is used as input for
the convolutional neural network. On the other word, we convert each speech
file into an image and propose image recognition system to classify speech in
different scenarios. Proposed CNN is based on the transfer learning method on
the pre-trained Alexnet. In this research, the efficiency of the proposed
system for speech recognition, speaker identification, and intelligibility
assessment is evaluated. According to the results on the UA dataset, the
proposed speech recognition system achieved 91.29% accuracy in
speaker-dependent mode, the speaker identification system acquired 87.74%
accuracy in text-dependent mode, and the intelligibility assessment system
achieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network
speech recognition system that works fully automatically. This system is
located in a cascade arrangement with the two-class intelligibility assessment
system, and the output of this system activates each one of the speech
recognition networks. This architecture achieves an accuracy of 92.3% WRR. The
source code of this paper is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures. Iran J Comput Sci (2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-19T00:00:00Z">2024-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducing the Acoustic Velocity Vectors in a Circular Listening Area 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Wang, Thushara Abhayapala, Jihui Aimee Zhang, Prasanga Samarasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic velocity vectors are important for human's localization of sound at
low frequencies. This paper proposes a sound field reproduction algorithm,
which matches the acoustic velocity vectors in a circular listening area. In
previous work, acoustic velocity vectors are matched either at sweet spots or
on the boundary of the listening area. Sweet spots restrict listener's
movement, whereas measuring the acoustic velocity vectors on the boundary
requires complicated measurement setup. This paper proposes the cylindrical
harmonic coefficients of the acoustic velocity vectors in a circular area (CHV
coefficients), which are calculated from the cylindrical harmonic coefficients
of the global pressure (global CHP coefficients) by using the sound field
translation formula. The global CHP coefficients can be measured by a circular
microphone array, which can be bought off-the-shelf. By matching the CHV
coefficients, the acoustic velocity vectors are reproduced throughout the
listening area. Hence, listener's movements are allowed. Simulations show that
at low frequency, where the acoustic velocity vectors are the dominant factor
for localization, the proposed reproduction method based on the CHV
coefficients results in higher accuracy in reproduced acoustic velocity vectors
when compared with traditional method based on the global CHP coefficients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Speech Extraction Using Spatially Regularized Independent
  Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix
  Estimation <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Ishikawa, Kohei Konaka, Tomohiko Nakamura, Norihiro Takamune, Hiroshi Saruwatari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time speech extraction is an important challenge with various
applications such as speech recognition in a human-like avatar/robot. In this
paper, we propose the real-time extension of a speech extraction method based
on independent low-rank matrix analysis (ILRMA) and rank-constrained spatial
covariance matrix estimation (RCSCME). The RCSCME-based method is a
multichannel blind speech extraction method that demonstrates superior speech
extraction performance in diffuse noise environments. To improve the
performance, we introduce spatial regularization into the ILRMA part of the
RCSCME-based speech extraction and design two regularizers. Speech extraction
experiments demonstrated that the proposed methods can function in real time
and the designed regularizers improve the speech extraction performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, accepted at HSCMA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSLM-S2ST: A Multitask Speech Language Model for Textless
  Speech-to-Speech Translation with Speaker Style Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, Hongyu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been emerging research interest and advances in speech-to-speech
translation (S2ST), translating utterances from one language to another. This
work proposes Multitask Speech Language Model (MSLM), which is a decoder-only
speech language model trained in a multitask setting. Without reliance on text
training data, our model is able to support multilingual S2ST with speaker
style preserved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Speech Language Models for Prompt-Conditioned
  <span class="highlight-title">Speech Synthesis</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, Hongyu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech language models (LMs) are promising for high-quality speech synthesis
through in-context learning. A typical speech LM takes discrete semantic units
as content and a short utterance as prompt, and synthesizes speech which
preserves the content's semantics but mimics the prompt's style. However, there
is no systematic understanding on how the synthesized audio is controlled by
the prompt and content. In this work, we conduct an empirical study of the
widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and
provide insights into the prompt design and content semantic units. Our
analysis reveals that heterogeneous and nonstationary prompts hurt the audio
quality in contrast to the previous finding that longer prompts always lead to
better synthesis. Moreover, we find that the speaker style of the synthesized
audio is also affected by the content in addition to the prompt. We further
show that semantic units carry rich acoustic information such as pitch, tempo,
volume and speech emphasis, which might be leaked from the content to the
synthesized audio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Listenable Maps for Audio Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Paissan, Mirco Ravanelli, Cem Subakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive performance of deep learning models across diverse
tasks, their complexity poses challenges for interpretation. This challenge is
particularly evident for audio signals, where conveying interpretations becomes
inherently difficult. To address this issue, we introduce Listenable Maps for
Audio Classifiers (L-MAC), a posthoc interpretation method that generates
faithful and listenable interpretations. L-MAC utilizes a decoder on top of a
pretrained classifier to generate binary masks that highlight relevant portions
of the input audio. We train the decoder with a special loss that maximizes the
confidence of the classifier decision on the masked-in portion of the audio
while minimizing the probability of model output for the masked-out portion.
Quantitative evaluations on both in-domain and out-of-domain data demonstrate
that L-MAC consistently produces more faithful interpretations than several
gradient and masking-based methodologies. Furthermore, a user study confirms
that, on average, users prefer the interpretations generated by the proposed
technique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of
  Complex Physical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Spitznagel, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven modeling of complex physical systems is receiving a growing
amount of attention in the simulation and machine learning communities. Since
most physical simulations are based on compute-intensive, iterative
implementations of differential equation systems, a (partial) replacement with
learned, 1-step inference models has the potential for significant speedups in
a wide range of application areas. In this context, we present a novel
benchmark for the evaluation of 1-step generative learning models in terms of
speed and physical correctness. Our Urban Sound Propagation benchmark is based
on the physically complex and practically relevant, yet intuitively easy to
grasp task of modeling the 2d propagation of waves from a sound source in an
urban environment. We provide a dataset with 100k samples, where each sample
consists of pairs of real 2d building maps drawn from OpenStreetmap, a
parameterized sound source, and a simulated ground truth sound propagation for
the given scene. The dataset provides four different simulation tasks with
increasing complexity regarding reflection, diffraction and source variance. A
first baseline evaluation of common generative U-Net, GAN and Diffusion models
shows, that while these models are very well capable of modeling sound
propagations in simple cases, the approximation of sub-systems represented by
higher order equations systematically fails. Information about the dataset,
download instructions and source codes are provided on our website:
https://www.urban-sound-data.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Microphone Array Calibration using Hybrid TDOA Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjie Zhang, Jiang Wang, He Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asynchronous microphone array calibration is a prerequisite for most audition
robot applications. A popular solution to the above calibration problem is the
batch form of Simultaneous Localisation and Mapping (SLAM), using the time
difference of arrival measurements between two microphones (TDOA-M), and the
robot (which serves as a moving sound source during calibration) odometry
information. In this paper, we introduce a new form of measurement for
microphone array calibration, i.e. the time difference of arrival between
adjacent sound events (TDOA-S) with respect to the microphone channels. We
propose to combine TDOA-S and TDOA-M, called hybrid TDOA, together with
odometry measurements for bath SLAM-based calibration of asynchronous
microphone arrays. Simulation and real-world experiment results consistently
show that our method is more independent of microphone number, less sensitive
to initial values (when using off-the-shelf algorithms such as Gauss-Newton
iterations), and has better calibration accuracy and robustness under various
TDOA noises. In addition, the simulation result demonstrates that our method
has a lower Cram\'er-Rao lower bound (CRLB) for microphone parameters. To
benefit the community, we open-source our code and data at
https://github.com/zcj808/Hybrid-TDOA-Calib.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music Source Separation Based on a Lightweight Deep Learning Framework
  (DTTNET: DUAL-PATH TFC-TDF UNET) <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Chen, Susmitha Vekkot, Pancham Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music source separation (MSS) aims to extract 'vocals', 'drums', 'bass' and
'other' tracks from a piece of mixed music. While deep learning methods have
shown impressive results, there is a trend toward larger models. In our paper,
we introduce a novel and lightweight architecture called DTTNet, which is based
on Dual-Path Module and Time-Frequency Convolutions Time-Distributed
Fully-connected UNet (TFC-TDF UNet). DTTNet achieves 10.12 dB cSDR on 'vocals'
compared to 10.01 dB reported for Bandsplit RNN (BSRNN) but with 86.7% fewer
parameters. We also assess pattern-specific performance and model
generalization for intricate audio patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ICASSP 2024. Additional experiments can be found in the
  published version on IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual
  Representation Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual representation learning aims to develop systems with human-like
perception by utilizing correlation between auditory and visual information.
However, current models often focus on a limited set of tasks, and
generalization abilities of learned representations are unclear. To this end,
we propose the AV-SUPERB benchmark that enables general-purpose evaluation of
unimodal audio/visual and bimodal fusion representations on 7 datasets covering
5 audio-visual tasks in speech and audio processing. We evaluate 5 recent
self-supervised models and show that none of these models generalize to all
tasks, emphasizing the need for future study on improving universal model
performance. In addition, we show that representations may be improved with
intermediate-task fine-tuning and audio event classification with AudioSet
serves as a strong intermediate task. We release our benchmark with evaluation
code and a model submission platform to encourage further research in
audio-visual learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024; Evaluation Code:
  https://github.com/roger-tseng/av-superb Submission Platform:
  https://av.superbbenchmark.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Feature Extraction and Late Fusion Strategy for Audiovisual
  Emotional Mimicry Intensity Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Wangyuan Zhu, Jichao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the solution to the Emotional Mimicry Intensity
(EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis
in-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to
evaluate the emotional intensity of seed videos by assessing them from a set of
predefined emotion categories (i.e., "Admiration", "Amusement",
"Determination", "Empathic Pain", "Excitement" and "Joy"). To tackle this
challenge, we extracted rich dual-channel visual features based on ResNet18 and
AUs for the video modality and effective single-channel features based on
Wav2Vec2.0 for the audio modality. This allowed us to obtain comprehensive
emotional features for the audiovisual modality. Additionally, leveraging a
late fusion strategy, we averaged the predictions of the visual and acoustic
models, resulting in a more accurate estimation of audiovisual emotional
mimicry intensity. Experimental results validate the effectiveness of our
approach, with the average Pearson's correlation Coefficient($\rho$) across the
6 emotion dimensionson the validation set achieving 0.3288.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducing the Acoustic Velocity Vectors in a Circular Listening Area 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Wang, Thushara Abhayapala, Jihui Aimee Zhang, Prasanga Samarasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic velocity vectors are important for human's localization of sound at
low frequencies. This paper proposes a sound field reproduction algorithm,
which matches the acoustic velocity vectors in a circular listening area. In
previous work, acoustic velocity vectors are matched either at sweet spots or
on the boundary of the listening area. Sweet spots restrict listener's
movement, whereas measuring the acoustic velocity vectors on the boundary
requires complicated measurement setup. This paper proposes the cylindrical
harmonic coefficients of the acoustic velocity vectors in a circular area (CHV
coefficients), which are calculated from the cylindrical harmonic coefficients
of the global pressure (global CHP coefficients) by using the sound field
translation formula. The global CHP coefficients can be measured by a circular
microphone array, which can be bought off-the-shelf. By matching the CHV
coefficients, the acoustic velocity vectors are reproduced throughout the
listening area. Hence, listener's movements are allowed. Simulations show that
at low frequency, where the acoustic velocity vectors are the dominant factor
for localization, the proposed reproduction method based on the CHV
coefficients results in higher accuracy in reproduced acoustic velocity vectors
when compared with traditional method based on the global CHP coefficients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Speech Extraction Using Spatially Regularized Independent
  Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix
  Estimation <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Ishikawa, Kohei Konaka, Tomohiko Nakamura, Norihiro Takamune, Hiroshi Saruwatari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time speech extraction is an important challenge with various
applications such as speech recognition in a human-like avatar/robot. In this
paper, we propose the real-time extension of a speech extraction method based
on independent low-rank matrix analysis (ILRMA) and rank-constrained spatial
covariance matrix estimation (RCSCME). The RCSCME-based method is a
multichannel blind speech extraction method that demonstrates superior speech
extraction performance in diffuse noise environments. To improve the
performance, we introduce spatial regularization into the ILRMA part of the
RCSCME-based speech extraction and design two regularizers. Speech extraction
experiments demonstrated that the proposed methods can function in real time
and the designed regularizers improve the speech extraction performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, accepted at HSCMA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSLM-S2ST: A Multitask Speech Language Model for Textless
  Speech-to-Speech Translation with Speaker Style Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, Hongyu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been emerging research interest and advances in speech-to-speech
translation (S2ST), translating utterances from one language to another. This
work proposes Multitask Speech Language Model (MSLM), which is a decoder-only
speech language model trained in a multitask setting. Without reliance on text
training data, our model is able to support multilingual S2ST with speaker
style preserved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Speech Language Models for Prompt-Conditioned
  <span class="highlight-title">Speech Synthesis</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, Hongyu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech language models (LMs) are promising for high-quality speech synthesis
through in-context learning. A typical speech LM takes discrete semantic units
as content and a short utterance as prompt, and synthesizes speech which
preserves the content's semantics but mimics the prompt's style. However, there
is no systematic understanding on how the synthesized audio is controlled by
the prompt and content. In this work, we conduct an empirical study of the
widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and
provide insights into the prompt design and content semantic units. Our
analysis reveals that heterogeneous and nonstationary prompts hurt the audio
quality in contrast to the previous finding that longer prompts always lead to
better synthesis. Moreover, we find that the speaker style of the synthesized
audio is also affected by the content in addition to the prompt. We further
show that semantic units carry rich acoustic information such as pitch, tempo,
volume and speech emphasis, which might be leaked from the content to the
synthesized audio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Listenable Maps for Audio Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Paissan, Mirco Ravanelli, Cem Subakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive performance of deep learning models across diverse
tasks, their complexity poses challenges for interpretation. This challenge is
particularly evident for audio signals, where conveying interpretations becomes
inherently difficult. To address this issue, we introduce Listenable Maps for
Audio Classifiers (L-MAC), a posthoc interpretation method that generates
faithful and listenable interpretations. L-MAC utilizes a decoder on top of a
pretrained classifier to generate binary masks that highlight relevant portions
of the input audio. We train the decoder with a special loss that maximizes the
confidence of the classifier decision on the masked-in portion of the audio
while minimizing the probability of model output for the masked-out portion.
Quantitative evaluations on both in-domain and out-of-domain data demonstrate
that L-MAC consistently produces more faithful interpretations than several
gradient and masking-based methodologies. Furthermore, a user study confirms
that, on average, users prefer the interpretations generated by the proposed
technique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of
  Complex Physical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Spitznagel, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven modeling of complex physical systems is receiving a growing
amount of attention in the simulation and machine learning communities. Since
most physical simulations are based on compute-intensive, iterative
implementations of differential equation systems, a (partial) replacement with
learned, 1-step inference models has the potential for significant speedups in
a wide range of application areas. In this context, we present a novel
benchmark for the evaluation of 1-step generative learning models in terms of
speed and physical correctness. Our Urban Sound Propagation benchmark is based
on the physically complex and practically relevant, yet intuitively easy to
grasp task of modeling the 2d propagation of waves from a sound source in an
urban environment. We provide a dataset with 100k samples, where each sample
consists of pairs of real 2d building maps drawn from OpenStreetmap, a
parameterized sound source, and a simulated ground truth sound propagation for
the given scene. The dataset provides four different simulation tasks with
increasing complexity regarding reflection, diffraction and source variance. A
first baseline evaluation of common generative U-Net, GAN and Diffusion models
shows, that while these models are very well capable of modeling sound
propagations in simple cases, the approximation of sub-systems represented by
higher order equations systematically fails. Information about the dataset,
download instructions and source codes are provided on our website:
https://www.urban-sound-data.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Microphone Array Calibration using Hybrid TDOA Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjie Zhang, Jiang Wang, He Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asynchronous microphone array calibration is a prerequisite for most audition
robot applications. A popular solution to the above calibration problem is the
batch form of Simultaneous Localisation and Mapping (SLAM), using the time
difference of arrival measurements between two microphones (TDOA-M), and the
robot (which serves as a moving sound source during calibration) odometry
information. In this paper, we introduce a new form of measurement for
microphone array calibration, i.e. the time difference of arrival between
adjacent sound events (TDOA-S) with respect to the microphone channels. We
propose to combine TDOA-S and TDOA-M, called hybrid TDOA, together with
odometry measurements for bath SLAM-based calibration of asynchronous
microphone arrays. Simulation and real-world experiment results consistently
show that our method is more independent of microphone number, less sensitive
to initial values (when using off-the-shelf algorithms such as Gauss-Newton
iterations), and has better calibration accuracy and robustness under various
TDOA noises. In addition, the simulation result demonstrates that our method
has a lower Cram\'er-Rao lower bound (CRLB) for microphone parameters. To
benefit the community, we open-source our code and data at
https://github.com/zcj808/Hybrid-TDOA-Calib.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music Source Separation Based on a Lightweight Deep Learning Framework
  (DTTNET: DUAL-PATH TFC-TDF UNET) <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Chen, Susmitha Vekkot, Pancham Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music source separation (MSS) aims to extract 'vocals', 'drums', 'bass' and
'other' tracks from a piece of mixed music. While deep learning methods have
shown impressive results, there is a trend toward larger models. In our paper,
we introduce a novel and lightweight architecture called DTTNet, which is based
on Dual-Path Module and Time-Frequency Convolutions Time-Distributed
Fully-connected UNet (TFC-TDF UNet). DTTNet achieves 10.12 dB cSDR on 'vocals'
compared to 10.01 dB reported for Bandsplit RNN (BSRNN) but with 86.7% fewer
parameters. We also assess pattern-specific performance and model
generalization for intricate audio patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ICASSP 2024. Additional experiments can be found in the
  published version on IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual
  Representation Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual representation learning aims to develop systems with human-like
perception by utilizing correlation between auditory and visual information.
However, current models often focus on a limited set of tasks, and
generalization abilities of learned representations are unclear. To this end,
we propose the AV-SUPERB benchmark that enables general-purpose evaluation of
unimodal audio/visual and bimodal fusion representations on 7 datasets covering
5 audio-visual tasks in speech and audio processing. We evaluate 5 recent
self-supervised models and show that none of these models generalize to all
tasks, emphasizing the need for future study on improving universal model
performance. In addition, we show that representations may be improved with
intermediate-task fine-tuning and audio event classification with AudioSet
serves as a strong intermediate task. We release our benchmark with evaluation
code and a model submission platform to encourage further research in
audio-visual learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024; Evaluation Code:
  https://github.com/roger-tseng/av-superb Submission Platform:
  https://av.superbbenchmark.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Feature Extraction and Late Fusion Strategy for Audiovisual
  Emotional Mimicry Intensity Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Wangyuan Zhu, Jichao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the solution to the Emotional Mimicry Intensity
(EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis
in-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to
evaluate the emotional intensity of seed videos by assessing them from a set of
predefined emotion categories (i.e., "Admiration", "Amusement",
"Determination", "Empathic Pain", "Excitement" and "Joy"). To tackle this
challenge, we extracted rich dual-channel visual features based on ResNet18 and
AUs for the video modality and effective single-channel features based on
Wav2Vec2.0 for the audio modality. This allowed us to obtain comprehensive
emotional features for the audiovisual modality. Additionally, leveraging a
late fusion strategy, we averaged the predictions of the visual and acoustic
models, resulting in a more accurate estimation of audiovisual emotional
mimicry intensity. Experimental results validate the effectiveness of our
approach, with the average Pearson's correlation Coefficient($\rho$) across the
6 emotion dimensionson the validation set achieving 0.3288.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-18T00:00:00Z">2024-03-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Multi-Source Inference for Text Conditioned Music <span class="highlight-title">Diffusion</span>
  Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Source Diffusion Models (MSDM) allow for compositional musical
generation tasks: generating a set of coherent sources, creating
accompaniments, and performing source separation. Despite their versatility,
they require estimating the joint distribution over the sources, necessitating
pre-separated musical data, which is rarely available, and fixing the number
and type of sources at training time. This paper generalizes MSDM to arbitrary
time-domain diffusion models conditioned on text embeddings. These models do
not require separated data as they are trained on mixtures, can parameterize an
arbitrary number of sources, and allow for rich semantic control. We propose an
inference procedure enabling the coherent generation of sources and
accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform
source separation. We experiment with diffusion models trained on Slakh2100 and
MTG-Jamendo, showcasing competitive generation and separation results in a
relaxed data setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QEAN: Quaternion-Enhanced Attention Network for Visual Dance <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizhen Zhou, Yejing Huo, Guoheng Huang, An Zeng, Xuhang Chen, Lian Huang, Zinuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of music-generated dance is a novel and challenging Image
generation task. It aims to input a piece of music and seed motions, then
generate natural dance movements for the subsequent music. Transformer-based
methods face challenges in time series prediction tasks related to human
movements and music due to their struggle in capturing the nonlinear
relationship and temporal aspects. This can lead to issues like joint
deformation, role deviation, floating, and inconsistencies in dance movements
generated in response to the music. In this paper, we propose a
Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a
quaternion perspective, which consists of a Spin Position Embedding (SPE)
module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds
position information into self-attention in a rotational manner, leading to
better learning of features of movement sequences and audio sequences, and
improved understanding of the connection between music and dance. Second, QRA
represents and fuses 3D motion features and audio features in the form of a
series of quaternions, enabling the model to better learn the temporal
coordination of music and dance under the complex temporal cycle conditions of
dance generation. Finally, we conducted experiments on the dataset AIST++, and
the results show that our approach achieves better and more robust performance
in generating accurate, high-quality dance movements. Our source code and
dataset can be available from https://github.com/MarasyZZ/QEAN and
https://google.github.io/aistplusplus_dataset respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Visual Computer Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-loudspeaker Binaural Room Impulse Response Dataset with
  High-Resolution Translational and Rotational Head Coordinates in a Listening
  Room 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Qiao, Ryan Miguel Gonzales, Edgar Choueiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data report for the 3D3A Lab Binaural Room Impulse Response (BRIR) Dataset
(https://doi.org/10.34770/6gc9-5787).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Frontiers in Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Shepardson, Jack Armitage, Thor Magnusson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based probabilistic models of musical data are producing
increasingly realistic results and promise to enter creative workflows of many
kinds. Yet they have been little-studied in a performance setting, where the
results of user actions typically ought to feel instantaneous. To enable such
study, we designed Notochord, a deep probabilistic model for sequences of
structured events, and trained an instance of it on the Lakh MIDI dataset. Our
probabilistic formulation allows interpretable interventions at a sub-event
level, which enables one model to act as a backbone for diverse interactive
musical functions including steerable generation, harmonization, machine
improvisation, and likelihood-based interfaces. Notochord can generate
polyphonic and multi-track MIDI, and respond to inputs with latency below ten
milliseconds. Training code, model checkpoints and interactive examples are
provided as open source software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures. Proceedings of the 3rd Conference on AI Music
  Creativity (2022, September 17)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sound Event Detection and Localization with Distance Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Aleksander Krause, Archontis Politis, Annamaria Mesaros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound Event Detection and Localization (SELD) is a combined task of
identifying sound events and their corresponding direction-of-arrival (DOA).
While this task has numerous applications and has been extensively researched
in recent years, it fails to provide full information about the sound source
position. In this paper, we overcome this problem by extending the task to
Sound Event Detection, Localization with Distance Estimation (3D SELD). We
study two ways of integrating distance estimation within the SELD core - a
multi-task approach, in which the problem is tackled by a separate model
output, and a single-task approach obtained by extending the multi-ACCDOA
method to include distance information. We investigate both methods for the
Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial
Soundscapes 2023. Moreover, our study involves experiments on the loss function
related to the distance estimation part. Our results show that it is possible
to perform 3D SELD without any degradation of performance in sound event
detection and DOA estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted for the 32nd European Signal Processing
  Conference EUSIPCO 2024 in Lyon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt-Singer: Controllable Singing-Voice-<span class="highlight-title">Synthesis</span> with Natural
  Language Prompt <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Wang, Ruofan Hu, Rongjie Huang, Zhiqing Hong, Ruiqi Li, Wenrui Liu, Fuming You, Tao Jin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio
quality and naturalness, yet they lack the capability to control the style
attributes of the synthesized singing explicitly. We propose Prompt-Singer, the
first SVS method that enables attribute controlling on singer gender, vocal
range and volume with natural language. We adopt a model architecture based on
a decoder-only transformer with a multi-scale hierarchy, and design a
range-melody decoupled pitch representation that enables text-conditioned vocal
range control while keeping melodic accuracy. Furthermore, we explore various
experiment settings, including different types of text representations, text
encoder fine-tuning, and introducing speech data to alleviate data scarcity,
aiming to facilitate further research. Experiments show that our model achieves
favorable controlling ability and audio quality. Audio samples are available at
http://prompt-singer.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards the Development of a Real-Time Deepfake Audio Detection System
  in Communication Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonat John Mathew, Rakin Ahsan, Sae Furukawa, Jagdish Gautham Krishna Kumar, Huzaifa Pallan, Agamjeet Singh Padda, Sara Adamski, Madhu Reddiboina, Arjun Pankajakshan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake audio poses a rising threat in communication platforms,
necessitating real-time detection for audio stream integrity. Unlike
traditional non-real-time approaches, this study assesses the viability of
employing static deepfake audio detection models in real-time communication
platforms. An executable software is developed for cross-platform
compatibility, enabling real-time execution. Two deepfake audio detection
models based on Resnet and LCNN architectures are implemented using the
ASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof
2019 challenge baselines. The study proposes strategies and frameworks for
enhancing these models, paving the way for real-time deepfake audio detection
in communication platforms. This work contributes to the advancement of audio
stream security, ensuring robust detection capabilities in dynamic, real-time
communication scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucination in Perceptual Metric-Driven Speech Enhancement Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the area of speech enhancement, there is an ongoing interest in the
creation of neural systems which explicitly aim to improve the perceptual
quality of the processed audio. In concert with this is the topic of
non-intrusive (i.e. without clean reference) speech quality prediction, for
which neural networks are trained to predict human-assigned quality labels
directly from distorted audio. When combined, these areas allow for the
creation of powerful new speech enhancement systems which can leverage large
real-world datasets of distorted audio, by taking inference of a pre-trained
speech quality predictor as the sole loss function of the speech enhancement
system. This paper aims to identify a potential pitfall with this approach,
namely hallucinations which are introduced by the enhancement system `tricking'
the speech quality predictor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Evaluation of Augmentations for Robust OOD
  <span class="highlight-title">Self-Supervised</span> Contrastive Phonocardiogram Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aristotelis Ballas, Vasileios Papapanagiotou, Christos Diou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent increase in research activity, deep-learning models have
not yet been widely accepted in several real-world settings, such as medicine.
The shortage of high-quality annotated data often hinders the development of
robust and generalizable models, which do not suffer from degraded
effectiveness when presented with newly-collected, out-of-distribution (OOD)
datasets. Contrastive Self-Supervised Learning (SSL) offers a potential
solution to labeled data scarcity, as it takes advantage of unlabeled data to
increase model effectiveness and robustness. In this research, we propose
applying contrastive SSL for detecting abnormalities in 1D phonocardiogram
(PCG) samples by learning a generalized representation of the signal.
Specifically, we perform an extensive comparative evaluation of a wide range of
audio-based augmentations, evaluate trained classifiers on multiple datasets
across different downstream tasks, and finally report on the impact of each
augmentation in model training. We experimentally demonstrate that, depending
on its training distribution, the effectiveness of a fully-supervised model can
degrade up to 32% when evaluated on unseen data, while SSL models only lose up
to 10% or even improve in some cases. We argue and experimentally demonstrate
that, contrastive SSL pretraining can assist in providing robust classifiers
which can generalize to unseen, OOD data, without relying on time- and
labor-intensive annotation processes by medical experts. Furthermore, the
proposed extensive evaluation protocol sheds light on the most promising and
appropriate augmentations for robust PCG signal processing, by calculating
their effect size on model training. Finally, we provide researchers and
practitioners with a roadmap towards producing robust models for PCG
classification, in addition to an open-source codebase for developing novel
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PREPRINT Manuscript under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mega-<span class="highlight-title">TTS</span> 2: Boosting Prompting Mechanisms for <span class="highlight-title">Zero-Shot</span> <span class="highlight-title">Speech Synthesis</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07218v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07218v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Zhenhui Ye, Shengpeng Ji, Qian Yang, Chen Zhang, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech
prompts, which significantly reduces the data and computation requirements for
voice cloning by skipping the fine-tuning process. However, the prompting
mechanisms of zero-shot TTS still face challenges in the following aspects: 1)
previous works of zero-shot TTS are typically trained with single-sentence
prompts, which significantly restricts their performance when the data is
relatively sufficient during the inference stage. 2) The prosodic information
in prompts is highly coupled with timbre, making it untransferable to each
other. This paper introduces Mega-TTS 2, a generic prompting mechanism for
zero-shot TTS, to tackle the aforementioned challenges. Specifically, we design
a powerful acoustic autoencoder that separately encodes the prosody and timbre
information into the compressed latent space while providing high-quality
reconstructions. Then, we propose a multi-reference timbre encoder and a
prosody latent language model (P-LLM) to extract useful information from
multi-sentence prompts. We further leverage the probabilities derived from
multiple P-LLM outputs to produce transferable and controllable prosody.
Experimental results demonstrate that Mega-TTS 2 could not only synthesize
identity-preserving speech with a short prompt of an unseen speaker from
arbitrary sources but consistently outperform the fine-tuning method when the
volume of data ranges from 10 seconds to 5 minutes. Furthermore, our method
enables to transfer various speaking styles to the target timbre in a
fine-grained and controlled manner. Audio samples can be found in
https://boostprompt.github.io/boostprompt/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero Resource Code-switched Speech Benchmark Using Speech Utterance
  Pairs For Multiple Spoken Languages <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03018v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03018v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Po Huang, Chih-Kai Yang, Yu-Kuan Fu, Ewan Dunbar, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new zero resource code-switched speech benchmark designed to
directly assess the code-switching capabilities of self-supervised speech
encoders. We showcase a baseline system of language modeling on discrete units
to demonstrate how the code-switching abilities of speech encoders can be
assessed in a zero-resource manner. Our experiments encompass a variety of
well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We
examine the impact of pre-training languages and model size on benchmark
performance. Notably, though our results demonstrate that speech encoders with
multilingual pre-training, exemplified by XLSR, outperform monolingual variants
(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial
room for improvement in their code-switching linguistic abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024 (v2)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken
  Question Answering <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chyi-Jiunn Lin, Guan-Ting Lin, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken Question Answering (SQA) is essential for machines to reply to user's
question by finding the answer span within a given spoken passage. SQA has been
previously achieved without ASR to avoid recognition errors and
Out-of-Vocabulary (OOV) problems. However, the real-world problem of
Open-domain SQA (openSQA), in which the machine needs to first retrieve
passages that possibly contain the answer from a spoken archive in addition,
was never considered. This paper proposes the first known end-to-end framework,
Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the
openSQA problem. SpeechDPR learns a sentence-level semantic representation by
distilling knowledge from the cascading model of unsupervised ASR (UASR) and
text dense retriever (TDR). No manually transcribed speech data is needed.
Initial experiments showed performance comparable to the cascading model of
UASR and TDR, and significantly better when UASR was poor, verifying this
approach is more robust to speech recognition errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal <span class="highlight-title">Transformer</span> Distillation for Audio-Visual Synchronization <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanjun Chen, Haibin Wu, Chung-Che Wang, Hung-yi Lee, Jyh-Shing Roger Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual synchronization aims to determine whether the mouth movements
and speech in the video are synchronized. VocaLiST reaches state-of-the-art
performance by incorporating multimodal Transformers to model audio-visual
interact information. However, it requires high computing resources, making it
impractical for real-world applications. This paper proposed an MTDVocaLiST
model, which is trained by our proposed multimodal Transformer distillation
(MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the
cross-attention distribution and value-relation in the Transformer of VocaLiST.
Additionally, we harness uncertainty weighting to fully exploit the interaction
information across all layers. Our proposed method is effective in two aspects:
From the distillation method perspective, MTD loss outperforms other strong
distillation baselines. From the distilled model's performance perspective: 1)
MTDVocaLiST outperforms similar-size SOTA models, SyncNet, and Perfect Match
models by 15.65% and 3.35%; 2) MTDVocaLiST reduces the model size of VocaLiST
by 83.52%, yet still maintaining similar performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A low latency attention module for streaming <span class="highlight-title">self-supervised</span> speech
  representation learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbo Ma, Siqi Pan, Deepak Chandran, Andrea Fanelli, Richard Cartwright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer is a fundamental building block in deep learning, and the
attention mechanism is the transformer's core component. Self-supervised speech
representation learning (SSRL) represents a popular use-case for the
transformer architecture. Due to transformers' acausal behavior, the use of
transformers for SSRL has been predominantly focused on acausal applications.
However, several media processing problems, such as speech processing, require
real-time solutions. In this paper, we present an implementation of the
attention module that enables training of SSRL architectures with low compute
and memory requirements, while allowing real-time inference with low and fixed
latency. The attention module proposed in this paper includes two components,
streaming attention (SA) and low-latency streaming attention (LLSA). The SA
represents our proposal for an efficient streaming SSRL implementation, while
the LLSA solves the latency build-up problem of other streaming attention
architectures, such as the masked acausal attention (MAA), guaranteeing a
latency equal to one layer even when multiple layers are stacked. We present a
comparative analysis between the vanilla attention, which we will refer here as
acausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with
automatic speech recognition as downstream task. When training on
librispeech-clean-100 and testing on librispeech-test-clean, our low-latency
attention module has a word error rate (WER) of 5.84%, which represents a
significant improvement over the MAA (WER = 13.82%). Our implementation also
reduces the inference latency from 1.92 to 0.16 seconds. The proposed
low-latency module preserves many of the benefits of conventional acausal
transformers, but also enables latency characteristics that make it applicable
to real-time streaming applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Source <span class="highlight-title">Diffusion</span> Models for Simultaneous Music <span class="highlight-title">Generation</span> and
  Separation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02257v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02257v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we define a diffusion-based generative model capable of both
music synthesis and source separation by learning the score of the joint
probability density of sources sharing a context. Alongside the classic total
inference tasks (i.e., generating a mixture, separating the sources), we also
introduce and experiment on the partial generation task of source imputation,
where we generate a subset of the sources given the others (e.g., play a piano
track that goes well with the drums). Additionally, we introduce a novel
inference method for the separation task based on Dirac likelihood functions.
We train our model on Slakh2100, a standard dataset for musical source
separation, provide qualitative results in the generation settings, and
showcase competitive quantitative results in the source separation setting. Our
method is the first example of a single model that can handle both generation
and separation tasks, thus representing a step toward general audio models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 oral presentation. Demo page:
  https://gladia-research-group.github.io/multi-source-diffusion-models/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for
  <span class="highlight-title">Self-supervised</span> Representations of French Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Titouan Parcollet, Ha Nguyen, Solene Evain, Marcely Zanon Boito, Adrien Pupier, Salima Mdhaffar, Hang Le, Sina Alisamir, Natalia Tomashenko, Marco Dinarelli, Shucong Zhang, Alexandre Allauzen, Maximin Coavoux, Yannick Esteve, Mickael Rouvier, Jerome Goulian, Benjamin Lecouteux, Francois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is at the origin of unprecedented improvements
in many different domains including computer vision and natural language
processing. Speech processing drastically benefitted from SSL as most of the
current domain-related tasks are now being approached with pre-trained models.
This work introduces LeBenchmark 2.0 an open-source framework for assessing and
building SSL-equipped French speech technologies. It includes documented,
large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous
speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to
one billion learnable parameters shared with the community, and an evaluation
protocol made of six downstream tasks to complement existing benchmarks.
LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for
speech with the investigation of frozen versus fine-tuned downstream models,
task-agnostic versus task-specific pre-trained models as well as a discussion
on the carbon footprint of large-scale model training. Overall, the newly
introduced models trained on 14,000 hours of French speech outperform
multilingual and previous LeBenchmark SSL models across the benchmark but also
required up to four times more energy for pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Computer Science and Language. Preprint allowed</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Multi-Source Inference for Text Conditioned Music <span class="highlight-title">Diffusion</span>
  Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Source Diffusion Models (MSDM) allow for compositional musical
generation tasks: generating a set of coherent sources, creating
accompaniments, and performing source separation. Despite their versatility,
they require estimating the joint distribution over the sources, necessitating
pre-separated musical data, which is rarely available, and fixing the number
and type of sources at training time. This paper generalizes MSDM to arbitrary
time-domain diffusion models conditioned on text embeddings. These models do
not require separated data as they are trained on mixtures, can parameterize an
arbitrary number of sources, and allow for rich semantic control. We propose an
inference procedure enabling the coherent generation of sources and
accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform
source separation. We experiment with diffusion models trained on Slakh2100 and
MTG-Jamendo, showcasing competitive generation and separation results in a
relaxed data setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QEAN: Quaternion-Enhanced Attention Network for Visual Dance <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizhen Zhou, Yejing Huo, Guoheng Huang, An Zeng, Xuhang Chen, Lian Huang, Zinuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of music-generated dance is a novel and challenging Image
generation task. It aims to input a piece of music and seed motions, then
generate natural dance movements for the subsequent music. Transformer-based
methods face challenges in time series prediction tasks related to human
movements and music due to their struggle in capturing the nonlinear
relationship and temporal aspects. This can lead to issues like joint
deformation, role deviation, floating, and inconsistencies in dance movements
generated in response to the music. In this paper, we propose a
Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a
quaternion perspective, which consists of a Spin Position Embedding (SPE)
module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds
position information into self-attention in a rotational manner, leading to
better learning of features of movement sequences and audio sequences, and
improved understanding of the connection between music and dance. Second, QRA
represents and fuses 3D motion features and audio features in the form of a
series of quaternions, enabling the model to better learn the temporal
coordination of music and dance under the complex temporal cycle conditions of
dance generation. Finally, we conducted experiments on the dataset AIST++, and
the results show that our approach achieves better and more robust performance
in generating accurate, high-quality dance movements. Our source code and
dataset can be available from https://github.com/MarasyZZ/QEAN and
https://google.github.io/aistplusplus_dataset respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Visual Computer Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaMER-CTC: Connectionist Temporal Classification with Adaptive Maximum
  Entropy Regularization for Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SooHwan Eom, Eunseop Yoon, Hee Suk Yoon, Chanwoo Kim, Mark Hasegawa-Johnson, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Automatic Speech Recognition (ASR) systems, a recurring obstacle is the
generation of narrowly focused output distributions. This phenomenon emerges as
a side effect of Connectionist Temporal Classification (CTC), a robust sequence
learning tool that utilizes dynamic programming for sequence mapping. While
earlier efforts have tried to combine the CTC loss with an entropy maximization
regularization term to mitigate this issue, they employed a constant weighting
term on the regularization during the training, which we find may not be
optimal. In this work, we introduce Adaptive Maximum Entropy Regularization
(AdaMER), a technique that can modulate the impact of entropy regularization
throughout the training process. This approach not only refines ASR model
training but ensures that as training proceeds, predictions display the desired
model confidence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discriminative Neighborhood Smoothing for Generative Anomalous Sound
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Fujimura, Keisuke Imoto, Tomoki Toda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose discriminative neighborhood smoothing of generative anomaly scores
for anomalous sound detection. While the discriminative approach is known to
achieve better performance than generative approaches often, we have found that
it sometimes causes significant performance degradation due to the discrepancy
between the training and test data, making it less robust than the generative
approach. Our proposed method aims to compensate for the disadvantages of
generative and discriminative approaches by combining them. Generative anomaly
scores are smoothed using multiple samples with similar discriminative features
to improve the performance of the generative approach in an ensemble manner
while keeping its robustness. Experimental results show that our proposed
method greatly improves the original generative method, including absolute
improvement of 22% in AUC and robustly works, while a discriminative method
suffers from the discrepancy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-loudspeaker Binaural Room Impulse Response Dataset with
  High-Resolution Translational and Rotational Head Coordinates in a Listening
  Room 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Qiao, Ryan Miguel Gonzales, Edgar Choueiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data report for the 3D3A Lab Binaural Room Impulse Response (BRIR) Dataset
(https://doi.org/10.34770/6gc9-5787).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Frontiers in Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent CLAP Loss for Better Foley Sound <span class="highlight-title">Synthesis</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tornike Karchkhadze, Hassan Salami Kavaki, Mohammad Rasool Izadi, Bryce Irvin, Mikolaj Kegler, Ari Hertz, Shuo Zhang, Marko Stamenovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foley sound generation, the art of creating audio for multimedia, has
recently seen notable advancements through text-conditioned latent diffusion
models. These systems use multimodal text-audio representation models, such as
Contrastive Language-Audio Pretraining (CLAP), whose objective is to map
corresponding audio and text prompts into a joint embedding space. AudioLDM, a
text-to-audio model, was the winner of the DCASE2023 task 7 Foley sound
synthesis challenge. The winning system fine-tuned the model for specific audio
classes and applied a post-filtering method using CLAP similarity scores
between output audio and input text at inference time, requiring the generation
of extra samples, thus reducing data generation efficiency. We introduce a new
loss term to enhance Foley sound generation in AudioLDM without post-filtering.
This loss term uses a new module based on the CLAP mode-Latent CLAP encode-to
align the latent diffusion output with real audio in a shared CLAP embedding
space. Our experiments demonstrate that our method effectively reduces the
Frechet Audio Distance (FAD) score of the generated audio and eliminates the
need for post-filtering, thus enhancing generation efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Shepardson, Jack Armitage, Thor Magnusson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based probabilistic models of musical data are producing
increasingly realistic results and promise to enter creative workflows of many
kinds. Yet they have been little-studied in a performance setting, where the
results of user actions typically ought to feel instantaneous. To enable such
study, we designed Notochord, a deep probabilistic model for sequences of
structured events, and trained an instance of it on the Lakh MIDI dataset. Our
probabilistic formulation allows interpretable interventions at a sub-event
level, which enables one model to act as a backbone for diverse interactive
musical functions including steerable generation, harmonization, machine
improvisation, and likelihood-based interfaces. Notochord can generate
polyphonic and multi-track MIDI, and respond to inputs with latency below ten
milliseconds. Training code, model checkpoints and interactive examples are
provided as open source software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures. Proceedings of the 3rd Conference on AI Music
  Creativity (2022, September 17)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sound Event Detection and Localization with Distance Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Aleksander Krause, Archontis Politis, Annamaria Mesaros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound Event Detection and Localization (SELD) is a combined task of
identifying sound events and their corresponding direction-of-arrival (DOA).
While this task has numerous applications and has been extensively researched
in recent years, it fails to provide full information about the sound source
position. In this paper, we overcome this problem by extending the task to
Sound Event Detection, Localization with Distance Estimation (3D SELD). We
study two ways of integrating distance estimation within the SELD core - a
multi-task approach, in which the problem is tackled by a separate model
output, and a single-task approach obtained by extending the multi-ACCDOA
method to include distance information. We investigate both methods for the
Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial
Soundscapes 2023. Moreover, our study involves experiments on the loss function
related to the distance estimation part. Our results show that it is possible
to perform 3D SELD without any degradation of performance in sound event
detection and DOA estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted for the 32nd European Signal Processing
  Conference EUSIPCO 2024 in Lyon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt-Singer: Controllable Singing-Voice-<span class="highlight-title">Synthesis</span> with Natural
  Language Prompt <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Wang, Ruofan Hu, Rongjie Huang, Zhiqing Hong, Ruiqi Li, Wenrui Liu, Fuming You, Tao Jin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio
quality and naturalness, yet they lack the capability to control the style
attributes of the synthesized singing explicitly. We propose Prompt-Singer, the
first SVS method that enables attribute controlling on singer gender, vocal
range and volume with natural language. We adopt a model architecture based on
a decoder-only transformer with a multi-scale hierarchy, and design a
range-melody decoupled pitch representation that enables text-conditioned vocal
range control while keeping melodic accuracy. Furthermore, we explore various
experiment settings, including different types of text representations, text
encoder fine-tuning, and introducing speech data to alleviate data scarcity,
aiming to facilitate further research. Experiments show that our model achieves
favorable controlling ability and audio quality. Audio samples are available at
http://prompt-singer.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards the Development of a Real-Time Deepfake Audio Detection System
  in Communication Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonat John Mathew, Rakin Ahsan, Sae Furukawa, Jagdish Gautham Krishna Kumar, Huzaifa Pallan, Agamjeet Singh Padda, Sara Adamski, Madhu Reddiboina, Arjun Pankajakshan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake audio poses a rising threat in communication platforms,
necessitating real-time detection for audio stream integrity. Unlike
traditional non-real-time approaches, this study assesses the viability of
employing static deepfake audio detection models in real-time communication
platforms. An executable software is developed for cross-platform
compatibility, enabling real-time execution. Two deepfake audio detection
models based on Resnet and LCNN architectures are implemented using the
ASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof
2019 challenge baselines. The study proposes strategies and frameworks for
enhancing these models, paving the way for real-time deepfake audio detection
in communication platforms. This work contributes to the advancement of audio
stream security, ensuring robust detection capabilities in dynamic, real-time
communication scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucination in Perceptual Metric-Driven Speech Enhancement Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the area of speech enhancement, there is an ongoing interest in the
creation of neural systems which explicitly aim to improve the perceptual
quality of the processed audio. In concert with this is the topic of
non-intrusive (i.e. without clean reference) speech quality prediction, for
which neural networks are trained to predict human-assigned quality labels
directly from distorted audio. When combined, these areas allow for the
creation of powerful new speech enhancement systems which can leverage large
real-world datasets of distorted audio, by taking inference of a pre-trained
speech quality predictor as the sole loss function of the speech enhancement
system. This paper aims to identify a potential pitfall with this approach,
namely hallucinations which are introduced by the enhancement system `tricking'
the speech quality predictor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mega-<span class="highlight-title">TTS</span> 2: Boosting Prompting Mechanisms for <span class="highlight-title">Zero-Shot</span> <span class="highlight-title">Speech Synthesis</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07218v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07218v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Zhenhui Ye, Shengpeng Ji, Qian Yang, Chen Zhang, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech
prompts, which significantly reduces the data and computation requirements for
voice cloning by skipping the fine-tuning process. However, the prompting
mechanisms of zero-shot TTS still face challenges in the following aspects: 1)
previous works of zero-shot TTS are typically trained with single-sentence
prompts, which significantly restricts their performance when the data is
relatively sufficient during the inference stage. 2) The prosodic information
in prompts is highly coupled with timbre, making it untransferable to each
other. This paper introduces Mega-TTS 2, a generic prompting mechanism for
zero-shot TTS, to tackle the aforementioned challenges. Specifically, we design
a powerful acoustic autoencoder that separately encodes the prosody and timbre
information into the compressed latent space while providing high-quality
reconstructions. Then, we propose a multi-reference timbre encoder and a
prosody latent language model (P-LLM) to extract useful information from
multi-sentence prompts. We further leverage the probabilities derived from
multiple P-LLM outputs to produce transferable and controllable prosody.
Experimental results demonstrate that Mega-TTS 2 could not only synthesize
identity-preserving speech with a short prompt of an unseen speaker from
arbitrary sources but consistently outperform the fine-tuning method when the
volume of data ranges from 10 seconds to 5 minutes. Furthermore, our method
enables to transfer various speaking styles to the target timbre in a
fine-grained and controlled manner. Audio samples can be found in
https://boostprompt.github.io/boostprompt/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero Resource Code-switched Speech Benchmark Using Speech Utterance
  Pairs For Multiple Spoken Languages <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03018v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03018v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Po Huang, Chih-Kai Yang, Yu-Kuan Fu, Ewan Dunbar, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new zero resource code-switched speech benchmark designed to
directly assess the code-switching capabilities of self-supervised speech
encoders. We showcase a baseline system of language modeling on discrete units
to demonstrate how the code-switching abilities of speech encoders can be
assessed in a zero-resource manner. Our experiments encompass a variety of
well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We
examine the impact of pre-training languages and model size on benchmark
performance. Notably, though our results demonstrate that speech encoders with
multilingual pre-training, exemplified by XLSR, outperform monolingual variants
(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial
room for improvement in their code-switching linguistic abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024 (v2)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken
  Question Answering <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chyi-Jiunn Lin, Guan-Ting Lin, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken Question Answering (SQA) is essential for machines to reply to user's
question by finding the answer span within a given spoken passage. SQA has been
previously achieved without ASR to avoid recognition errors and
Out-of-Vocabulary (OOV) problems. However, the real-world problem of
Open-domain SQA (openSQA), in which the machine needs to first retrieve
passages that possibly contain the answer from a spoken archive in addition,
was never considered. This paper proposes the first known end-to-end framework,
Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the
openSQA problem. SpeechDPR learns a sentence-level semantic representation by
distilling knowledge from the cascading model of unsupervised ASR (UASR) and
text dense retriever (TDR). No manually transcribed speech data is needed.
Initial experiments showed performance comparable to the cascading model of
UASR and TDR, and significantly better when UASR was poor, verifying this
approach is more robust to speech recognition errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal <span class="highlight-title">Transformer</span> Distillation for Audio-Visual Synchronization <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanjun Chen, Haibin Wu, Chung-Che Wang, Hung-yi Lee, Jyh-Shing Roger Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual synchronization aims to determine whether the mouth movements
and speech in the video are synchronized. VocaLiST reaches state-of-the-art
performance by incorporating multimodal Transformers to model audio-visual
interact information. However, it requires high computing resources, making it
impractical for real-world applications. This paper proposed an MTDVocaLiST
model, which is trained by our proposed multimodal Transformer distillation
(MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the
cross-attention distribution and value-relation in the Transformer of VocaLiST.
Additionally, we harness uncertainty weighting to fully exploit the interaction
information across all layers. Our proposed method is effective in two aspects:
From the distillation method perspective, MTD loss outperforms other strong
distillation baselines. From the distilled model's performance perspective: 1)
MTDVocaLiST outperforms similar-size SOTA models, SyncNet, and Perfect Match
models by 15.65% and 3.35%; 2) MTDVocaLiST reduces the model size of VocaLiST
by 83.52%, yet still maintaining similar performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SwG-former: A Sliding-Window Graph Convolutional Network for
  Simultaneous Spatial-Temporal Information Extraction in Sound Event
  Localization and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Huang, Qinghua Huang, Liyan Ma, Chuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound event localization and detection (SELD) involves sound event detection
(SED) and direction of arrival (DoA) estimation tasks. SED mainly relies on
temporal dependencies to distinguish different sound classes, while DoA
estimation depends on spatial correlations to estimate source directions. This
paper addresses the need to simultaneously extract spatial-temporal information
in audio signals to improve SELD performance. A novel block, the sliding-window
graph-former (SwG-former), is designed to learn temporal context information of
sound events based on their spatial correlations. The SwG-former block
transforms audio signals into a graph representation and constructs graph
vertices to capture higher abstraction levels for spatial correlations. It uses
different-sized sliding windows to adapt various sound event durations and
aggregates temporal features with similar spatial information while
incorporating multi-head self-attention (MHSA) to model global information.
Furthermore, as the cornerstone of message passing, a robust Conv2dAgg function
is proposed and embedded into the block to aggregate the features of neighbor
vertices. As a result, a SwG-former model, which stacks the SwG-former blocks,
demonstrates superior performance compared to recent advanced SELD models. The
SwG-former block is also integrated into the event-independent network version
2 (EINV2), called SwG-EINV2, which surpasses the state-of-the-art (SOTA)
methods under the same acoustic environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A low latency attention module for streaming <span class="highlight-title">self-supervised</span> speech
  representation learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbo Ma, Siqi Pan, Deepak Chandran, Andrea Fanelli, Richard Cartwright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer is a fundamental building block in deep learning, and the
attention mechanism is the transformer's core component. Self-supervised speech
representation learning (SSRL) represents a popular use-case for the
transformer architecture. Due to transformers' acausal behavior, the use of
transformers for SSRL has been predominantly focused on acausal applications.
However, several media processing problems, such as speech processing, require
real-time solutions. In this paper, we present an implementation of the
attention module that enables training of SSRL architectures with low compute
and memory requirements, while allowing real-time inference with low and fixed
latency. The attention module proposed in this paper includes two components,
streaming attention (SA) and low-latency streaming attention (LLSA). The SA
represents our proposal for an efficient streaming SSRL implementation, while
the LLSA solves the latency build-up problem of other streaming attention
architectures, such as the masked acausal attention (MAA), guaranteeing a
latency equal to one layer even when multiple layers are stacked. We present a
comparative analysis between the vanilla attention, which we will refer here as
acausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with
automatic speech recognition as downstream task. When training on
librispeech-clean-100 and testing on librispeech-test-clean, our low-latency
attention module has a word error rate (WER) of 5.84%, which represents a
significant improvement over the MAA (WER = 13.82%). Our implementation also
reduces the inference latency from 1.92 to 0.16 seconds. The proposed
low-latency module preserves many of the benefits of conventional acausal
transformers, but also enables latency characteristics that make it applicable
to real-time streaming applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Source <span class="highlight-title">Diffusion</span> Models for Simultaneous Music <span class="highlight-title">Generation</span> and
  Separation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02257v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02257v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we define a diffusion-based generative model capable of both
music synthesis and source separation by learning the score of the joint
probability density of sources sharing a context. Alongside the classic total
inference tasks (i.e., generating a mixture, separating the sources), we also
introduce and experiment on the partial generation task of source imputation,
where we generate a subset of the sources given the others (e.g., play a piano
track that goes well with the drums). Additionally, we introduce a novel
inference method for the separation task based on Dirac likelihood functions.
We train our model on Slakh2100, a standard dataset for musical source
separation, provide qualitative results in the generation settings, and
showcase competitive quantitative results in the source separation setting. Our
method is the first example of a single model that can handle both generation
and separation tasks, thus representing a step toward general audio models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 oral presentation. Demo page:
  https://gladia-research-group.github.io/multi-source-diffusion-models/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for
  <span class="highlight-title">Self-supervised</span> Representations of French Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Titouan Parcollet, Ha Nguyen, Solene Evain, Marcely Zanon Boito, Adrien Pupier, Salima Mdhaffar, Hang Le, Sina Alisamir, Natalia Tomashenko, Marco Dinarelli, Shucong Zhang, Alexandre Allauzen, Maximin Coavoux, Yannick Esteve, Mickael Rouvier, Jerome Goulian, Benjamin Lecouteux, Francois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is at the origin of unprecedented improvements
in many different domains including computer vision and natural language
processing. Speech processing drastically benefitted from SSL as most of the
current domain-related tasks are now being approached with pre-trained models.
This work introduces LeBenchmark 2.0 an open-source framework for assessing and
building SSL-equipped French speech technologies. It includes documented,
large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous
speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to
one billion learnable parameters shared with the community, and an evaluation
protocol made of six downstream tasks to complement existing benchmarks.
LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for
speech with the investigation of frozen versus fine-tuned downstream models,
task-agnostic versus task-specific pre-trained models as well as a discussion
on the carbon footprint of large-scale model training. Overall, the newly
introduced models trained on 14,000 hours of French speech outperform
multilingual and previous LeBenchmark SSL models across the benchmark but also
required up to four times more energy for pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Computer Science and Language. Preprint allowed</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-17T00:00:00Z">2024-03-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitask frame-level learning for few-shot sound event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Zou, Genwei Yan, Ruoyu Wang, Jun Du, Meng Lei, Tian Gao, Xin Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on few-shot Sound Event Detection (SED), which aims to
automatically recognize and classify sound events with limited samples.
However, prevailing methods methods in few-shot SED predominantly rely on
segment-level predictions, which often providing detailed, fine-grained
predictions, particularly for events of brief duration. Although frame-level
prediction strategies have been proposed to overcome these limitations, these
strategies commonly face difficulties with prediction truncation caused by
background noise. To alleviate this issue, we introduces an innovative
multitask frame-level SED framework. In addition, we introduce TimeFilterAug, a
linear timing mask for data augmentation, to increase the model's robustness
and adaptability to diverse acoustic environments. The proposed method achieves
a F-score of 63.8%, securing the 1st rank in the few-shot bioacoustic event
detection category of the Detection and Classification of Acoustic Scenes and
Events Challenge 2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Visual Segmentation via Unlabeled Frame Exploitation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinxiang Liu, Yikun Liu, Fei Zhang, Chen Ju, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual segmentation (AVS) aims to segment the sounding objects in video
frames. Although great progress has been witnessed, we experimentally reveal
that current methods reach marginal performance gain within the use of the
unlabeled frames, leading to the underutilization issue. To fully explore the
potential of the unlabeled frames for AVS, we explicitly divide them into two
categories based on their temporal characteristics, i.e., neighboring frame
(NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame,
often contain rich motion information that assists in the accurate localization
of sounding objects. Contrary to NFs, DFs have long temporal distances from the
labeled frame, which share semantic-similar objects with appearance variations.
Considering their unique characteristics, we propose a versatile framework that
effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the
motion cues as the dynamic guidance to improve the objectness localization.
Besides, we exploit the semantic cues in DFs by treating them as valid
augmentations to the labeled frames, which are then used to enrich data
diversity in a self-training manner. Extensive experimental results demonstrate
the versatility and superiority of our method, unleashing the power of the
abundant unlabeled frames.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaceTalk: Audio-Driven Motion <span class="highlight-title">Diffusion</span> for Neural Parametric Head
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FaceTalk, a novel generative approach designed for synthesizing
high-fidelity 3D motion sequences of talking human heads from input audio
signal. To capture the expressive, detailed nature of human heads, including
hair, ears, and finer-scale eye movements, we propose to couple speech signal
with the latent space of neural parametric head models to create high-fidelity,
temporally coherent motion sequences. We propose a new latent diffusion model
for this task, operating in the expression space of neural parametric head
models, to synthesize audio-driven realistic head sequences. In the absence of
a dataset with corresponding NPHM expressions to audio, we optimize for these
correspondences to produce a dataset of temporally-optimized NPHM expressions
fit to audio-video recordings of people talking. To the best of our knowledge,
this is the first work to propose a generative approach for realistic and
high-quality motion synthesis of volumetric human heads, representing a
significant advancement in the field of audio-driven 3D animation. Notably, our
approach stands out in its ability to generate plausible motion sequences that
can produce high-fidelity head animation coupled with the NPHM shape space. Our
experimental results substantiate the effectiveness of FaceTalk, consistently
achieving superior and visually natural motion, encompassing diverse facial
expressions and styles, outperforming existing methods by 75% in perceptual
user study evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper Video: https://youtu.be/7Jf0kawrA3Q Project Page:
  https://shivangi-aneja.github.io/projects/facetalk/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EM-<span class="highlight-title">TTS</span>: Efficiently Trained Low-Resource Mongolian Lightweight
  <span class="highlight-title">Text-to-Speech</span> <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Liang, Haoxiang Shi, Jiawei Wang, Keda Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved
high-quality speech synthesis results. Recurrent neural networks have become a
standard modeling technique for sequential data in TTS systems and are widely
used. However, training a TTS model which includes RNN components requires
powerful GPU performance and takes a long time. In contrast, CNN-based sequence
synthesis techniques can significantly reduce the parameters and training time
of a TTS model while guaranteeing a certain performance due to their high
parallelism, which alleviate these economic costs of training. In this paper,
we propose a lightweight TTS system based on deep convolutional neural
networks, which is a two-stage training end-to-end TTS model and does not
employ any recurrent units. Our model consists of two stages: Text2Spectrum and
SSRN. The former is used to encode phonemes into a coarse mel spectrogram and
the latter is used to synthesize the complete spectrum from the coarse mel
spectrogram. Meanwhile, we improve the robustness of our model by a series of
data augmentations, such as noise suppression, time warping, frequency masking
and time masking, for solving the low resource mongolian problem. Experiments
show that our model can reduce the training time and parameters while ensuring
the quality and naturalness of the synthesized speech compared to using
mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for
validation, which significantly reduces training time while maintaining a
certain accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 27th IEEE International Conference on Computer
  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:
  substantial text overlap with arXiv:2211.01948</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitask frame-level learning for few-shot sound event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Zou, Genwei Yan, Ruoyu Wang, Jun Du, Meng Lei, Tian Gao, Xin Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on few-shot Sound Event Detection (SED), which aims to
automatically recognize and classify sound events with limited samples.
However, prevailing methods methods in few-shot SED predominantly rely on
segment-level predictions, which often providing detailed, fine-grained
predictions, particularly for events of brief duration. Although frame-level
prediction strategies have been proposed to overcome these limitations, these
strategies commonly face difficulties with prediction truncation caused by
background noise. To alleviate this issue, we introduces an innovative
multitask frame-level SED framework. In addition, we introduce TimeFilterAug, a
linear timing mask for data augmentation, to increase the model's robustness
and adaptability to diverse acoustic environments. The proposed method achieves
a F-score of 63.8%, securing the 1st rank in the few-shot bioacoustic event
detection category of the Detection and Classification of Acoustic Scenes and
Events Challenge 2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Visual Segmentation via Unlabeled Frame Exploitation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinxiang Liu, Yikun Liu, Fei Zhang, Chen Ju, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual segmentation (AVS) aims to segment the sounding objects in video
frames. Although great progress has been witnessed, we experimentally reveal
that current methods reach marginal performance gain within the use of the
unlabeled frames, leading to the underutilization issue. To fully explore the
potential of the unlabeled frames for AVS, we explicitly divide them into two
categories based on their temporal characteristics, i.e., neighboring frame
(NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame,
often contain rich motion information that assists in the accurate localization
of sounding objects. Contrary to NFs, DFs have long temporal distances from the
labeled frame, which share semantic-similar objects with appearance variations.
Considering their unique characteristics, we propose a versatile framework that
effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the
motion cues as the dynamic guidance to improve the objectness localization.
Besides, we exploit the semantic cues in DFs by treating them as valid
augmentations to the labeled frames, which are then used to enrich data
diversity in a self-training manner. Extensive experimental results demonstrate
the versatility and superiority of our method, unleashing the power of the
abundant unlabeled frames.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review
  of Healthcare Strategies, Challenges, and Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Billel Essaid, Hamza Kheddar, Noureddine Batel, Abderrahmane Lakas, Muhammad E. H. Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) plays a pivotal role in our daily lives,
offering utility not only for interacting with machines but also for
facilitating communication for individuals with either partial or profound
hearing impairments. The process involves receiving the speech signal in
analogue form, followed by various signal processing algorithms to make it
compatible with devices of limited capacity, such as cochlear implants (CIs).
Unfortunately, these implants, equipped with a finite number of electrodes,
often result in speech distortion during synthesis. Despite efforts by
researchers to enhance received speech quality using various state-of-the-art
signal processing techniques, challenges persist, especially in scenarios
involving multiple sources of speech, environmental noise, and other
circumstances. The advent of new artificial intelligence (AI) methods has
ushered in cutting-edge strategies to address the limitations and difficulties
associated with traditional signal processing techniques dedicated to CIs. This
review aims to comprehensively review advancements in CI-based ASR and speech
enhancement, among other related aspects. The primary objective is to provide a
thorough overview of metrics and datasets, exploring the capabilities of AI
algorithms in this biomedical field, summarizing and commenting on the best
results obtained. Additionally, the review will delve into potential
applications and suggest future directions to bridge existing research gaps in
this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaceTalk: Audio-Driven Motion <span class="highlight-title">Diffusion</span> for Neural Parametric Head
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FaceTalk, a novel generative approach designed for synthesizing
high-fidelity 3D motion sequences of talking human heads from input audio
signal. To capture the expressive, detailed nature of human heads, including
hair, ears, and finer-scale eye movements, we propose to couple speech signal
with the latent space of neural parametric head models to create high-fidelity,
temporally coherent motion sequences. We propose a new latent diffusion model
for this task, operating in the expression space of neural parametric head
models, to synthesize audio-driven realistic head sequences. In the absence of
a dataset with corresponding NPHM expressions to audio, we optimize for these
correspondences to produce a dataset of temporally-optimized NPHM expressions
fit to audio-video recordings of people talking. To the best of our knowledge,
this is the first work to propose a generative approach for realistic and
high-quality motion synthesis of volumetric human heads, representing a
significant advancement in the field of audio-driven 3D animation. Notably, our
approach stands out in its ability to generate plausible motion sequences that
can produce high-fidelity head animation coupled with the NPHM shape space. Our
experimental results substantiate the effectiveness of FaceTalk, consistently
achieving superior and visually natural motion, encompassing diverse facial
expressions and styles, outperforming existing methods by 75% in perceptual
user study evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper Video: https://youtu.be/7Jf0kawrA3Q Project Page:
  https://shivangi-aneja.github.io/projects/facetalk/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EM-<span class="highlight-title">TTS</span>: Efficiently Trained Low-Resource Mongolian Lightweight
  <span class="highlight-title">Text-to-Speech</span> <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Liang, Haoxiang Shi, Jiawei Wang, Keda Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved
high-quality speech synthesis results. Recurrent neural networks have become a
standard modeling technique for sequential data in TTS systems and are widely
used. However, training a TTS model which includes RNN components requires
powerful GPU performance and takes a long time. In contrast, CNN-based sequence
synthesis techniques can significantly reduce the parameters and training time
of a TTS model while guaranteeing a certain performance due to their high
parallelism, which alleviate these economic costs of training. In this paper,
we propose a lightweight TTS system based on deep convolutional neural
networks, which is a two-stage training end-to-end TTS model and does not
employ any recurrent units. Our model consists of two stages: Text2Spectrum and
SSRN. The former is used to encode phonemes into a coarse mel spectrogram and
the latter is used to synthesize the complete spectrum from the coarse mel
spectrogram. Meanwhile, we improve the robustness of our model by a series of
data augmentations, such as noise suppression, time warping, frequency masking
and time masking, for solving the low resource mongolian problem. Experiments
show that our model can reduce the training time and parameters while ensuring
the quality and naturalness of the synthesized speech compared to using
mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for
validation, which significantly reduces training time while maintaining a
certain accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 27th IEEE International Conference on Computer
  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:
  substantial text overlap with arXiv:2211.01948</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-16T00:00:00Z">2024-03-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Fedorishin, Livio Forte III, Philip Schneider, Srirangaraj Setlur, Venu Govindaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound event detection (SED) is an active area of audio research that aims to
detect the temporal occurrence of sounds. In this paper, we apply SED to engine
fault detection by introducing a multimodal SED framework that detects
fine-grained engine faults of automobile engines using audio and
accelerometer-recorded vibration. We first introduce the problem of engine
fault SED on a dataset collected from a large variety of vehicles with
expertly-labeled engine fault sound events. Next, we propose a SED model to
temporally detect ten fine-grained engine faults that occur within vehicle
engines and further explore a pretraining strategy using a large-scale
weakly-labeled engine fault dataset. Through multiple evaluations, we show our
proposed framework is able to effectively detect engine fault sound events.
Finally, we investigate the interaction and characteristics of each modality
and show that fusing features from audio and vibration improves overall engine
fault SED capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Based Models with Applications to Speech and Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-Based Models (EBMs) are an important class of probabilistic models,
also known as random fields and undirected graphical models. EBMs are
un-normalized and thus radically different from other popular self-normalized
probabilistic models such as hidden Markov models (HMMs), autoregressive
models, generative adversarial nets (GANs) and variational auto-encoders
(VAEs). Over the past years, EBMs have attracted increasing interest not only
from the core machine learning community, but also from application domains
such as speech, vision, natural language processing (NLP) and so on, due to
significant theoretical and algorithmic progress. The sequential nature of
speech and language also presents special challenges and needs a different
treatment from processing fix-dimensional data (e.g., images). Therefore, the
purpose of this monograph is to present a systematic introduction to
energy-based models, including both algorithmic progress and applications in
speech and language processing. First, the basics of EBMs are introduced,
including classic models, recent models parameterized by neural networks,
sampling methods, and various learning methods from the classic learning
algorithms to the most advanced ones. Then, the application of EBMs in three
different scenarios is presented, i.e., for modeling marginal, conditional and
joint distributions, respectively. 1) EBMs for sequential data with
applications in language modeling, where the main focus is on the marginal
distribution of a sequence itself; 2) EBMs for modeling conditional
distributions of target sequences given observation sequences, with
applications in speech recognition, sequence labeling and text generation; 3)
EBMs for modeling joint distributions of both sequences of observations and
targets, and their applications in semi-supervised learning and calibrated
natural language understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The version before publisher editing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech-driven Personalized Gesture Synthetics: Harnessing Automatic
  Fuzzy Feature Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhang, Zhaohan Wang, Xin Lyu, Siyuan Zhao, Mengjian Li, Weidong Geng, Naye Ji, Hui Du, Fuxing Gao, Hao Wu, Shunman Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven gesture generation is an emerging field within virtual human
creation. However, a significant challenge lies in accurately determining and
processing the multitude of input features (such as acoustic, semantic,
emotional, personality, and even subtle unknown features). Traditional
approaches, reliant on various explicit feature inputs and complex multimodal
processing, constrain the expressiveness of resulting gestures and limit their
applicability. To address these challenges, we present Persona-Gestor, a novel
end-to-end generative model designed to generate highly personalized 3D
full-body gestures solely relying on raw speech audio. The model combines a
fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization
(AdaLN) transformer diffusion architecture. The fuzzy feature extractor
harnesses a fuzzy inference strategy that automatically infers implicit,
continuous fuzzy features. These fuzzy features, represented as a unified
latent feature, are fed into the AdaLN transformer. The AdaLN transformer
introduces a conditional mechanism that applies a uniform function across all
tokens, thereby effectively modeling the correlation between the fuzzy features
and the gesture sequence. This module ensures a high level of gesture-speech
synchronization while preserving naturalness. Finally, we employ the diffusion
model to train and infer various gestures. Extensive subjective and objective
evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's
superior performance to the current state-of-the-art approaches. Persona-Gestor
improves the system's usability and generalization capabilities, setting a new
benchmark in speech-driven gesture synthesis and broadening the horizon for
virtual human technology. Supplementary videos and code can be accessed at
https://zf223669.github.io/Diffmotion-v2-website/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoPlay: Audio-agnostic Cognitive Scaling for Acoustic Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Li, Rajalakshmi Nanadakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic sensing manifests great potential in various applications that
encompass health monitoring, gesture interface and imaging by leveraging the
speakers and microphones on smart devices. However, in ongoing research and
development in acoustic sensing, one problem is often overlooked: the same
speaker, when used concurrently for sensing and other traditional applications
(like playing music), could cause interference in both making it impractical to
use in the real world. The strong ultrasonic sensing signals mixed with music
would overload the speaker's mixer. To confront this issue of overloaded
signals, current solutions are clipping or down-scaling, both of which affect
the music playback quality and also sensing range and accuracy. To address this
challenge, we propose CoPlay, a deep learning based optimization algorithm to
cognitively adapt the sensing signal. It can 1) maximize the sensing signal
magnitude within the available bandwidth left by the concurrent music to
optimize sensing range and accuracy and 2) minimize any consequential frequency
distortion that can affect music playback. In this work, we design a deep
learning model and test it on common types of sensing signals (sine wave or
Frequency Modulated Continuous Wave FMCW) as inputs with various agnostic
concurrent music and speech. First, we evaluated the model performance to show
the quality of the generated signals. Then we conducted field studies of
downstream acoustic sensing tasks in the real world. A study with 12 users
proved that respiration monitoring and gesture recognition using our adapted
signal achieve similar accuracy as no-concurrent-music scenarios, while
clipping or down-scaling manifests worse accuracy. A qualitative study also
manifests that the music play quality is not degraded, unlike traditional
clipping or down-scaling methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refining Knowledge Transfer on Audio-Image Temporal Agreement for
  Audio-Text Cross Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunsuke Tsubaki, Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Keisuke Imoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this research is to refine knowledge transfer on audio-image
temporal agreement for audio-text cross retrieval. To address the limited
availability of paired non-speech audio-text data, learning methods for
transferring the knowledge acquired from a large amount of paired audio-image
data to shared audio-text representation have been investigated, suggesting the
importance of how audio-image co-occurrence is learned. Conventional approaches
in audio-image learning assign a single image randomly selected from the
corresponding video stream to the entire audio clip, assuming their
co-occurrence. However, this method may not accurately capture the temporal
agreement between the target audio and image because a single image can only
represent a snapshot of a scene, though the target audio changes from moment to
moment. To address this problem, we propose two methods for audio and image
matching that effectively capture the temporal information: (i) Nearest Match
wherein an image is selected from multiple time frames based on similarity with
audio, and (ii) Multiframe Match wherein audio and image pairs of multiple time
frames are used. Experimental results show that method (i) improves the
audio-text retrieval performance by selecting the nearest image that aligns
with the audio information and transferring the learned knowledge. Conversely,
method (ii) improves the performance of audio-image retrieval while not showing
significant improvements in audio-text retrieval performance. These results
indicate that refining audio-image temporal agreement may contribute to better
knowledge transfer to audio-text retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Respiratory Disease Classification and Biometric Analysis Using
  Biosignals from Digital Stethoscopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantino Álvarez Casado, Manuel Lage Cañellas, Matteo Pedone, Xiaoting Wu, Le Nguyen, Miguel Bordallo López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Respiratory diseases remain a leading cause of mortality worldwide,
highlighting the need for faster and more accurate diagnostic tools. This work
presents a novel approach leveraging digital stethoscope technology for
automatic respiratory disease classification and biometric analysis. Our
approach has the potential to significantly enhance traditional auscultation
practices. By leveraging one of the largest publicly available medical database
of respiratory sounds, we train machine learning models to classify various
respiratory health conditions. Our method differs from conventional methods by
using Empirical Mode Decomposition (EMD) and spectral analysis techniques to
isolate clinically relevant biosignals embedded within acoustic data captured
by digital stethoscopes. This approach focuses on information closely tied to
cardiovascular and respiratory patterns within the acoustic data. Spectral
analysis and filtering techniques isolate Intrinsic Mode Functions (IMFs)
strongly correlated with these physiological phenomena. These biosignals
undergo a comprehensive feature extraction process for predictive modeling.
These features then serve as input to train several machine learning models for
both classification and regression tasks. Our approach achieves high accuracy
in both binary classification (89% balanced accuracy for healthy vs. diseased)
and multi-class classification (72% balanced accuracy for specific diseases
like pneumonia and COPD). For the first time, this work introduces regression
models capable of estimating age and body mass index (BMI) based solely on
acoustic data, as well as a model for sex classification. Our findings
underscore the potential of intelligent digital stethoscopes to significantly
enhance assistive and remote diagnostic capabilities, contributing to
advancements in digital health, telehealth, and remote patient monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 3 tables, Conference paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Fedorishin, Livio Forte III, Philip Schneider, Srirangaraj Setlur, Venu Govindaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound event detection (SED) is an active area of audio research that aims to
detect the temporal occurrence of sounds. In this paper, we apply SED to engine
fault detection by introducing a multimodal SED framework that detects
fine-grained engine faults of automobile engines using audio and
accelerometer-recorded vibration. We first introduce the problem of engine
fault SED on a dataset collected from a large variety of vehicles with
expertly-labeled engine fault sound events. Next, we propose a SED model to
temporally detect ten fine-grained engine faults that occur within vehicle
engines and further explore a pretraining strategy using a large-scale
weakly-labeled engine fault dataset. Through multiple evaluations, we show our
proposed framework is able to effectively detect engine fault sound events.
Finally, we investigate the interaction and characteristics of each modality
and show that fusing features from audio and vibration improves overall engine
fault SED capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Based Models with Applications to Speech and Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-Based Models (EBMs) are an important class of probabilistic models,
also known as random fields and undirected graphical models. EBMs are
un-normalized and thus radically different from other popular self-normalized
probabilistic models such as hidden Markov models (HMMs), autoregressive
models, generative adversarial nets (GANs) and variational auto-encoders
(VAEs). Over the past years, EBMs have attracted increasing interest not only
from the core machine learning community, but also from application domains
such as speech, vision, natural language processing (NLP) and so on, due to
significant theoretical and algorithmic progress. The sequential nature of
speech and language also presents special challenges and needs a different
treatment from processing fix-dimensional data (e.g., images). Therefore, the
purpose of this monograph is to present a systematic introduction to
energy-based models, including both algorithmic progress and applications in
speech and language processing. First, the basics of EBMs are introduced,
including classic models, recent models parameterized by neural networks,
sampling methods, and various learning methods from the classic learning
algorithms to the most advanced ones. Then, the application of EBMs in three
different scenarios is presented, i.e., for modeling marginal, conditional and
joint distributions, respectively. 1) EBMs for sequential data with
applications in language modeling, where the main focus is on the marginal
distribution of a sequence itself; 2) EBMs for modeling conditional
distributions of target sequences given observation sequences, with
applications in speech recognition, sequence labeling and text generation; 3)
EBMs for modeling joint distributions of both sequences of observations and
targets, and their applications in semi-supervised learning and calibrated
natural language understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The version before publisher editing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initial Decoding with Minimally Augmented Language Model for Improved
  Lattice Rescoring in Low Resource ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savitha Murthy, Dinkar Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of improving speech recognition accuracy
with lattice rescoring in low-resource languages where the baseline language
model is insufficient for generating inclusive lattices. We minimally augment
the baseline language model with word unigram counts that are present in a
larger text corpus of the target language but absent in the baseline. The
lattices generated after decoding with such an augmented baseline language
model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada)
relative word error reduction with our proposed method. This reduction in word
error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word
error reduction obtained by decoding with full Wikipedia text augmented
language mode while our approach consumes only 1/8th the memory. We demonstrate
that our method is comparable with various text selection-based language model
augmentation and also consistent for data sets of different sizes. Our approach
is applicable for training speech recognition systems under low resource
conditions where speech data and compute resources are insufficient, while
there is a large text corpus that is available in the target language. Our
research involves addressing the issue of out-of-vocabulary words of the
baseline in general and does not focus on resolving the absence of named
entities. Our proposed method is simple and yet computationally less expensive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, Accepted in Sadhana Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech-driven Personalized Gesture Synthetics: Harnessing Automatic
  Fuzzy Feature Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhang, Zhaohan Wang, Xin Lyu, Siyuan Zhao, Mengjian Li, Weidong Geng, Naye Ji, Hui Du, Fuxing Gao, Hao Wu, Shunman Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven gesture generation is an emerging field within virtual human
creation. However, a significant challenge lies in accurately determining and
processing the multitude of input features (such as acoustic, semantic,
emotional, personality, and even subtle unknown features). Traditional
approaches, reliant on various explicit feature inputs and complex multimodal
processing, constrain the expressiveness of resulting gestures and limit their
applicability. To address these challenges, we present Persona-Gestor, a novel
end-to-end generative model designed to generate highly personalized 3D
full-body gestures solely relying on raw speech audio. The model combines a
fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization
(AdaLN) transformer diffusion architecture. The fuzzy feature extractor
harnesses a fuzzy inference strategy that automatically infers implicit,
continuous fuzzy features. These fuzzy features, represented as a unified
latent feature, are fed into the AdaLN transformer. The AdaLN transformer
introduces a conditional mechanism that applies a uniform function across all
tokens, thereby effectively modeling the correlation between the fuzzy features
and the gesture sequence. This module ensures a high level of gesture-speech
synchronization while preserving naturalness. Finally, we employ the diffusion
model to train and infer various gestures. Extensive subjective and objective
evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's
superior performance to the current state-of-the-art approaches. Persona-Gestor
improves the system's usability and generalization capabilities, setting a new
benchmark in speech-driven gesture synthesis and broadening the horizon for
virtual human technology. Supplementary videos and code can be accessed at
https://zf223669.github.io/Diffmotion-v2-website/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoPlay: Audio-agnostic Cognitive Scaling for Acoustic Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Li, Rajalakshmi Nanadakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic sensing manifests great potential in various applications that
encompass health monitoring, gesture interface and imaging by leveraging the
speakers and microphones on smart devices. However, in ongoing research and
development in acoustic sensing, one problem is often overlooked: the same
speaker, when used concurrently for sensing and other traditional applications
(like playing music), could cause interference in both making it impractical to
use in the real world. The strong ultrasonic sensing signals mixed with music
would overload the speaker's mixer. To confront this issue of overloaded
signals, current solutions are clipping or down-scaling, both of which affect
the music playback quality and also sensing range and accuracy. To address this
challenge, we propose CoPlay, a deep learning based optimization algorithm to
cognitively adapt the sensing signal. It can 1) maximize the sensing signal
magnitude within the available bandwidth left by the concurrent music to
optimize sensing range and accuracy and 2) minimize any consequential frequency
distortion that can affect music playback. In this work, we design a deep
learning model and test it on common types of sensing signals (sine wave or
Frequency Modulated Continuous Wave FMCW) as inputs with various agnostic
concurrent music and speech. First, we evaluated the model performance to show
the quality of the generated signals. Then we conducted field studies of
downstream acoustic sensing tasks in the real world. A study with 12 users
proved that respiration monitoring and gesture recognition using our adapted
signal achieve similar accuracy as no-concurrent-music scenarios, while
clipping or down-scaling manifests worse accuracy. A qualitative study also
manifests that the music play quality is not degraded, unlike traditional
clipping or down-scaling methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refining Knowledge Transfer on Audio-Image Temporal Agreement for
  Audio-Text Cross Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunsuke Tsubaki, Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Keisuke Imoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this research is to refine knowledge transfer on audio-image
temporal agreement for audio-text cross retrieval. To address the limited
availability of paired non-speech audio-text data, learning methods for
transferring the knowledge acquired from a large amount of paired audio-image
data to shared audio-text representation have been investigated, suggesting the
importance of how audio-image co-occurrence is learned. Conventional approaches
in audio-image learning assign a single image randomly selected from the
corresponding video stream to the entire audio clip, assuming their
co-occurrence. However, this method may not accurately capture the temporal
agreement between the target audio and image because a single image can only
represent a snapshot of a scene, though the target audio changes from moment to
moment. To address this problem, we propose two methods for audio and image
matching that effectively capture the temporal information: (i) Nearest Match
wherein an image is selected from multiple time frames based on similarity with
audio, and (ii) Multiframe Match wherein audio and image pairs of multiple time
frames are used. Experimental results show that method (i) improves the
audio-text retrieval performance by selecting the nearest image that aligns
with the audio information and transferring the learned knowledge. Conversely,
method (ii) improves the performance of audio-image retrieval while not showing
significant improvements in audio-text retrieval performance. These results
indicate that refining audio-image temporal agreement may contribute to better
knowledge transfer to audio-text retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mustango: Toward Controllable Text-to-Music <span class="highlight-title">Generation</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Melechovsky, Zixun Guo, Deepanway Ghosal, Navonil Majumder, Dorien Herremans, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of the text-to-music models has reached new heights due to recent
advancements in diffusion models. The controllability of various musical
aspects, however, has barely been explored. In this paper, we propose Mustango:
a music-domain-knowledge-inspired text-to-music system based on diffusion.
Mustango aims to control the generated music, not only with general text
captions, but with more rich captions that can include specific instructions
related to chords, beats, tempo, and key. At the core of Mustango is MuNet, a
Music-Domain-Knowledge-Informed UNet guidance module that steers the generated
music to include the music-specific conditions, which we predict from the text
prompt, as well as the general text embedding, during the reverse diffusion
process. To overcome the limited availability of open datasets of music with
text captions, we propose a novel data augmentation method that includes
altering the harmonic, rhythmic, and dynamic aspects of music audio and using
state-of-the-art Music Information Retrieval methods to extract the music
features which will then be appended to the existing descriptions in text
format. We release the resulting MusicBench dataset which contains over 52K
instances and includes music-theory-based descriptions in the caption text.
Through extensive experiments, we show that the quality of the music generated
by Mustango is state-of-the-art, and the controllability through music-specific
text prompts greatly outperforms other models such as MusicGen and AudioLDM2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-15T00:00:00Z">2024-03-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lodge: A Coarse to Fine <span class="highlight-title">Diffusion</span> Network for Long Dance <span class="highlight-title">Generation</span>
  Guided by the Characteristic Dance Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lodge, a network capable of generating extremely long dance
sequences conditioned on given music. We design Lodge as a two-stage coarse to
fine diffusion architecture, and propose the characteristic dance primitives
that possess significant expressiveness as intermediate representations between
two diffusion models. The first stage is global diffusion, which focuses on
comprehending the coarse-level music-dance correlation and production
characteristic dance primitives. In contrast, the second-stage is the local
diffusion, which parallelly generates detailed motion sequences under the
guidance of the dance primitives and choreographic rules. In addition, we
propose a Foot Refine Block to optimize the contact between the feet and the
ground, enhancing the physical realism of the motion. Our approach can
parallelly generate dance sequences of extremely long length, striking a
balance between global choreographic patterns and local motion quality and
expressiveness. Extensive experiments validate the efficacy of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MusicHiFi: Fast High-Fidelity Stereo Vocoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Juan-Pablo Caceres, Zhiyao Duan, Nicholas J. Bryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based audio and music generation models commonly generate music by
constructing an image representation of audio (e.g., a mel-spectrogram) and
then converting it to audio using a phase reconstruction model or vocoder.
Typical vocoders, however, produce monophonic audio at lower resolutions (e.g.,
16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an
efficient high-fidelity stereophonic vocoder. Our method employs a cascade of
three generative adversarial networks (GANs) that convert low-resolution
mel-spectrograms to audio, upsamples to high-resolution audio via bandwidth
expansion, and upmixes to stereophonic audio. Compared to previous work, we
propose 1) a unified GAN-based generator and discriminator architecture and
training procedure for each stage of our cascade, 2) a new fast, near
downsampling-compatible bandwidth extension module, and 3) a new fast
downmix-compatible mono-to-stereo upmixer that ensures the preservation of
monophonic content in the output. We evaluate our approach using both objective
and subjective listening tests and find our approach yields comparable or
better audio quality, better spatialization control, and significantly faster
inference speed compared to past work. Sound examples are at
https://MusicHiFi.github.io/web/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Multimodal <span class="highlight-title">Transformer</span> for Dimensional Emotional Recognition in
  the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Waligora, Osama Zeeshan, Haseeb Aslam, Soufiane Belharbi, Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audiovisual emotion recognition (ER) in videos has immense potential over
unimodal performance. It effectively leverages the inter- and intra-modal
dependencies between visual and auditory modalities. This work proposes a novel
audio-visual emotion recognition system utilizing a joint multimodal
transformer architecture with key-based cross-attention. This framework aims to
exploit the complementary nature of audio and visual cues (facial expressions
and vocal patterns) in videos, leading to superior performance compared to
solely relying on a single modality. The proposed model leverages separate
backbones for capturing intra-modal temporal dependencies within each modality
(audio and visual). Subsequently, a joint multimodal transformer architecture
integrates the individual modality embeddings, enabling the model to
effectively capture inter-modal (between audio and visual) and intra-modal
(within each modality) relationships. Extensive evaluations on the challenging
Affwild2 dataset demonstrate that the proposed model significantly outperforms
baseline and state-of-the-art methods in ER tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Rauch, Raphael Schwinger, Moritz Wirth, René Heinrich, Jonas Lange, Stefan Kahl, Bernhard Sick, Sven Tomforde, Christoph Scholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models have emerged as a powerful tool in avian
bioacoustics to diagnose environmental health and biodiversity. However,
inconsistencies in research pose notable challenges hindering progress in this
domain. Reliable DL models need to analyze bird calls flexibly across various
species and environments to fully harness the potential of bioacoustics in a
cost-effective passive acoustic monitoring scenario. Data fragmentation and
opacity across studies complicate a comprehensive evaluation of general model
performance. To overcome these challenges, we present the BirdSet benchmark, a
unified framework consolidating research efforts with a holistic approach for
classifying bird vocalizations in avian bioacoustics. BirdSet harmonizes
open-source bird recordings into a curated dataset collection. This unified
approach provides an in-depth understanding of model performance and identifies
potential shortcomings across different tasks. By establishing baseline results
of current models, BirdSet aims to facilitate comparability, guide subsequent
data collection, and increase accessibility for newcomers to avian
bioacoustics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress, to be submitted @DMLR next month</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Source Localization and Data Association for Time-Difference of
  Arrival Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrielle Flood, Filip Elvander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the problem of localizing multiple signal sources
based on time-difference of arrival (TDOA) measurements. In the blind setting,
in which the source signals are not known, the localization task is challenging
due to the data association problem. That is, it is not known which of the TDOA
measurements correspond to the same source. Herein, we propose to perform joint
localization and data association by means of an optimal transport formulation.
The method operates by finding optimal groupings of TDOA measurements and
associating these with candidate source locations. To allow for computationally
feasible localization in three-dimensional space, an efficient set of candidate
locations is constructed using a minimal multilateration solver based on
minimal sets of receiver pairs. In numerical simulations, we demonstrate that
the proposed method is robust both to measurement noise and TDOA detection
errors. Furthermore, it is shown that the data association provided by the
proposed method allows for statistically efficient estimates of the source
locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiscale Matching Driven by Cross-Modal Similarity Consistency for
  Audio-Text Retrieval <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Jia-Chen Gu, Zhen-Hua Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-text retrieval (ATR), which retrieves a relevant caption given an audio
clip (A2T) and vice versa (T2A), has recently attracted much research
attention. Existing methods typically aggregate information from each modality
into a single vector for matching, but this sacrifices local details and can
hardly capture intricate relationships within and between modalities.
Furthermore, current ATR datasets lack comprehensive alignment information, and
simple binary contrastive learning labels overlook the measurement of
fine-grained semantic differences between samples. To counter these challenges,
we present a novel ATR framework that comprehensively captures the matching
relationships of multimodal information from different perspectives and finer
granularities. Specifically, a fine-grained alignment method is introduced,
achieving a more detail-oriented matching through a multiscale process from
local to global levels to capture meticulous cross-modal relationships. In
addition, we pioneer the application of cross-modal similarity consistency,
leveraging intra-modal similarity relationships as soft supervision to boost
more intricate alignment. Extensive experiments validate the effectiveness of
our approach, outperforming previous methods by significant margins of at least
3.9% (T2A) / 6.9% (A2T) R@1 on the AudioCaps dataset and 2.9% (T2A) / 5.4%
(A2T) R@1 on the Clotho dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted to ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate
  Instrument Leakage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Hao Tan, Kin Wai Cheuk, Taemin Cho, Wei-Hsiang Liao, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents enhancements to the MT3 model, a state-of-the-art (SOTA)
token-based multi-instrument automatic music transcription (AMT) model. Despite
SOTA performance, MT3 has the issue of instrument leakage, where transcriptions
are fragmented across different instruments. To mitigate this, we propose
MR-MT3, with enhancements including a memory retention mechanism, prior token
sampling, and token shuffling are proposed. These methods are evaluated on the
Slakh2100 dataset, demonstrating improved onset F1 scores and reduced
instrument leakage. In addition to the conventional multi-instrument
transcription F1 score, new metrics such as the instrument leakage ratio and
the instrument detection F1 score are introduced for a more comprehensive
assessment of transcription quality. The study also explores the issue of
domain overfitting by evaluating MT3 on single-instrument monophonic datasets
such as ComMU and NSynth. The findings, along with the source code, are shared
to facilitate future work aimed at refining token-based multi-instrument AMT
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gender-ambiguous voice <span class="highlight-title">generation</span> through feminine speaking style
  transfer in male voices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Koutsogiannaki, Shafel Mc Dowall, Ioannis Agiomyrgiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, and under the umbrella of Responsible AI, efforts have been made to
develop gender-ambiguous synthetic speech to represent with a single voice all
individuals in the gender spectrum. However, research efforts have completely
overlooked the speaking style despite differences found among binary and
non-binary populations. In this work, we synthesise gender-ambiguous speech by
combining the timbre of a male speaker with the manner of speech of a female
speaker using voice morphing and pitch shifting towards the male-female
boundary. Subjective evaluations indicate that the ambiguity of the morphed
samples that convey the female speech style is higher than those that undergo
plain pitch transformations suggesting that the speaking style can be a
contributing factor in creating gender-ambiguous speech. To our knowledge, this
is the first study that explicitly uses the transfer of the speaking style to
create gender-ambiguous voices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Interspeech</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Face Detection with Audio-Based Region Proposals for
  Human-Robot Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Aris, François Grondin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient face detection is critical to provide natural human-robot
interactions. However, computer vision tends to involve a large computational
load due to the amount of data (i.e. pixels) that needs to be processed in a
short amount of time. This is undesirable on robotics platforms where multiple
processes need to run in parallel and where the processing power is limited by
portability constraints. Existing solutions often involve reducing image
quality which can negatively impact processing. The literature also reports
methods to generate regions of interest in images from pixel data. Although it
is a promising idea, these methods often involve heavy vision algorithms. In
this paper, we evaluate how audio can be used to generate regions of interest
in optical images to reduce the number of pixels to process with computer
vision. Thereby, we propose a unique attention mechanism to localize a speech
source and evaluate its impact on an existing face detection algorithm. Our
results show that the attention mechanism reduces the computational load and
offers an interesting trade-off between speed and accuracy. The proposed
pipeline is flexible and can be easily adapted to other applications such as
robot surveillance, video conferences or smart glasses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A time-causal and time-recursive analogue of the Gabor transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14512v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14512v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a time-causal analogue of the Gabor filter, as well as a
both time-causal and time-recursive analogue of the Gabor transform, where the
proposed time-causal representations obey both temporal scale covariance and a
cascade property with a simplifying kernel over temporal scales. The motivation
behind these constructions is to enable theoretically well-founded
time-frequency analysis over multiple temporal scales for real-time situations,
or for physical or biological modelling situations, when the future cannot be
accessed, and the non-causal access to future in Gabor filtering is therefore
not viable for a time-frequency analysis of the system.
  We develop the theory for these representations, obtained by replacing the
Gaussian kernel in Gabor filtering with a time-causal kernel, referred to as
the time-causal limit kernel, which guarantees simplification properties from
finer to coarser levels of scales in a time-causal situation, similar as the
Gaussian kernel can be shown to guarantee over a non-causal temporal domain. In
these ways, the proposed time-frequency representations guarantee well-founded
treatment over multiple scales, in situations when the characteristic scales in
the signals, or physical or biological phenomena, to be analyzed may vary
substantially, and additionally all steps in the time-frequency analysis have
to be fully time-causal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lodge: A Coarse to Fine <span class="highlight-title">Diffusion</span> Network for Long Dance <span class="highlight-title">Generation</span>
  Guided by the Characteristic Dance Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lodge, a network capable of generating extremely long dance
sequences conditioned on given music. We design Lodge as a two-stage coarse to
fine diffusion architecture, and propose the characteristic dance primitives
that possess significant expressiveness as intermediate representations between
two diffusion models. The first stage is global diffusion, which focuses on
comprehending the coarse-level music-dance correlation and production
characteristic dance primitives. In contrast, the second-stage is the local
diffusion, which parallelly generates detailed motion sequences under the
guidance of the dance primitives and choreographic rules. In addition, we
propose a Foot Refine Block to optimize the contact between the feet and the
ground, enhancing the physical realism of the motion. Our approach can
parallelly generate dance sequences of extremely long length, striking a
balance between global choreographic patterns and local motion quality and
expressiveness. Extensive experiments validate the efficacy of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MusicHiFi: Fast High-Fidelity Stereo Vocoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Juan-Pablo Caceres, Zhiyao Duan, Nicholas J. Bryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based audio and music generation models commonly generate music by
constructing an image representation of audio (e.g., a mel-spectrogram) and
then converting it to audio using a phase reconstruction model or vocoder.
Typical vocoders, however, produce monophonic audio at lower resolutions (e.g.,
16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an
efficient high-fidelity stereophonic vocoder. Our method employs a cascade of
three generative adversarial networks (GANs) that convert low-resolution
mel-spectrograms to audio, upsamples to high-resolution audio via bandwidth
expansion, and upmixes to stereophonic audio. Compared to previous work, we
propose 1) a unified GAN-based generator and discriminator architecture and
training procedure for each stage of our cascade, 2) a new fast, near
downsampling-compatible bandwidth extension module, and 3) a new fast
downmix-compatible mono-to-stereo upmixer that ensures the preservation of
monophonic content in the output. We evaluate our approach using both objective
and subjective listening tests and find our approach yields comparable or
better audio quality, better spatialization control, and significantly faster
inference speed compared to past work. Sound examples are at
https://MusicHiFi.github.io/web/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Multimodal <span class="highlight-title">Transformer</span> for Dimensional Emotional Recognition in
  the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Waligora, Osama Zeeshan, Haseeb Aslam, Soufiane Belharbi, Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audiovisual emotion recognition (ER) in videos has immense potential over
unimodal performance. It effectively leverages the inter- and intra-modal
dependencies between visual and auditory modalities. This work proposes a novel
audio-visual emotion recognition system utilizing a joint multimodal
transformer architecture with key-based cross-attention. This framework aims to
exploit the complementary nature of audio and visual cues (facial expressions
and vocal patterns) in videos, leading to superior performance compared to
solely relying on a single modality. The proposed model leverages separate
backbones for capturing intra-modal temporal dependencies within each modality
(audio and visual). Subsequently, a joint multimodal transformer architecture
integrates the individual modality embeddings, enabling the model to
effectively capture inter-modal (between audio and visual) and intra-modal
(within each modality) relationships. Extensive evaluations on the challenging
Affwild2 dataset demonstrate that the proposed model significantly outperforms
baseline and state-of-the-art methods in ER tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to train your ears: Auditory-model emulation for large-dynamic-range
  inputs and mild-to-severe hearing losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Leer, Jesper Jensen, Zheng-Hua Tan, Jan Østergaard, Lars Bramsløw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced auditory models are useful in designing signal-processing algorithms
for hearing-loss compensation or speech enhancement. Such auditory models
provide rich and detailed descriptions of the auditory pathway, and might allow
for individualization of signal-processing strategies, based on physiological
measurements. However, these auditory models are often computationally
demanding, requiring significant time to compute. To address this issue,
previous studies have explored the use of deep neural networks to emulate
auditory models and reduce inference time. While these deep neural networks
offer impressive efficiency gains in terms of computational time, they may
suffer from uneven emulation performance as a function of auditory-model
frequency-channels and input sound pressure level, making them unsuitable for
many tasks. In this study, we demonstrate that the conventional
machine-learning optimization objective used in existing state-of-the-art
methods is the primary source of this limitation. Specifically, the
optimization objective fails to account for the frequency- and
level-dependencies of the auditory model, caused by a large input dynamic range
and different types of hearing losses emulated by the auditory model. To
overcome this limitation, we propose a new optimization objective that
explicitly embeds the frequency- and level-dependencies of the auditory model.
Our results show that this new optimization objective significantly improves
the emulation performance of deep neural networks across relevant input sound
levels and auditory-model frequency channels, without increasing the
computational load during inference. Addressing these limitations is essential
for advancing the application of auditory models in signal-processing tasks,
ensuring their efficacy in diverse scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE/ACM Transactions on Audio, Speech and Language
  Processing. This version is the authors' version and may vary from the final
  publication in details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Networks Hear You Loud And Clear: Hearing Loss Compensation Using
  Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Leer, Jesper Jensen, Laurel Carney, Zheng-Hua Tan, Jan Østergaard, Lars Bramsløw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article investigates the use of deep neural networks (DNNs) for
hearing-loss compensation. Hearing loss is a prevalent issue affecting millions
of people worldwide, and conventional hearing aids have limitations in
providing satisfactory compensation. DNNs have shown remarkable performance in
various auditory tasks, including speech recognition, speaker identification,
and music classification. In this study, we propose a DNN-based approach for
hearing-loss compensation, which is trained on the outputs of hearing-impaired
and normal-hearing DNN-based auditory models in response to speech signals.
First, we introduce a framework for emulating auditory models using DNNs,
focusing on an auditory-nerve model in the auditory pathway. We propose a
linearization of the DNN-based approach, which we use to analyze the DNN-based
hearing-loss compensation. Additionally we develop a simple approach to choose
the acoustic center frequencies of the auditory model used for the compensation
strategy. Finally, we evaluate the DNN-based hearing-loss compensation
strategies using listening tests with hearing impaired listeners. The results
demonstrate that the proposed approach results in feasible hearing-loss
compensation strategies. Our proposed approach was shown to provide an increase
in speech intelligibility and was found to outperform a conventional approach
in terms of perceived speech quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Rauch, Raphael Schwinger, Moritz Wirth, René Heinrich, Jonas Lange, Stefan Kahl, Bernhard Sick, Sven Tomforde, Christoph Scholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models have emerged as a powerful tool in avian
bioacoustics to diagnose environmental health and biodiversity. However,
inconsistencies in research pose notable challenges hindering progress in this
domain. Reliable DL models need to analyze bird calls flexibly across various
species and environments to fully harness the potential of bioacoustics in a
cost-effective passive acoustic monitoring scenario. Data fragmentation and
opacity across studies complicate a comprehensive evaluation of general model
performance. To overcome these challenges, we present the BirdSet benchmark, a
unified framework consolidating research efforts with a holistic approach for
classifying bird vocalizations in avian bioacoustics. BirdSet harmonizes
open-source bird recordings into a curated dataset collection. This unified
approach provides an in-depth understanding of model performance and identifies
potential shortcomings across different tasks. By establishing baseline results
of current models, BirdSet aims to facilitate comparability, guide subsequent
data collection, and increase accessibility for newcomers to avian
bioacoustics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress, to be submitted @DMLR next month</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Source Localization and Data Association for Time-Difference of
  Arrival Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrielle Flood, Filip Elvander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the problem of localizing multiple signal sources
based on time-difference of arrival (TDOA) measurements. In the blind setting,
in which the source signals are not known, the localization task is challenging
due to the data association problem. That is, it is not known which of the TDOA
measurements correspond to the same source. Herein, we propose to perform joint
localization and data association by means of an optimal transport formulation.
The method operates by finding optimal groupings of TDOA measurements and
associating these with candidate source locations. To allow for computationally
feasible localization in three-dimensional space, an efficient set of candidate
locations is constructed using a minimal multilateration solver based on
minimal sets of receiver pairs. In numerical simulations, we demonstrate that
the proposed method is robust both to measurement noise and TDOA detection
errors. Furthermore, it is shown that the data association provided by the
proposed method allows for statistically efficient estimates of the source
locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperME: Supervised and Mixture-to-Mixture Co-Learning for Speech
  Enhancement and Robust ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhong-Qiu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current dominant approach for neural speech enhancement is based on
supervised learning by using simulated training data. The trained models,
however, often exhibit limited generalizability to real-recorded data. To
address this, we investigate training models directly on real target-domain
data, and propose two algorithms, mixture-to-mixture (M2M) training and a
co-learning algorithm that improves M2M with the help of supervised algorithms.
When paired close-talk and far-field mixtures are available for training, M2M
realizes speech enhancement by training a deep neural network (DNN) to produce
speech and noise estimates in a way such that they can be linearly filtered to
reconstruct the close-talk and far-field mixtures. This way, the DNN can be
trained directly on real mixtures, and can leverage close-talk mixtures as a
weak supervision to enhance far-field mixtures. To improve M2M, we combine it
with supervised approaches to co-train the DNN, where mini-batches of real
close-talk and far-field mixture pairs and mini-batches of simulated mixture
and clean speech pairs are alternately fed to the DNN, and the loss functions
are respectively (a) the mixture reconstruction loss on the real close-talk and
far-field mixtures and (b) the regular enhancement loss on the simulated clean
speech and noise. We find that, this way, the DNN can learn from real and
simulated data to achieve better generalization to real data. We name this
algorithm SuperME, $\underline{super}$vised and
$\underline{m}$ixture-to-mixtur$\underline{e}$ co-learning. Evaluation results
on the CHiME-4 dataset show its effectiveness and potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiscale Matching Driven by Cross-Modal Similarity Consistency for
  Audio-Text Retrieval <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Jia-Chen Gu, Zhen-Hua Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-text retrieval (ATR), which retrieves a relevant caption given an audio
clip (A2T) and vice versa (T2A), has recently attracted much research
attention. Existing methods typically aggregate information from each modality
into a single vector for matching, but this sacrifices local details and can
hardly capture intricate relationships within and between modalities.
Furthermore, current ATR datasets lack comprehensive alignment information, and
simple binary contrastive learning labels overlook the measurement of
fine-grained semantic differences between samples. To counter these challenges,
we present a novel ATR framework that comprehensively captures the matching
relationships of multimodal information from different perspectives and finer
granularities. Specifically, a fine-grained alignment method is introduced,
achieving a more detail-oriented matching through a multiscale process from
local to global levels to capture meticulous cross-modal relationships. In
addition, we pioneer the application of cross-modal similarity consistency,
leveraging intra-modal similarity relationships as soft supervision to boost
more intricate alignment. Extensive experiments validate the effectiveness of
our approach, outperforming previous methods by significant margins of at least
3.9% (T2A) / 6.9% (A2T) R@1 on the AudioCaps dataset and 2.9% (T2A) / 5.4%
(A2T) R@1 on the Clotho dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted to ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate
  Instrument Leakage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Hao Tan, Kin Wai Cheuk, Taemin Cho, Wei-Hsiang Liao, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents enhancements to the MT3 model, a state-of-the-art (SOTA)
token-based multi-instrument automatic music transcription (AMT) model. Despite
SOTA performance, MT3 has the issue of instrument leakage, where transcriptions
are fragmented across different instruments. To mitigate this, we propose
MR-MT3, with enhancements including a memory retention mechanism, prior token
sampling, and token shuffling are proposed. These methods are evaluated on the
Slakh2100 dataset, demonstrating improved onset F1 scores and reduced
instrument leakage. In addition to the conventional multi-instrument
transcription F1 score, new metrics such as the instrument leakage ratio and
the instrument detection F1 score are introduced for a more comprehensive
assessment of transcription quality. The study also explores the issue of
domain overfitting by evaluating MT3 on single-instrument monophonic datasets
such as ComMU and NSynth. The findings, along with the source code, are shared
to facilitate future work aimed at refining token-based multi-instrument AMT
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WavCraft: Audio Editing and <span class="highlight-title">Generation</span> with Natural Language Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhua Liang, Huan Zhang, Haohe Liu, Yin Cao, Qiuqiang Kong, Xubo Liu, Wenwu Wang, Mark D. Plumbley, Huy Phan, Emmanouil Benetos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce WavCraft, a collective system that leverages large language
models (LLMs) to connect diverse task-specific models for audio content
creation and editing. Specifically, WavCraft describes the content of raw sound
materials in natural language and prompts the LLM conditioned on audio
descriptions and users' requests. WavCraft leverages the in-context learning
ability of the LLM to decomposes users' instructions into several tasks and
tackle each task collaboratively with audio expert modules. Through task
decomposition along with a set of task-specific models, WavCraft follows the
input instruction to create or edit audio content with more details and
rationales, facilitating users' control. In addition, WavCraft is able to
cooperate with users via dialogue interaction and even produce the audio
content without explicit user commands. Experiments demonstrate that WavCraft
yields a better performance than existing methods, especially when adjusting
the local regions of audio clips. Moreover, WavCraft can follow complex
instructions to edit and even create audio content on the top of input
recordings, facilitating audio producers in a broader range of applications.
Our implementation and demos are available at
https://github.com/JinhuaLiang/WavCraft.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gender-ambiguous voice <span class="highlight-title">generation</span> through feminine speaking style
  transfer in male voices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Koutsogiannaki, Shafel Mc Dowall, Ioannis Agiomyrgiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, and under the umbrella of Responsible AI, efforts have been made to
develop gender-ambiguous synthetic speech to represent with a single voice all
individuals in the gender spectrum. However, research efforts have completely
overlooked the speaking style despite differences found among binary and
non-binary populations. In this work, we synthesise gender-ambiguous speech by
combining the timbre of a male speaker with the manner of speech of a female
speaker using voice morphing and pitch shifting towards the male-female
boundary. Subjective evaluations indicate that the ambiguity of the morphed
samples that convey the female speech style is higher than those that undergo
plain pitch transformations suggesting that the speaking style can be a
contributing factor in creating gender-ambiguous speech. To our knowledge, this
is the first study that explicitly uses the transfer of the speaking style to
create gender-ambiguous voices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Interspeech</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Face Detection with Audio-Based Region Proposals for
  Human-Robot Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Aris, François Grondin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient face detection is critical to provide natural human-robot
interactions. However, computer vision tends to involve a large computational
load due to the amount of data (i.e. pixels) that needs to be processed in a
short amount of time. This is undesirable on robotics platforms where multiple
processes need to run in parallel and where the processing power is limited by
portability constraints. Existing solutions often involve reducing image
quality which can negatively impact processing. The literature also reports
methods to generate regions of interest in images from pixel data. Although it
is a promising idea, these methods often involve heavy vision algorithms. In
this paper, we evaluate how audio can be used to generate regions of interest
in optical images to reduce the number of pixels to process with computer
vision. Thereby, we propose a unique attention mechanism to localize a speech
source and evaluate its impact on an existing face detection algorithm. Our
results show that the attention mechanism reduces the computational load and
offers an interesting trade-off between speed and accuracy. The proposed
pipeline is flexible and can be easily adapted to other applications such as
robot surveillance, video conferences or smart glasses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A time-causal and time-recursive analogue of the Gabor transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14512v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14512v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a time-causal analogue of the Gabor filter, as well as a
both time-causal and time-recursive analogue of the Gabor transform, where the
proposed time-causal representations obey both temporal scale covariance and a
cascade property with a simplifying kernel over temporal scales. The motivation
behind these constructions is to enable theoretically well-founded
time-frequency analysis over multiple temporal scales for real-time situations,
or for physical or biological modelling situations, when the future cannot be
accessed, and the non-causal access to future in Gabor filtering is therefore
not viable for a time-frequency analysis of the system.
  We develop the theory for these representations, obtained by replacing the
Gaussian kernel in Gabor filtering with a time-causal kernel, referred to as
the time-causal limit kernel, which guarantees simplification properties from
finer to coarser levels of scales in a time-causal situation, similar as the
Gaussian kernel can be shown to guarantee over a non-causal temporal domain. In
these ways, the proposed time-frequency representations guarantee well-founded
treatment over multiple scales, in situations when the characteristic scales in
the signals, or physical or biological phenomena, to be analyzed may vary
substantially, and additionally all steps in the time-frequency analysis have
to be fully time-causal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleTalker: One-shot Style-based Audio-driven Talking Head Video
  <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongchan Min, Minyoung Song, Eunji Ko, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose StyleTalker, a novel audio-driven talking head generation model
that can synthesize a video of a talking person from a single reference image
with accurately audio-synced lip shapes, realistic head poses, and eye blinks.
Specifically, by leveraging a pretrained image generator and an image encoder,
we estimate the latent codes of the talking head video that faithfully reflects
the given audio. This is made possible with several newly devised components:
1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A
conditional sequential variational autoencoder that learns the latent motion
space disentangled from the lip movements, such that we can independently
manipulate the motions and lip movements while preserving the identity. 3) An
auto-regressive prior augmented with normalizing flow to learn a complex
audio-to-motion multi-modal latent space. Equipped with these components,
StyleTalker can generate talking head videos not only in a motion-controllable
way when another motion source video is given but also in a completely
audio-driven manner by inferring realistic motions from the input audio.
Through extensive experiments and user studies, we show that our model is able
to synthesize talking head videos with impressive perceptual quality which are
accurately lip-synced with the input audios, largely outperforming
state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-14T00:00:00Z">2024-03-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyass Moummad, Nicolas Farrugia, Romain Serizel, Jeremy Froidevaux, Vincent Lostanlen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label imbalanced classification poses a significant challenge in
machine learning, particularly evident in bioacoustics where animal sounds
often co-occur, and certain sounds are much less frequent than others. This
paper focuses on the specific case of classifying anuran species sounds using
the dataset AnuraSet, that contains both class imbalance and multi-label
examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a
framework that leverages mixing regularization methods Mixup, Manifold Mixup,
and MultiMix. Experimental results show that these methods, individually, may
lead to suboptimal results; however, when applied randomly, with one selected
at each training iteration, they prove effective in addressing the mentioned
challenges, particularly for rare classes with few occurrences. Further
analysis reveals that Mix2 is also proficient in classifying sounds across
various levels of class co-occurrences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ uaMix-MAE: Efficient Tuning of <span class="highlight-title">Pretrain</span>ed Audio <span class="highlight-title">Transformer</span>s with
  Unsupervised Audio Mixtures <span class="chip">ICASSP'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afrina Tabassum, Dung Tran, Trung Dang, Ismini Lourentzou, Kazuhito Koishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Autoencoders (MAEs) learn rich low-level representations from
unlabeled data but require substantial labeled data to effectively adapt to
downstream tasks. Conversely, Instance Discrimination (ID) emphasizes
high-level semantics, offering a potential solution to alleviate annotation
requirements in MAEs. Although combining these two approaches can address
downstream tasks with limited labeled data, naively integrating ID into MAEs
leads to extended training times and high computational costs. To address this
challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that
leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE
aligns the representations of pretrained MAEs, thereby facilitating effective
adaptation to task-specific semantics. To optimize the model with small amounts
of unlabeled data, we propose an audio mixing technique that manipulates audio
samples in both input and virtual label spaces. Experiments in low/few-shot
settings demonstrate that \modelname achieves 4-6% accuracy improvements over
various benchmarks when tuned with limited unlabeled data, such as
AudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, 4 tables. To appear in ICASSP'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Neural-SRP method for positional sound source localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Grinstein, Toon van Waterschoot, Mike Brookes, Patrick A. Naylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steered Response Power (SRP) is a widely used method for the task of sound
source localization using microphone arrays, showing satisfactory localization
performance on many practical scenarios. However, its performance is diminished
under highly reverberant environments. Although Deep Neural Networks (DNNs)
have been previously proposed to overcome this limitation, most are trained for
a specific number of microphones with fixed spatial coordinates. This restricts
their practical application on scenarios frequently observed in wireless
acoustic sensor networks, where each application has an ad-hoc microphone
topology. We propose Neural-SRP, a DNN which combines the flexibility of SRP
with the performance gains of DNNs. We train our network using simulated data
and transfer learning, and evaluate our approach on recorded and simulated
data. Results verify that Neural-SRP's localization performance significantly
outperforms the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Asilomar Conference on Signals, Systems, and Computers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in
  Cognitive Load Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the M&M model, a novel multimodal-multitask learning
framework, applied to the AVCAffe dataset for cognitive load assessment (CLA).
M&M uniquely integrates audiovisual cues through a dual-pathway architecture,
featuring specialized streams for audio and video inputs. A key innovation lies
in its cross-modality multihead attention mechanism, fusing the different
modalities for synchronized multitasking. Another notable feature is the
model's three specialized branches, each tailored to a specific cognitive load
label, enabling nuanced, task-specific analysis. While it shows modest
performance compared to the AVCAffe's single-task baseline, M\&M demonstrates a
promising framework for integrated multimodal processing. This work paves the
way for future enhancements in multimodal-multitask learning systems,
emphasizing the fusion of diverse data types for complex task handling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LM2D: Lyrics- and Music-Driven Dance <span class="highlight-title">Synthesis</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Yin, Xuejiao Zhao, Yi Yu, Hang Yin, Danica Kragic, Mårten Björkman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dance typically involves professional choreography with complex movements
that follow a musical rhythm and can also be influenced by lyrical content. The
integration of lyrics in addition to the auditory dimension, enriches the
foundational tone and makes motion generation more amenable to its semantic
meanings. However, existing dance synthesis methods tend to model motions only
conditioned on audio signals. In this work, we make two contributions to bridge
this gap. First, we propose LM2D, a novel probabilistic architecture that
incorporates a multimodal diffusion model with consistency distillation,
designed to create dance conditioned on both music and lyrics in one diffusion
generation step. Second, we introduce the first 3D dance-motion dataset that
encompasses both music and lyrics, obtained with pose estimation technologies.
We evaluate our model against music-only baseline models with objective metrics
and human evaluations, including dancers and choreographers. The results
demonstrate LM2D is able to produce realistic and diverse dance matching both
lyrics and music. A video summary can be accessed at:
https://youtu.be/4XCgvYookvA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Practical Guide to Spectrogram Analysis for Audio Signal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zulfidin Khodzhaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper summarizes spectrogram and gives practical application of
spectrogram in signal processing. For analysis, finger-snapping is recorded
with a sampling rate of 441000 Hz and 96000 Hz. The effects of the number of
segments on the Power Spectral Density (PSD) and spectrogram are analyzed and
visualized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More than words: Advancements and challenges in speech recognition for
  singing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Kruspe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges and advancements in speech recognition
for singing, a domain distinctly different from standard speech recognition.
Singing encompasses unique challenges, including extensive pitch variations,
diverse vocal styles, and background music interference. We explore key areas
such as phoneme recognition, language identification in songs, keyword
spotting, and full lyrics transcription. I will describe some of my own
experiences when performing research on these tasks just as they were starting
to gain traction, but will also show how recent developments in deep learning
and large-scale datasets have propelled progress in this field. My goal is to
illuminate the complexities of applying speech recognition to singing, evaluate
current capabilities, and outline future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Electronic Speech Signal Processing (ESSV) 2024,
  Keynote</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from
  Acoustic Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Wang, Xiaomeng Li, Na Li, Longlong Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aimed to develop a deep learning model for the classification of
bearing faults in wind turbine generators from acoustic signals. A
convolutional LSTM model was successfully constructed and trained by using
audio data from five predefined fault types for both training and validation.
To create the dataset, raw audio signal data was collected and processed in
frames to capture time and frequency domain information. The model exhibited
outstanding accuracy on training samples and demonstrated excellent
generalization ability during validation, indicating its proficiency of
generalization capability. On the test samples, the model achieved remarkable
classification performance, with an overall accuracy exceeding 99.5%, and a
false positive rate of less than 1% for normal status. The findings of this
study provide essential support for the diagnosis and maintenance of bearing
faults in wind turbine generators, with the potential to enhance the
reliability and efficiency of wind power generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audiosockets: A Python socket package for Real-Time Audio Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Shu, David V. Anderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are many packages in Python which allow one to perform real-time
processing on audio data. Unfortunately, due to the synchronous nature of the
language, there lacks a framework which allows for distributed parallel
processing of the data without requiring a large programming overhead and in
which the data acquisition is not blocked by subsequent processing operations.
This work improves on packages used for audio data collection with a
light-weight backend and a simple interface that allows for distributed
processing through a socket-based structure. This is intended for real-time
audio machine learning and data processing in Python with a quick deployment of
multiple parallel operations on the same data, allowing users to spend less
time debugging and more time developing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux
  pour la détection du trouble de stress post-traumatique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to provide a more objective and quicker way to diagnose
post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two
unimodal convolutional neural networks and which gives low detection error
rate. By taking only videos and audios as inputs, the model could be used in
the configuration of teleconsultation sessions, in the optimization of patient
journeys or for human-robot interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language. GRETSI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification
  of Spoken Numbers in Different Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Groh, Nina Goes, Andreas M. Kist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking plays a pivotal role in assessing and enhancing the performance
of compact deep learning models designed for execution on resource-constrained
devices, such as microcontrollers. Our study introduces a novel, entirely
artificially generated benchmarking dataset tailored for speech recognition,
representing a core challenge in the field of tiny deep learning. SpokeN-100
consists of spoken numbers from 0 to 99 spoken by 32 different speakers in four
different languages, namely English, Mandarin, German and French, resulting in
12,800 audio samples. We determine auditory features and use UMAP (Uniform
Manifold Approximation and Projection for Dimension Reduction) as a
dimensionality reduction method to show the diversity and richness of the
dataset. To highlight the use case of the dataset, we introduce two benchmark
tasks: given an audio sample, classify (i) the used language and/or (ii) the
spoken number. We optimized state-of-the-art deep neural networks and performed
an evolutionary neural architecture search to find tiny architectures optimized
for the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent
the first benchmark data achieved for SpokeN-100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper by the tinyML Research Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MINT: Boosting Audio-Language Model via Multi-Target <span class="highlight-title">Pre-Train</span>ing and
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07485v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07485v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhao, Yifei Xin, Zhesong Yu, Bilei Zhu, Lu Lu, Zejun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of audio-language pre-training (ALP), the challenge of achieving
cross-modal alignment is significant. Moreover, the integration of audio inputs
with diverse distributions and task variations poses challenges in developing
generic audio-language models. In this study, we introduce MINT, a novel ALP
framework boosting audio-language models through multi-target pre-training and
instruction tuning. MINT leverages the strength of frozen pre-trained audio
encoders and large language models (LLMs) to improve audio-language
pre-training, enabling effective transferablility to both audio-text
understanding and generation tasks. To address the modality gap, we propose
Bridge-Net, a lightweight trainable module that enhances cross-modality
alignment and the model's ability to follow instructions for a variety of
audio-text tasks. Bridge-Net is pivotal within MINT, initially enhancing
audio-language representation learning through a multi-target pre-training
approach. Subsequently, Bridge-Net further boosts audio-to-language generative
learning by integrating a frozen language model with instruction tuning. This
integration empowers MINT to extract features in a flexible and effective
manner, specifically tailored to the provided instructions for diverse tasks.
Experimental results demonstrate that MINT attains superior performance across
various audio-language understanding and generation tasks, highlighting its
robust generalization capabilities even in zero-shot scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GESI: Gammachirp Envelope Similarity Index for Predicting
  Intelligibility of Simulated Hearing Loss Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15399v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15399v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayako Yamamoto, Toshio Irino, Fuki Miyazaki, Honoka Tamaru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an objective intelligibility measure (OIM), called the Gammachirp
Envelope Similarity Index (GESI), which can predict the speech intelligibility
(SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners.
GESI is an intrusive method that computes the SI metric using the gammachirp
filterbank (GCFB), the modulation filterbank, and the extended cosine
similarity measure. The unique features of GESI are that i) it reflects the
hearing impaired (HI) listener's HL that appears in the audiogram and is caused
by active and passive cochlear dysfunction, ii) it provides a single goodness
metric, as in the widely used STOI and ESTOI, that can be used immediately to
evaluate SE algorithms, and iii) it provides a simple control parameter to
accept the level asymmetry of the reference and test sounds and to deal with
individual listening conditions and environments. We evaluated GESI and the
conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using
four SI experiments on words of male and female speech sounds in both
laboratory and remote environments. GESI was shown to outperform the other OIMs
in the evaluations. GESI could be used to improve SE algorithms in assistive
listening devices for individual HI listeners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to JASA on March 14, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Turn-taking Prediction Using Voice Activity Projection <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of voice activity projection (VAP), a
predictive turn-taking model for spoken dialogue, on multilingual data,
encompassing English, Mandarin, and Japanese. The VAP model continuously
predicts the upcoming voice activities of participants in dyadic dialogue,
leveraging a cross-attention Transformer to capture the dynamic interplay
between participants. The results show that a monolingual VAP model trained on
one language does not make good predictions when applied to other languages.
However, a multilingual model, trained on all three languages, demonstrates
predictive performance on par with monolingual models across all languages.
Further analyses show that the multilingual model has learned to discern the
language of the input signal. We also analyze the sensitivity to pitch, a
prosodic cue that is thought to be important for turn-taking. Finally, we
compare two different audio encoders, contrastive predictive coding (CPC)
pre-trained on English, with a recent model based on multilingual wav2vec 2.0
(MMS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Brain Exhibits Distinct Patterns When Listening to Fake Versus
  Real Audio: Preliminary Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Salehi, Kalin Stefanov, Ehsan Shareghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we study the variations in human brain activity when listening
to real and fake audio. Our preliminary results suggest that the
representations learned by a state-of-the-art deepfake audio detection
algorithm, do not exhibit clear distinct patterns between real and fake audio.
In contrast, human brain activity, as measured by EEG, displays distinct
patterns when individuals are exposed to fake versus real audio. This
preliminary evidence enables future research directions in areas such as
deepfake audio detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyass Moummad, Nicolas Farrugia, Romain Serizel, Jeremy Froidevaux, Vincent Lostanlen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label imbalanced classification poses a significant challenge in
machine learning, particularly evident in bioacoustics where animal sounds
often co-occur, and certain sounds are much less frequent than others. This
paper focuses on the specific case of classifying anuran species sounds using
the dataset AnuraSet, that contains both class imbalance and multi-label
examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a
framework that leverages mixing regularization methods Mixup, Manifold Mixup,
and MultiMix. Experimental results show that these methods, individually, may
lead to suboptimal results; however, when applied randomly, with one selected
at each training iteration, they prove effective in addressing the mentioned
challenges, particularly for rare classes with few occurrences. Further
analysis reveals that Mix2 is also proficient in classifying sounds across
various levels of class co-occurrences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ uaMix-MAE: Efficient Tuning of <span class="highlight-title">Pretrain</span>ed Audio <span class="highlight-title">Transformer</span>s with
  Unsupervised Audio Mixtures <span class="chip">ICASSP'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afrina Tabassum, Dung Tran, Trung Dang, Ismini Lourentzou, Kazuhito Koishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Autoencoders (MAEs) learn rich low-level representations from
unlabeled data but require substantial labeled data to effectively adapt to
downstream tasks. Conversely, Instance Discrimination (ID) emphasizes
high-level semantics, offering a potential solution to alleviate annotation
requirements in MAEs. Although combining these two approaches can address
downstream tasks with limited labeled data, naively integrating ID into MAEs
leads to extended training times and high computational costs. To address this
challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that
leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE
aligns the representations of pretrained MAEs, thereby facilitating effective
adaptation to task-specific semantics. To optimize the model with small amounts
of unlabeled data, we propose an audio mixing technique that manipulates audio
samples in both input and virtual label spaces. Experiments in low/few-shot
settings demonstrate that \modelname achieves 4-6% accuracy improvements over
various benchmarks when tuned with limited unlabeled data, such as
AudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, 4 tables. To appear in ICASSP'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WavCraft: Audio Editing and <span class="highlight-title">Generation</span> with Natural Language Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhua Liang, Huan Zhang, Haohe Liu, Yin Cao, Qiuqiang Kong, Xubo Liu, Wenwu Wang, Mark D. Plumbley, Huy Phan, Emmanouil Benetos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce WavCraft, a collective system that leverages large language
models (LLMs) to connect diverse task-specific models for audio content
creation and editing. Specifically, WavCraft describes the content of raw sound
materials in natural language and prompts the LLM conditioned on audio
descriptions and users' requests. WavCraft leverages the in-context learning
ability of the LLM to decomposes users' instructions into several tasks and
tackle each task collaboratively with audio expert modules. Through task
decomposition along with a set of task-specific models, WavCraft follows the
input instruction to create or edit audio content with more details and
rationales, facilitating users' control. In addition, WavCraft is able to
cooperate with users via dialogue interaction and even produce the audio
content without explicit user commands. Experiments demonstrate that WavCraft
yields a better performance than existing methods, especially when adjusting
the local regions of audio clips. Moreover, WavCraft can follow complex
instructions to edit and even create audio content on the top of input
recordings, facilitating audio producers in a broader range of applications.
Our implementation and demos are available at
https://github.com/JinhuaLiang/WavCraft.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Neural Network for Volumetric Sound field
  Reconstruction of Speech Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Olivieri, Xenofon Karakonstantis, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti, Efren Fernandez-Grande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in acoustic signal processing have seen the integration
of deep learning methodologies, alongside the continued prominence of classical
wave expansion-based approaches, particularly in sound field reconstruction.
Physics-Informed Neural Networks (PINNs) have emerged as a novel framework,
bridging the gap between data-driven and model-based techniques for addressing
physical phenomena governed by partial differential equations. This paper
introduces a PINN-based approach for the recovery of arbitrary volumetric
acoustic fields. The network incorporates the wave equation to impose a
regularization on signal reconstruction in the time domain. This methodology
enables the network to learn the underlying physics of sound propagation and
allows for the complete characterization of the sound field based on a limited
set of observations. The proposed method's efficacy is validated through
experiments involving speech signals in a real-world environment, considering
varying numbers of available measurements. Moreover, a comparative analysis is
undertaken against state-of-the-art frequency-domain and time-domain
reconstruction methods from existing literature, highlighting the increased
accuracy across the various measurement configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EURASIP Journal on Audio, Speech, and Music Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Neural-SRP method for positional sound source localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Grinstein, Toon van Waterschoot, Mike Brookes, Patrick A. Naylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steered Response Power (SRP) is a widely used method for the task of sound
source localization using microphone arrays, showing satisfactory localization
performance on many practical scenarios. However, its performance is diminished
under highly reverberant environments. Although Deep Neural Networks (DNNs)
have been previously proposed to overcome this limitation, most are trained for
a specific number of microphones with fixed spatial coordinates. This restricts
their practical application on scenarios frequently observed in wireless
acoustic sensor networks, where each application has an ad-hoc microphone
topology. We propose Neural-SRP, a DNN which combines the flexibility of SRP
with the performance gains of DNNs. We train our network using simulated data
and transfer learning, and evaluate our approach on recorded and simulated
data. Results verify that Neural-SRP's localization performance significantly
outperforms the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Asilomar Conference on Signals, Systems, and Computers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in
  Cognitive Load Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the M&M model, a novel multimodal-multitask learning
framework, applied to the AVCAffe dataset for cognitive load assessment (CLA).
M&M uniquely integrates audiovisual cues through a dual-pathway architecture,
featuring specialized streams for audio and video inputs. A key innovation lies
in its cross-modality multihead attention mechanism, fusing the different
modalities for synchronized multitasking. Another notable feature is the
model's three specialized branches, each tailored to a specific cognitive load
label, enabling nuanced, task-specific analysis. While it shows modest
performance compared to the AVCAffe's single-task baseline, M\&M demonstrates a
promising framework for integrated multimodal processing. This work paves the
way for future enhancements in multimodal-multitask learning systems,
emphasizing the fusion of diverse data types for complex task handling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LM2D: Lyrics- and Music-Driven Dance <span class="highlight-title">Synthesis</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Yin, Xuejiao Zhao, Yi Yu, Hang Yin, Danica Kragic, Mårten Björkman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dance typically involves professional choreography with complex movements
that follow a musical rhythm and can also be influenced by lyrical content. The
integration of lyrics in addition to the auditory dimension, enriches the
foundational tone and makes motion generation more amenable to its semantic
meanings. However, existing dance synthesis methods tend to model motions only
conditioned on audio signals. In this work, we make two contributions to bridge
this gap. First, we propose LM2D, a novel probabilistic architecture that
incorporates a multimodal diffusion model with consistency distillation,
designed to create dance conditioned on both music and lyrics in one diffusion
generation step. Second, we introduce the first 3D dance-motion dataset that
encompasses both music and lyrics, obtained with pose estimation technologies.
We evaluate our model against music-only baseline models with objective metrics
and human evaluations, including dancers and choreographers. The results
demonstrate LM2D is able to produce realistic and diverse dance matching both
lyrics and music. A video summary can be accessed at:
https://youtu.be/4XCgvYookvA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Practical Guide to Spectrogram Analysis for Audio Signal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zulfidin Khodzhaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper summarizes spectrogram and gives practical application of
spectrogram in signal processing. For analysis, finger-snapping is recorded
with a sampling rate of 441000 Hz and 96000 Hz. The effects of the number of
segments on the Power Spectral Density (PSD) and spectrogram are analyzed and
visualized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More than words: Advancements and challenges in speech recognition for
  singing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Kruspe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges and advancements in speech recognition
for singing, a domain distinctly different from standard speech recognition.
Singing encompasses unique challenges, including extensive pitch variations,
diverse vocal styles, and background music interference. We explore key areas
such as phoneme recognition, language identification in songs, keyword
spotting, and full lyrics transcription. I will describe some of my own
experiences when performing research on these tasks just as they were starting
to gain traction, but will also show how recent developments in deep learning
and large-scale datasets have propelled progress in this field. My goal is to
illuminate the complexities of applying speech recognition to singing, evaluate
current capabilities, and outline future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Electronic Speech Signal Processing (ESSV) 2024,
  Keynote</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from
  Acoustic Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Wang, Xiaomeng Li, Na Li, Longlong Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aimed to develop a deep learning model for the classification of
bearing faults in wind turbine generators from acoustic signals. A
convolutional LSTM model was successfully constructed and trained by using
audio data from five predefined fault types for both training and validation.
To create the dataset, raw audio signal data was collected and processed in
frames to capture time and frequency domain information. The model exhibited
outstanding accuracy on training samples and demonstrated excellent
generalization ability during validation, indicating its proficiency of
generalization capability. On the test samples, the model achieved remarkable
classification performance, with an overall accuracy exceeding 99.5%, and a
false positive rate of less than 1% for normal status. The findings of this
study provide essential support for the diagnosis and maintenance of bearing
faults in wind turbine generators, with the potential to enhance the
reliability and efficiency of wind power generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audiosockets: A Python socket package for Real-Time Audio Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Shu, David V. Anderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are many packages in Python which allow one to perform real-time
processing on audio data. Unfortunately, due to the synchronous nature of the
language, there lacks a framework which allows for distributed parallel
processing of the data without requiring a large programming overhead and in
which the data acquisition is not blocked by subsequent processing operations.
This work improves on packages used for audio data collection with a
light-weight backend and a simple interface that allows for distributed
processing through a socket-based structure. This is intended for real-time
audio machine learning and data processing in Python with a quick deployment of
multiple parallel operations on the same data, allowing users to spend less
time debugging and more time developing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux
  pour la détection du trouble de stress post-traumatique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to provide a more objective and quicker way to diagnose
post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two
unimodal convolutional neural networks and which gives low detection error
rate. By taking only videos and audios as inputs, the model could be used in
the configuration of teleconsultation sessions, in the optimization of patient
journeys or for human-robot interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language. GRETSI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification
  of Spoken Numbers in Different Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Groh, Nina Goes, Andreas M. Kist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking plays a pivotal role in assessing and enhancing the performance
of compact deep learning models designed for execution on resource-constrained
devices, such as microcontrollers. Our study introduces a novel, entirely
artificially generated benchmarking dataset tailored for speech recognition,
representing a core challenge in the field of tiny deep learning. SpokeN-100
consists of spoken numbers from 0 to 99 spoken by 32 different speakers in four
different languages, namely English, Mandarin, German and French, resulting in
12,800 audio samples. We determine auditory features and use UMAP (Uniform
Manifold Approximation and Projection for Dimension Reduction) as a
dimensionality reduction method to show the diversity and richness of the
dataset. To highlight the use case of the dataset, we introduce two benchmark
tasks: given an audio sample, classify (i) the used language and/or (ii) the
spoken number. We optimized state-of-the-art deep neural networks and performed
an evolutionary neural architecture search to find tiny architectures optimized
for the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent
the first benchmark data achieved for SpokeN-100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper by the tinyML Research Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MINT: Boosting Audio-Language Model via Multi-Target <span class="highlight-title">Pre-Train</span>ing and
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07485v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07485v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhao, Yifei Xin, Zhesong Yu, Bilei Zhu, Lu Lu, Zejun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of audio-language pre-training (ALP), the challenge of achieving
cross-modal alignment is significant. Moreover, the integration of audio inputs
with diverse distributions and task variations poses challenges in developing
generic audio-language models. In this study, we introduce MINT, a novel ALP
framework boosting audio-language models through multi-target pre-training and
instruction tuning. MINT leverages the strength of frozen pre-trained audio
encoders and large language models (LLMs) to improve audio-language
pre-training, enabling effective transferablility to both audio-text
understanding and generation tasks. To address the modality gap, we propose
Bridge-Net, a lightweight trainable module that enhances cross-modality
alignment and the model's ability to follow instructions for a variety of
audio-text tasks. Bridge-Net is pivotal within MINT, initially enhancing
audio-language representation learning through a multi-target pre-training
approach. Subsequently, Bridge-Net further boosts audio-to-language generative
learning by integrating a frozen language model with instruction tuning. This
integration empowers MINT to extract features in a flexible and effective
manner, specifically tailored to the provided instructions for diverse tasks.
Experimental results demonstrate that MINT attains superior performance across
various audio-language understanding and generation tasks, highlighting its
robust generalization capabilities even in zero-shot scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GESI: Gammachirp Envelope Similarity Index for Predicting
  Intelligibility of Simulated Hearing Loss Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15399v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15399v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayako Yamamoto, Toshio Irino, Fuki Miyazaki, Honoka Tamaru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an objective intelligibility measure (OIM), called the Gammachirp
Envelope Similarity Index (GESI), which can predict the speech intelligibility
(SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners.
GESI is an intrusive method that computes the SI metric using the gammachirp
filterbank (GCFB), the modulation filterbank, and the extended cosine
similarity measure. The unique features of GESI are that i) it reflects the
hearing impaired (HI) listener's HL that appears in the audiogram and is caused
by active and passive cochlear dysfunction, ii) it provides a single goodness
metric, as in the widely used STOI and ESTOI, that can be used immediately to
evaluate SE algorithms, and iii) it provides a simple control parameter to
accept the level asymmetry of the reference and test sounds and to deal with
individual listening conditions and environments. We evaluated GESI and the
conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using
four SI experiments on words of male and female speech sounds in both
laboratory and remote environments. GESI was shown to outperform the other OIMs
in the evaluations. GESI could be used to improve SE algorithms in assistive
listening devices for individual HI listeners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to JASA on March 14, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Turn-taking Prediction Using Voice Activity Projection <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of voice activity projection (VAP), a
predictive turn-taking model for spoken dialogue, on multilingual data,
encompassing English, Mandarin, and Japanese. The VAP model continuously
predicts the upcoming voice activities of participants in dyadic dialogue,
leveraging a cross-attention Transformer to capture the dynamic interplay
between participants. The results show that a monolingual VAP model trained on
one language does not make good predictions when applied to other languages.
However, a multilingual model, trained on all three languages, demonstrates
predictive performance on par with monolingual models across all languages.
Further analyses show that the multilingual model has learned to discern the
language of the input signal. We also analyze the sensitivity to pitch, a
prosodic cue that is thought to be important for turn-taking. Finally, we
compare two different audio encoders, contrastive predictive coding (CPC)
pre-trained on English, with a recent model based on multilingual wav2vec 2.0
(MMS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Brain Exhibits Distinct Patterns When Listening to Fake Versus
  Real Audio: Preliminary Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Salehi, Kalin Stefanov, Ehsan Shareghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we study the variations in human brain activity when listening
to real and fake audio. Our preliminary results suggest that the
representations learned by a state-of-the-art deepfake audio detection
algorithm, do not exhibit clear distinct patterns between real and fake audio.
In contrast, human brain activity, as measured by EEG, displays distinct
patterns when individuals are exposed to fake versus real audio. This
preliminary evidence enables future research directions in areas such as
deepfake audio detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-13T00:00:00Z">2024-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Acoustic Word Embeddings through Correspondence Training of
  <span class="highlight-title">Self-supervised</span> Speech Representations <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Meghanani, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic word embeddings (AWEs) are vector representations of spoken words.
An effective method for obtaining AWEs is the Correspondence Auto-Encoder
(CAE). In the past, the CAE method has been associated with traditional MFCC
features. Representations obtained from self-supervised learning (SSL)-based
speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many
downstream tasks. However, they have not been well studied in the context of
learning AWEs. This work explores the effectiveness of CAE with SSL-based
speech representations to obtain improved AWEs. Additionally, the capabilities
of SSL-based speech models are explored in cross-lingual scenarios for
obtaining AWEs. Experiments are conducted on five languages: Polish,
Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the
best results for word discrimination in all languages, despite Hu-BERT being
pre-trained on English only. Also, the HuBERT-based CAE model works well in
cross-lingual settings. It outperforms MFCC-based CAE models trained on the
target languages when trained on one source language and tested on target
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024 Main Conference, Long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient End-to-End Approach to Noise Invariant Speech Features via
  Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heitor R. Guimarães, Arthur Pimentel, Anderson R. Avila, Mehdi Rezagholizadeh, Boxing Chen, Tiago H. Falk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech representation learning enables the extraction of
meaningful features from raw waveforms. These features can then be efficiently
used across multiple downstream tasks. However, two significant issues arise
when considering the deployment of such methods ``in-the-wild": (i) Their large
size, which can be prohibitive for edge applications; and (ii) their robustness
to detrimental factors, such as noise and/or reverberation, that can heavily
degrade the performance of such systems. In this work, we propose
RobustDistiller, a novel knowledge distillation mechanism that tackles both
problems jointly. Simultaneously to the distillation recipe, we apply a
multi-task learning objective to encourage the network to learn noise-invariant
representations by denoising the input. The proposed mechanism is evaluated on
twelve different downstream tasks. It outperforms several benchmarks regardless
of noise type, or noise and reverberation levels. Experimental results show
that the new Student model with 23M parameters can achieve results comparable
to the Teacher model with 95M parameters. Lastly, we show that the proposed
recipe can be applied to other distillation methodologies, such as the recent
DPWavLM. For reproducibility, code and model checkpoints will be made available
at \mbox{\url{https://github.com/Hguimaraes/robustdistiller}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review on IEEE Transactions on Audio, Speech, and Language
  Processing (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier
  Models <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauri Juvela, Eero-Pekka Damskägg, Aleksi Peussa, Jaakko Mäkinen, Thomas Sherson, Stylianos I. Mimilakis, Athanasios Gotsopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a data-driven approach to creating real-time neural
network models of guitar amplifiers, recreating the amplifiers' sonic response
to arbitrary inputs at the full range of controls present on the physical
device. While the focus on the paper is on the data collection pipeline, we
demonstrate the effectiveness of this conditioned black-box approach by
training an LSTM model to the task, and comparing its performance to an offline
white-box SPICE circuit simulation. Our listening test results demonstrate that
the neural amplifier modeling approach can match the subjective performance of
a high-quality SPICE model, all while using an automated, non-intrusive data
collection process, and an end-to-end trainable, real-time feasible neural
network model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICASSP 2023 - 2023 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Weak to Strong Sound Event Labels using Adaptive Change-Point
  Detection and Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Martinsson, Olof Mogren, Maria Sandsten, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose an audio recording segmentation method based on an
adaptive change point detection (A-CPD) for machine guided weak label
annotation of audio recording segments. The goal is to maximize the amount of
information gained about the temporal activation's of the target sounds. For
each unlabeled audio recording, we use a prediction model to derive a
probability curve used to guide annotation. The prediction model is initially
pre-trained on available annotated sound event data with classes that are
disjoint from the classes in the unlabeled dataset. The prediction model then
gradually adapts to the annotations provided by the annotator in an active
learning loop. The queries used to guide the weak label annotator towards
strong labels are derived using change point detection on these probabilities.
We show that it is possible to derive strong labels of high quality even with a
limited annotation budget, and show favorable results for A-CPD when compared
to two baseline query strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of
  Speech Sound Disorders in Korean children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taekyung Ahn, Yeonjung Hong, Younggon Im, Do Hyung Kim, Dayoung Kang, Joo Won Jeong, Jae Won Kim, Min Jung Kim, Ah-ra Cho, Dae-Hyun Jang, Hosung Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a model of automatic speech recognition (ASR) designed to
diagnose pronunciation issues in children with speech sound disorders (SSDs) to
replace manual transcriptions in clinical procedures. Since ASR models trained
for general purposes primarily predict input speech into real words, employing
a well-known high-performance ASR model for evaluating pronunciation in
children with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to
recognize speech as pronounced rather than as existing words. The model was
fine-tuned with a speech dataset from 137 children with inadequate speech
production pronouncing 73 Korean words selected for actual clinical diagnosis.
The model's predictions of the pronunciations of the words matched the human
annotations with about 90% accuracy. While the model still requires improvement
in recognizing unclear pronunciation, this study demonstrates that ASR models
can streamline complex pronunciation error diagnostic procedures in clinical
fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EM-<span class="highlight-title">TTS</span>: Efficiently Trained Low-Resource Mongolian Lightweight
  <span class="highlight-title">Text-to-Speech</span> <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Liang, Haoxiang Shi, Jiawei Wang, Keda Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved
high-quality speech synthesis results. Recurrent neural networks have become a
standard modeling technique for sequential data in TTS systems and are widely
used. However, training a TTS model which includes RNN components requires
powerful GPU performance and takes a long time. In contrast, CNN-based sequence
synthesis techniques can significantly reduce the parameters and training time
of a TTS model while guaranteeing a certain performance due to their high
parallelism, which alleviate these economic costs of training. In this paper,
we propose a lightweight TTS system based on deep convolutional neural
networks, which is a two-stage training end-to-end TTS model and does not
employ any recurrent units. Our model consists of two stages: Text2Spectrum and
SSRN. The former is used to encode phonemes into a coarse mel spectrogram and
the latter is used to synthesize the complete spectrum from the coarse mel
spectrogram. Meanwhile, we improve the robustness of our model by a series of
data augmentations, such as noise suppression, time warping, frequency masking
and time masking, for solving the low resource mongolian problem. Experiments
show that our model can reduce the training time and parameters while ensuring
the quality and naturalness of the synthesized speech compared to using
mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for
validation, which significantly reduces training time while maintaining a
certain accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 27th IEEE International Conference on Computer
  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:
  substantial text overlap with arXiv:2211.01948</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Turn-taking Prediction Using Voice Activity Projection <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of voice activity projection (VAP), a
predictive turn-taking model for spoken dialogue, on multilingual data,
encompassing English, Mandarin, and Japanese. The VAP model continuously
predicts the upcoming voice activities of participants in dyadic dialogue,
leveraging a cross-attention Transformer to capture the dynamic interplay
between participants. The results show that a monolingual VAP model trained on
one language does not make good predictions when applied to other languages.
However, a multilingual model, trained on all three languages, demonstrates
predictive performance on par with monolingual models across all languages.
Further analyses show that the multilingual model has learned to discern the
language of the input signal. We also analyze the sensitivity to pitch, a
prosodic cue that is thought to be important for turn-taking. Finally, we
compare two different audio encoders, contrastive predictive coding (CPC)
pre-trained on English, with a recent model based on multilingual wav2vec 2.0
(MMS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-verbal information in spontaneous speech -- towards a new framework
  of analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tirza Biron, Moshe Barboy, Eran Ben-Artzy, Alona Golubchik, Yanir Marmor, Smadar Szekely, Yaron Winter, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-verbal signals in speech are encoded by prosody and carry information
that ranges from conversation action to attitude and emotion. Despite its
importance, the principles that govern prosodic structure are not yet
adequately understood. This paper offers an analytical schema and a
technological proof-of-concept for the categorization of prosodic signals and
their association with meaning. The schema interprets surface-representations
of multi-layered prosodic events. As a first step towards implementation, we
present a classification process that disentangles prosodic phenomena of three
orders. It relies on fine-tuning a pre-trained speech recognition model,
enabling the simultaneous multi-class/multi-label detection. It generalizes
over a large variety of spontaneous data, performing on a par with, or superior
to, human annotation. In addition to a standardized formalization of prosody,
disentangling prosodic patterns can direct a theory of communication and speech
organization. A welcome by-product is an interpretation of prosody that will
enhance speech- and language-related technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality
  Assessment Model <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryandhimas E. Zezario, Bo-Ren Brian Bai, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a multi-task pseudo-label learning (MPL)-based
non-intrusive speech quality assessment model called MTQ-Net. MPL consists of
two stages: obtaining pseudo-label scores from a pretrained model and
performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS),
Noise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The
pretrained MOSA-Net model is utilized to estimate three pseudo labels:
perceptual evaluation of speech quality (PESQ), short-time objective
intelligibility (STOI), and speech distortion index (SDI). Multi-task learning
is then employed to train MTQ-Net by combining a supervised loss (derived from
the difference between the estimated score and the ground-truth label) and a
semi-supervised loss (derived from the difference between the estimated score
and the pseudo label), where the Huber loss is employed as the loss function.
Experimental results first demonstrate the advantages of MPL compared to
training a model from scratch and using a direct knowledge transfer mechanism.
Second, the benefit of the Huber loss for improving the predictive ability of
MTQ-Net is verified. Finally, the MTQ-Net with the MPL approach exhibits higher
overall predictive power compared to other SSL-based speech assessment models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation
  Using Simulated Data and a Teacher Model <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jozef Coldenhoff, Andrew Harper, Paul Kendrick, Tijana Stojkovic, Milos Cernak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous methods for predicting room acoustic parameters and speech quality
metrics have focused on the single-channel case, where room acoustics and Mean
Opinion Score (MOS) are predicted for a single recording device. However,
quality-based device selection for rooms with multiple recording devices may
benefit from a multi-channel approach where the descriptive metrics are
predicted for multiple devices in parallel. Following our hypothesis that a
model may benefit from multi-channel training, we develop a multi-channel model
for joint MOS and room acoustics prediction (MOSRA) for five channels in
parallel. The lack of multi-channel audio data with ground truth labels
necessitated the creation of simulated data using an acoustic simulator with
room acoustic labels extracted from the generated impulse responses and labels
for MOS generated in a student-teacher setup using a wav2vec2-based MOS
prediction model. Our experiments show that the multi-channel model improves
the prediction of the direct-to-reverberation ratio, clarity, and speech
transmission index over the single-channel model with roughly 5$\times$ less
computation while suffering minimal losses in the performance of the other
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Acoustic Word Embeddings through Correspondence Training of
  <span class="highlight-title">Self-supervised</span> Speech Representations <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Meghanani, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic word embeddings (AWEs) are vector representations of spoken words.
An effective method for obtaining AWEs is the Correspondence Auto-Encoder
(CAE). In the past, the CAE method has been associated with traditional MFCC
features. Representations obtained from self-supervised learning (SSL)-based
speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many
downstream tasks. However, they have not been well studied in the context of
learning AWEs. This work explores the effectiveness of CAE with SSL-based
speech representations to obtain improved AWEs. Additionally, the capabilities
of SSL-based speech models are explored in cross-lingual scenarios for
obtaining AWEs. Experiments are conducted on five languages: Polish,
Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the
best results for word discrimination in all languages, despite Hu-BERT being
pre-trained on English only. Also, the HuBERT-based CAE model works well in
cross-lingual settings. It outperforms MFCC-based CAE models trained on the
target languages when trained on one source language and tested on target
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024 Main Conference, Long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient End-to-End Approach to Noise Invariant Speech Features via
  Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heitor R. Guimarães, Arthur Pimentel, Anderson R. Avila, Mehdi Rezagholizadeh, Boxing Chen, Tiago H. Falk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech representation learning enables the extraction of
meaningful features from raw waveforms. These features can then be efficiently
used across multiple downstream tasks. However, two significant issues arise
when considering the deployment of such methods ``in-the-wild": (i) Their large
size, which can be prohibitive for edge applications; and (ii) their robustness
to detrimental factors, such as noise and/or reverberation, that can heavily
degrade the performance of such systems. In this work, we propose
RobustDistiller, a novel knowledge distillation mechanism that tackles both
problems jointly. Simultaneously to the distillation recipe, we apply a
multi-task learning objective to encourage the network to learn noise-invariant
representations by denoising the input. The proposed mechanism is evaluated on
twelve different downstream tasks. It outperforms several benchmarks regardless
of noise type, or noise and reverberation levels. Experimental results show
that the new Student model with 23M parameters can achieve results comparable
to the Teacher model with 95M parameters. Lastly, we show that the proposed
recipe can be applied to other distillation methodologies, such as the recent
DPWavLM. For reproducibility, code and model checkpoints will be made available
at \mbox{\url{https://github.com/Hguimaraes/robustdistiller}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review on IEEE Transactions on Audio, Speech, and Language
  Processing (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier
  Models <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauri Juvela, Eero-Pekka Damskägg, Aleksi Peussa, Jaakko Mäkinen, Thomas Sherson, Stylianos I. Mimilakis, Athanasios Gotsopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a data-driven approach to creating real-time neural
network models of guitar amplifiers, recreating the amplifiers' sonic response
to arbitrary inputs at the full range of controls present on the physical
device. While the focus on the paper is on the data collection pipeline, we
demonstrate the effectiveness of this conditioned black-box approach by
training an LSTM model to the task, and comparing its performance to an offline
white-box SPICE circuit simulation. Our listening test results demonstrate that
the neural amplifier modeling approach can match the subjective performance of
a high-quality SPICE model, all while using an automated, non-intrusive data
collection process, and an end-to-end trainable, real-time feasible neural
network model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICASSP 2023 - 2023 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech
  Recognition Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Du, Jinpeng Li, Guoguo Chen, Wei-Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the wake of the surging tide of deep learning over the past decade,
Automatic Speech Recognition (ASR) has garnered substantial attention, leading
to the emergence of numerous publicly accessible ASR systems that are actively
being integrated into our daily lives. Nonetheless, the impartial and
replicable evaluation of these ASR systems encounters challenges due to various
crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a
general-purpose, open-source platform designed for ASR evaluation. With this
platform: (i) We report a comprehensive benchmark, unveiling the current
state-of-the-art panorama for ASR systems, covering both open-source models and
industrial commercial services. (ii) We quantize how distinct nuances in the
scoring pipeline influence the final benchmark outcomes. These include nuances
related to capitalization, punctuation, interjection, contraction, synonym
usage, compound words, etc. These issues have gained prominence in the context
of the transition towards an End-to-End future. (iii) We propose a practical
modification to the conventional Token-Error-Rate (TER) evaluation metric, with
inspirations from Kolmogorov complexity and Normalized Information Distance
(NID). This adaptation, called modified-TER (mTER), achieves proper
normalization and symmetrical treatment of reference and hypothesis. By
leveraging this platform as a large-scale testing ground, this study
demonstrates the robustness and backward compatibility of mTER when compared to
TER. The SpeechColab Leaderboard is accessible at
https://github.com/SpeechColab/Leaderboard
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of
  Speech Sound Disorders in Korean children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taekyung Ahn, Yeonjung Hong, Younggon Im, Do Hyung Kim, Dayoung Kang, Joo Won Jeong, Jae Won Kim, Min Jung Kim, Ah-ra Cho, Dae-Hyun Jang, Hosung Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a model of automatic speech recognition (ASR) designed to
diagnose pronunciation issues in children with speech sound disorders (SSDs) to
replace manual transcriptions in clinical procedures. Since ASR models trained
for general purposes primarily predict input speech into real words, employing
a well-known high-performance ASR model for evaluating pronunciation in
children with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to
recognize speech as pronounced rather than as existing words. The model was
fine-tuned with a speech dataset from 137 children with inadequate speech
production pronouncing 73 Korean words selected for actual clinical diagnosis.
The model's predictions of the pronunciations of the words matched the human
annotations with about 90% accuracy. While the model still requires improvement
in recognizing unclear pronunciation, this study demonstrates that ASR models
can streamline complex pronunciation error diagnostic procedures in clinical
fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EM-<span class="highlight-title">TTS</span>: Efficiently Trained Low-Resource Mongolian Lightweight
  <span class="highlight-title">Text-to-Speech</span> <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Liang, Haoxiang Shi, Jiawei Wang, Keda Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved
high-quality speech synthesis results. Recurrent neural networks have become a
standard modeling technique for sequential data in TTS systems and are widely
used. However, training a TTS model which includes RNN components requires
powerful GPU performance and takes a long time. In contrast, CNN-based sequence
synthesis techniques can significantly reduce the parameters and training time
of a TTS model while guaranteeing a certain performance due to their high
parallelism, which alleviate these economic costs of training. In this paper,
we propose a lightweight TTS system based on deep convolutional neural
networks, which is a two-stage training end-to-end TTS model and does not
employ any recurrent units. Our model consists of two stages: Text2Spectrum and
SSRN. The former is used to encode phonemes into a coarse mel spectrogram and
the latter is used to synthesize the complete spectrum from the coarse mel
spectrogram. Meanwhile, we improve the robustness of our model by a series of
data augmentations, such as noise suppression, time warping, frequency masking
and time masking, for solving the low resource mongolian problem. Experiments
show that our model can reduce the training time and parameters while ensuring
the quality and naturalness of the synthesized speech compared to using
mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for
validation, which significantly reduces training time while maintaining a
certain accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 27th IEEE International Conference on Computer
  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:
  substantial text overlap with arXiv:2211.01948</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Weak to Strong Sound Event Labels using Adaptive Change-Point
  Detection and Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Martinsson, Olof Mogren, Maria Sandsten, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose an audio recording segmentation method based on an
adaptive change point detection (A-CPD) for machine guided weak label
annotation of audio recording segments. The goal is to maximize the amount of
information gained about the temporal activation's of the target sounds. For
each unlabeled audio recording, we use a prediction model to derive a
probability curve used to guide annotation. The prediction model is initially
pre-trained on available annotated sound event data with classes that are
disjoint from the classes in the unlabeled dataset. The prediction model then
gradually adapts to the annotations provided by the annotator in an active
learning loop. The queries used to guide the weak label annotator towards
strong labels are derived using change point detection on these probabilities.
We show that it is possible to derive strong labels of high quality even with a
limited annotation budget, and show favorable results for A-CPD when compared
to two baseline query strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Turn-taking Prediction Using Voice Activity Projection <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of voice activity projection (VAP), a
predictive turn-taking model for spoken dialogue, on multilingual data,
encompassing English, Mandarin, and Japanese. The VAP model continuously
predicts the upcoming voice activities of participants in dyadic dialogue,
leveraging a cross-attention Transformer to capture the dynamic interplay
between participants. The results show that a monolingual VAP model trained on
one language does not make good predictions when applied to other languages.
However, a multilingual model, trained on all three languages, demonstrates
predictive performance on par with monolingual models across all languages.
Further analyses show that the multilingual model has learned to discern the
language of the input signal. We also analyze the sensitivity to pitch, a
prosodic cue that is thought to be important for turn-taking. Finally, we
compare two different audio encoders, contrastive predictive coding (CPC)
pre-trained on English, with a recent model based on multilingual wav2vec 2.0
(MMS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-verbal information in spontaneous speech -- towards a new framework
  of analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tirza Biron, Moshe Barboy, Eran Ben-Artzy, Alona Golubchik, Yanir Marmor, Smadar Szekely, Yaron Winter, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-verbal signals in speech are encoded by prosody and carry information
that ranges from conversation action to attitude and emotion. Despite its
importance, the principles that govern prosodic structure are not yet
adequately understood. This paper offers an analytical schema and a
technological proof-of-concept for the categorization of prosodic signals and
their association with meaning. The schema interprets surface-representations
of multi-layered prosodic events. As a first step towards implementation, we
present a classification process that disentangles prosodic phenomena of three
orders. It relies on fine-tuning a pre-trained speech recognition model,
enabling the simultaneous multi-class/multi-label detection. It generalizes
over a large variety of spontaneous data, performing on a par with, or superior
to, human annotation. In addition to a standardized formalization of prosody,
disentangling prosodic patterns can direct a theory of communication and speech
organization. A welcome by-product is an interpretation of prosody that will
enhance speech- and language-related technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality
  Assessment Model <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryandhimas E. Zezario, Bo-Ren Brian Bai, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a multi-task pseudo-label learning (MPL)-based
non-intrusive speech quality assessment model called MTQ-Net. MPL consists of
two stages: obtaining pseudo-label scores from a pretrained model and
performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS),
Noise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The
pretrained MOSA-Net model is utilized to estimate three pseudo labels:
perceptual evaluation of speech quality (PESQ), short-time objective
intelligibility (STOI), and speech distortion index (SDI). Multi-task learning
is then employed to train MTQ-Net by combining a supervised loss (derived from
the difference between the estimated score and the ground-truth label) and a
semi-supervised loss (derived from the difference between the estimated score
and the pseudo label), where the Huber loss is employed as the loss function.
Experimental results first demonstrate the advantages of MPL compared to
training a model from scratch and using a direct knowledge transfer mechanism.
Second, the benefit of the Huber loss for improving the predictive ability of
MTQ-Net is verified. Finally, the MTQ-Net with the MPL approach exhibits higher
overall predictive power compared to other SSL-based speech assessment models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation
  Using Simulated Data and a Teacher Model <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jozef Coldenhoff, Andrew Harper, Paul Kendrick, Tijana Stojkovic, Milos Cernak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous methods for predicting room acoustic parameters and speech quality
metrics have focused on the single-channel case, where room acoustics and Mean
Opinion Score (MOS) are predicted for a single recording device. However,
quality-based device selection for rooms with multiple recording devices may
benefit from a multi-channel approach where the descriptive metrics are
predicted for multiple devices in parallel. Following our hypothesis that a
model may benefit from multi-channel training, we develop a multi-channel model
for joint MOS and room acoustics prediction (MOSRA) for five channels in
parallel. The lack of multi-channel audio data with ground truth labels
necessitated the creation of simulated data using an acoustic simulator with
room acoustic labels extracted from the generated impulse responses and labels
for MOS generated in a student-teacher setup using a wav2vec2-based MOS
prediction model. Our experiments show that the multi-channel model improves
the prediction of the direct-to-reverberation ratio, clarity, and speech
transmission index over the single-channel model with roughly 5$\times$ less
computation while suffering minimal losses in the performance of the other
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-12T00:00:00Z">2024-03-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting keyword spotting through on-device learnable user speech
  characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Cioflan, Lukas Cavigelli, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyword spotting systems for always-on TinyML-constrained applications
require on-site tuning to boost the accuracy of offline trained classifiers
when deployed in unseen inference conditions. Adapting to the speech
peculiarities of target users requires many in-domain samples, often
unavailable in real-world scenarios. Furthermore, current on-device learning
techniques rely on computationally intensive and memory-hungry backbone update
schemes, unfit for always-on, battery-powered devices. In this work, we propose
a novel on-device learning architecture, composed of a pretrained backbone and
a user-aware embedding learning the user's speech characteristics. The
so-generated features are fused and used to classify the input utterance. For
domain shifts generated by unseen speakers, we measure error rate reductions of
up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google
Speech Commands dataset, through the inexpensive update of the user
projections. We moreover demonstrate the few-shot learning capabilities of our
proposed architecture in sample- and class-scarce learning conditions. With
23.7 kparameters and 1 MFLOP per epoch required for on-device training, our
system is feasible for TinyML applications aimed at battery-powered
microcontrollers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 tables, 2 figures. Accepted as a full paper by the tinyML
  Research Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multichannel Long-Term Streaming Neural Speech Enhancement for Static
  and Moving Speakers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsheng Quan, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we extend our previously proposed offline SpatialNet for
long-term streaming multichannel speech enhancement in both static and moving
speaker scenarios. SpatialNet exploits spatial information, such as the
spatial/steering direction of speech, for discriminating between target speech
and interferences, and achieved outstanding performance. The core of SpatialNet
is a narrow-band self-attention module used for learning the temporal dynamic
of spatial vectors. Towards long-term streaming speech enhancement, we propose
to replace the offline self-attention network with online networks that have
linear inference complexity w.r.t signal length and meanwhile maintain the
capability of learning long-term information. Three variants are developed
based on (i) masked self-attention, (ii) Retention, a self-attention variant
with linear inference complexity, and (iii) Mamba, a
structured-state-space-based RNN-like network. Moreover, we investigate the
length extrapolation ability of different networks, namely test on signals that
are much longer than training signals, and propose a short-signal training plus
long-signal fine-tuning strategy, which largely improves the length
extrapolation ability of the networks within limited training time. Overall,
the proposed online SpatialNet achieves outstanding speech enhancement
performance for long audio streams, and for both static and moving speakers.
The proposed method will be open-sourced in
https://github.com/Audio-WestlakeU/NBSS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gender-ambiguous voice <span class="highlight-title">generation</span> through feminine speaking style
  transfer in male voices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Koutsogiannaki, Shafel Mc Dowall, Ioannis Agiomyrgiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, and under the umbrella of Responsible AI, efforts have been made to
develop gender-ambiguous synthetic speech to represent with a single voice all
individuals in the gender spectrum. However, research efforts have completely
overlooked the speaking style despite differences found among binary and
non-binary populations. In this work, we synthesise gender-ambiguous speech by
combining the timbre of a male speaker with the manner of speech of a female
speaker using voice morphing and pitch shifting towards the male-female
boundary. Subjective evaluations indicate that the ambiguity of the morphed
samples that convey the female speech style is higher than those that undergo
pure pitch transformations suggesting that the speaking style can be a
contributing factor in creating gender-ambiguous speech. To our knowledge, this
is the first study that explicitly uses the transfer of the speaking style to
create gender-ambiguous voices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic
  Music <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshav Bhandari, Simon Colton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling musical structure is vital yet challenging for artificial
intelligence systems that generate symbolic music compositions. This literature
review dissects the evolution of techniques for incorporating coherent
structure, from symbolic approaches to foundational and transformative deep
learning methods that harness the power of computation and data across a wide
variety of training paradigms. In the later stages, we review an emerging
technique which we refer to as "sub-task decomposition" that involves
decomposing music generation into separate high-level structural planning and
content creation stages. Such systems incorporate some form of musical
knowledge or neuro-symbolic methods by extracting melodic skeletons or
structural templates to guide the generation. Progress is evident in capturing
motifs and repetitions across all three eras reviewed, yet modelling the
nuanced development of themes across extended compositions in the style of
human composers remains difficult. We outline several key future directions to
realize the synergistic benefits of combining approaches from all eras
examined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 13th International Conference on Artificial Intelligence
  in Music, Sound, Art and Design (EvoMUSART) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge
  Embedded Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Cioflan, Lukas Cavigelli, Manuele Rusci, Miguel de Prado, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyword spotting accuracy degrades when neural networks are exposed to noisy
environments. On-site adaptation to previously unseen noise is crucial to
recovering accuracy loss, and on-device learning is required to ensure that the
adaptation process happens entirely on the edge device. In this work, we
propose a fully on-device domain adaptation system achieving up to 14% accuracy
gains over already-robust keyword spotting models. We enable on-device learning
with less than 10 kB of memory, using only 100 labeled utterances to recover 5%
accuracy after adapting to the complex speech noise. We demonstrate that domain
adaptation can be achieved on ultra-low-power microcontrollers with as little
as 806 mJ in only 14 s on always-on, battery-operated devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 tables, 2 figures. Accepted at IEEE AICAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-sided Acoustic Metascreen for Broadband and Individual Reflection
  and Transmission Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Chen, Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic wave modulation plays a pivotal role in various applications,
including sound-field reconstruction, wireless communication, and particle
manipulation, among others. However, current acoustic metamaterial and
metasurface designs typically focus on controlling either reflection or
transmission waves, often overlooking the coupling between amplitude and phase
of acoustic waves. To fulfill this gap, we propose and experimentally validate
a design enabling complete control of reflected and transmitted acoustic waves
individually across a frequency range of 4 kHz to 8 kHz, allowing arbitrary
combinations of amplitude and phase for reflected and transmitted sound in a
broadband manner. Additionally, we demonstrate the significance of our approach
for sound manipulation by achieving acoustic diffusion, reflection, focusing,
and generating a two-sided 3D hologram at three distinct frequencies. These
findings open an alternative avenue for extensively engineering sound waves,
promising applications in acoustics and related fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMO-SUPERB: An In-depth Look at Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13018v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13018v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibin Wu, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, Jiawei Du, Jyh-Shing Roger Jang, Chi-Chun Lee, Hung-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) is a pivotal technology for human-computer
interaction systems. However, 80.77% of SER papers yield results that cannot be
reproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal
PERformance Benchmark, which aims to enhance open-source initiatives for SER.
EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art
speech self-supervised learning models (SSLMs) for exhaustive evaluation across
six open-source SER datasets. EMO-SUPERB streamlines result sharing via an
online leaderboard, fostering collaboration within a community-driven benchmark
and thereby enhancing the development of SER. On average, 2.58% of annotations
are annotated using natural language. SER relies on classification models and
is unable to process natural languages, leading to the discarding of these
valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural
language annotations, and subsequently re-label the data. By utilizing labels
generated by ChatGPT, we consistently achieve an average relative gain of 3.08%
across all settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>webpage: https://emosuperb.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Audio-textual <span class="highlight-title">Diffusion</span> Model For Converting Speech Signals Into
  Ultrasound Tongue Imaging Data <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Yang, Rongfeng Su, Xiaokang Liu, Nan Yan, Lan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator
movements, such as ultrasound tongue imaging (UTI) data. An issue of existing
AAI methods is only using the personalized acoustic information to derive the
general patterns of tongue motions, and thus the quality of generated UTI data
is limited. To address this issue, this paper proposes an audio-textual
diffusion model for the UTI data generation task. In this model, the inherent
acoustic characteristics of individuals related to the tongue motion details
are encoded by using wav2vec 2.0, while the ASR transcriptions related to the
universality of tongue motions are encoded by using BERT. UTI data are then
generated by using a diffusion module. Experimental results showed that the
proposed diffusion model could generate high-quality UTI data with clear tongue
contour that is crucial for the linguistic analysis and clinical assessment.
The project can be found on the
website\footnote{https://yangyudong2020.github.io/wav2uti/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP2024 Accept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Microphone Array Calibration using Hybrid TDOA Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjie Zhang, Jiang Wang, He Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asynchronous Microphone array calibration is a prerequisite for most audition
robot applications. In practice, the calibration requires estimating microphone
positions, time offsets, clock drift rates, and sound event locations
simultaneously. The existing method proposed Graph-based Simultaneous
Localisation and Mapping (Graph-SLAM) utilizing common TDOA, time difference of
arrival between two microphones (TDOA-M), and odometry measurement, however, it
heavily depends on the initial value. In this paper, we propose a novel TDOA,
time difference of arrival between adjacent sound events (TDOA-S), combine it
with TDOA-M, called hybrid TDOA, and add odometry measurement to construct
Graph-SLAM and use the Gauss-Newton (GN) method to solve. TDOA-S is simple and
efficient because it eliminates time offset without generating new variables.
Simulation and real-world experiment results consistently show that our method
is independent of microphone number, insensitive to initial values, and has
better calibration accuracy and stability under various TDOA noises. In
addition, the simulation result demonstrates that our method has a lower
Cram\'er-Rao lower bound (CRLB) for microphone parameters, which explains the
advantages of my method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StoRM: A <span class="highlight-title">Diffusion</span>-based Stochastic Re<span class="highlight-title">generation</span> Model for Speech
  Enhancement and Dereverberation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Marie Lemercier, Julius Richter, Simon Welker, Timo Gerkmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown a great ability at bridging the performance gap
between predictive and generative approaches for speech enhancement. We have
shown that they may even outperform their predictive counterparts for
non-additive corruption types or when they are evaluated on mismatched
conditions. However, diffusion models suffer from a high computational burden,
mainly as they require to run a neural network for each reverse diffusion step,
whereas predictive approaches only require one pass. As diffusion models are
generative approaches they may also produce vocalizing and breathing artifacts
in adverse conditions. In comparison, in such difficult scenarios, predictive
models typically do not produce such artifacts but tend to distort the target
speech instead, thereby degrading the speech quality. In this work, we present
a stochastic regeneration approach where an estimate given by a predictive
model is provided as a guide for further diffusion. We show that the proposed
approach uses the predictive model to remove the vocalizing and breathing
artifacts while producing very high quality samples thanks to the diffusion
model, even in adverse conditions. We further show that this approach enables
to use lighter sampling schemes with fewer diffusion steps without sacrificing
quality, thus lifting the computational burden by an order of magnitude. Source
code and audio examples are available online (https://uhh.de/inf-sp-storm).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE/ACM Transactions on Audio, Speech and Language
  Processing, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech
  Models via Language-Specific Experts <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, Vassilina Nikoulina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whisper is a multitask and multilingual speech model covering 99 languages.
It yields commendable automatic speech recognition (ASR) results in a subset of
its covered languages, but the model still underperforms on a non-negligible
number of under-represented languages, a problem exacerbated in smaller model
versions. In this work, we propose DistilWhisper, an approach able to bridge
the performance gap in ASR for these languages while retaining the advantages
of multitask and multilingual capabilities. Our approach involves two key
strategies: lightweight modular ASR fine-tuning of whisper-small using
language-specific experts, and knowledge distillation from whisper-large-v2.
This dual approach allows us to effectively boost ASR performance while keeping
the robustness inherited from the multitask and multilingual pre-training.
Results demonstrate that our approach is more effective than standard
fine-tuning or LoRA adapters, boosting performance in the targeted languages
for both in- and out-of-domain test sets, while introducing only a negligible
parameter overhead at inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Steganalysis of Diverse Data Types: A review of
  methods, taxonomy, challenges and future directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04522v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04522v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Kheddar, Mustapha Hemis, Yassine Himeur, David Megías, Abbes Amira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steganography and steganalysis are two interrelated aspects of the field of
information security. Steganography seeks to conceal communications, whereas
steganalysis is aimed to either find them or even, if possible, recover the
data they contain. Steganography and steganalysis have attracted a great deal
of interest, particularly from law enforcement. Steganography is often used by
cybercriminals and even terrorists to avoid being captured while in possession
of incriminating evidence, even encrypted, since cryptography is prohibited or
restricted in many countries. Therefore, knowledge of cutting-edge techniques
to uncover concealed information is crucial in exposing illegal acts. Over the
last few years, a number of strong and reliable steganography and steganalysis
techniques have been introduced in the literature. This review paper provides a
comprehensive overview of deep learning-based steganalysis techniques used to
detect hidden information within digital media. The paper covers all types of
cover in steganalysis, including image, audio, and video, and discusses the
most commonly used deep learning techniques. In addition, the paper explores
the use of more advanced deep learning techniques, such as deep transfer
learning (DTL) and deep reinforcement learning (DRL), to enhance the
performance of steganalysis systems. The paper provides a systematic review of
recent research in the field, including data sets and evaluation metrics used
in recent studies. It also presents a detailed analysis of DTL-based
steganalysis approaches and their performance on different data sets. The
review concludes with a discussion on the current state of deep learning-based
steganalysis, challenges, and future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Cross Attention for Audio-Visual Person Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Gnana Praveen, Jahangir Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although person or identity verification has been predominantly explored
using individual modalities such as face and voice, audio-visual fusion has
recently shown immense potential to outperform unimodal approaches. Audio and
visual modalities are often expected to pose strong complementary
relationships, which plays a crucial role in effective audio-visual fusion.
However, they may not always strongly complement each other, they may also
exhibit weak complementary relationships, resulting in poor audio-visual
feature representations. In this paper, we propose a Dynamic Cross-Attention
(DCA) model that can dynamically select the cross-attended or unattended
features on the fly based on the strong or weak complementary relationships,
respectively, across audio and visual modalities. In particular, a conditional
gating layer is designed to evaluate the contribution of the cross-attention
mechanism and choose cross-attended features only when they exhibit strong
complementary relationships, otherwise unattended features. Extensive
experiments are conducted on the Voxceleb1 dataset to demonstrate the
robustness of the proposed model. Results indicate that the proposed model
consistently improves the performance on multiple variants of cross-attention
while outperforming the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to FG2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Person Verification based on Recursive Fusion of Joint
  Cross-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Gnana Praveen, Jahangir Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person or identity verification has been recently gaining a lot of attention
using audio-visual fusion as faces and voices share close associations with
each other. Conventional approaches based on audio-visual fusion rely on
score-level or early feature-level fusion techniques. Though existing
approaches showed improvement over unimodal systems, the potential of
audio-visual fusion for person verification is not fully exploited. In this
paper, we have investigated the prospect of effectively capturing both the
intra- and inter-modal relationships across audio and visual modalities, which
can play a crucial role in significantly improving the fusion performance over
unimodal systems. In particular, we introduce a recursive fusion of a joint
cross-attentional model, where a joint audio-visual feature representation is
employed in the cross-attention framework in a recursive fashion to
progressively refine the feature representations that can efficiently capture
the intra-and inter-modal relationships. To further enhance the audio-visual
feature representations, we have also explored BLSTMs to improve the temporal
modeling of audio-visual feature representations. Extensive experiments are
conducted on the Voxceleb1 dataset to evaluate the proposed model. Results
indicate that the proposed model shows promising improvement in fusion
performance by adeptly capturing the intra-and inter-modal relationships across
audio and visual modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to FG2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Acoustic Scene Mapping Based on Acoustic Features and
  Dimensionality Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idan Cohen, Ofir Lindenbaum, Sharon Gannot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical methods for acoustic scene mapping require the estimation of time
difference of arrival (TDOA) between microphones. Unfortunately, TDOA
estimation is very sensitive to reverberation and additive noise. We introduce
an unsupervised data-driven approach that exploits the natural structure of the
data. Our method builds upon local conformal autoencoders (LOCA) - an offline
deep learning scheme for learning standardized data coordinates from
measurements. Our experimental setup includes a microphone array that measures
the transmitted sound source at multiple locations across the acoustic
enclosure. We demonstrate that LOCA learns a representation that is isometric
to the spatial locations of the microphones. The performance of our method is
evaluated using a series of realistic simulations and compared with other
dimensionality-reduction schemes. We further assess the influence of
reverberation on the results of LOCA and show that it demonstrates considerable
robustness.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting keyword spotting through on-device learnable user speech
  characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Cioflan, Lukas Cavigelli, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyword spotting systems for always-on TinyML-constrained applications
require on-site tuning to boost the accuracy of offline trained classifiers
when deployed in unseen inference conditions. Adapting to the speech
peculiarities of target users requires many in-domain samples, often
unavailable in real-world scenarios. Furthermore, current on-device learning
techniques rely on computationally intensive and memory-hungry backbone update
schemes, unfit for always-on, battery-powered devices. In this work, we propose
a novel on-device learning architecture, composed of a pretrained backbone and
a user-aware embedding learning the user's speech characteristics. The
so-generated features are fused and used to classify the input utterance. For
domain shifts generated by unseen speakers, we measure error rate reductions of
up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google
Speech Commands dataset, through the inexpensive update of the user
projections. We moreover demonstrate the few-shot learning capabilities of our
proposed architecture in sample- and class-scarce learning conditions. With
23.7 kparameters and 1 MFLOP per epoch required for on-device training, our
system is feasible for TinyML applications aimed at battery-powered
microcontrollers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 tables, 2 figures. Accepted as a full paper by the tinyML
  Research Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech
  Recognition Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Pešán, Santosh Kesiraju, Lukáš Burget, Jan ''Honza'' Černocký
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Paralinguistic traits like cognitive load and emotion are increasingly
recognized as pivotal areas in speech recognition research, often examined
through specialized datasets like CLSE and IEMOCAP. However, the integrity of
these datasets is seldom scrutinized for text-dependency. This paper critically
evaluates the prevalent assumption that machine learning models trained on such
datasets genuinely learn to identify paralinguistic traits, rather than merely
capturing lexical features. By examining the lexical overlap in these datasets
and testing the performance of machine learning models, we expose significant
text-dependency in trait-labeling. Our results suggest that some machine
learning models, especially large pre-trained models like HuBERT, might
inadvertently focus on lexical characteristics rather than the intended
paralinguistic features. The study serves as a call to action for the research
community to reevaluate the reliability of existing datasets and methodologies,
ensuring that machine learning models genuinely learn what they are designed to
recognize.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multichannel Long-Term Streaming Neural Speech Enhancement for Static
  and Moving Speakers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsheng Quan, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we extend our previously proposed offline SpatialNet for
long-term streaming multichannel speech enhancement in both static and moving
speaker scenarios. SpatialNet exploits spatial information, such as the
spatial/steering direction of speech, for discriminating between target speech
and interferences, and achieved outstanding performance. The core of SpatialNet
is a narrow-band self-attention module used for learning the temporal dynamic
of spatial vectors. Towards long-term streaming speech enhancement, we propose
to replace the offline self-attention network with online networks that have
linear inference complexity w.r.t signal length and meanwhile maintain the
capability of learning long-term information. Three variants are developed
based on (i) masked self-attention, (ii) Retention, a self-attention variant
with linear inference complexity, and (iii) Mamba, a
structured-state-space-based RNN-like network. Moreover, we investigate the
length extrapolation ability of different networks, namely test on signals that
are much longer than training signals, and propose a short-signal training plus
long-signal fine-tuning strategy, which largely improves the length
extrapolation ability of the networks within limited training time. Overall,
the proposed online SpatialNet achieves outstanding speech enhancement
performance for long audio streams, and for both static and moving speakers.
The proposed method will be open-sourced in
https://github.com/Audio-WestlakeU/NBSS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gender-ambiguous voice <span class="highlight-title">generation</span> through feminine speaking style
  transfer in male voices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Koutsogiannaki, Shafel Mc Dowall, Ioannis Agiomyrgiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, and under the umbrella of Responsible AI, efforts have been made to
develop gender-ambiguous synthetic speech to represent with a single voice all
individuals in the gender spectrum. However, research efforts have completely
overlooked the speaking style despite differences found among binary and
non-binary populations. In this work, we synthesise gender-ambiguous speech by
combining the timbre of a male speaker with the manner of speech of a female
speaker using voice morphing and pitch shifting towards the male-female
boundary. Subjective evaluations indicate that the ambiguity of the morphed
samples that convey the female speech style is higher than those that undergo
pure pitch transformations suggesting that the speaking style can be a
contributing factor in creating gender-ambiguous speech. To our knowledge, this
is the first study that explicitly uses the transfer of the speaking style to
create gender-ambiguous voices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On HRTF Notch Frequency Prediction Using Anthropometric Features and
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lior Arbel, Ishwarya Ananthabhotla, Zamir Ben-Hur, David Lou Alon, Boaz Rafaely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High fidelity spatial audio often performs better when produced using a
personalized head-related transfer function (HRTF). However, the direct
acquisition of HRTFs is cumbersome and requires specialized equipment. Thus,
many personalization methods estimate HRTF features from easily obtained
anthropometric features of the pinna, head, and torso. The first HRTF notch
frequency (N1) is known to be a dominant feature in elevation localization, and
thus a useful feature for HRTF personalization. This paper describes the
prediction of N1 frequency from pinna anthropometry using a neural model.
Prediction is performed separately on three databases, both simulated and
measured, and then by domain mixing in-between the databases. The model
successfully predicts N1 frequency for individual databases and by domain
mixing between some databases. Prediction errors are better or comparable to
those previously reported, showing significant improvement when acquired over a
large database and with a larger output range.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic
  Music <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshav Bhandari, Simon Colton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling musical structure is vital yet challenging for artificial
intelligence systems that generate symbolic music compositions. This literature
review dissects the evolution of techniques for incorporating coherent
structure, from symbolic approaches to foundational and transformative deep
learning methods that harness the power of computation and data across a wide
variety of training paradigms. In the later stages, we review an emerging
technique which we refer to as "sub-task decomposition" that involves
decomposing music generation into separate high-level structural planning and
content creation stages. Such systems incorporate some form of musical
knowledge or neuro-symbolic methods by extracting melodic skeletons or
structural templates to guide the generation. Progress is evident in capturing
motifs and repetitions across all three eras reviewed, yet modelling the
nuanced development of themes across extended compositions in the style of
human composers remains difficult. We outline several key future directions to
realize the synergistic benefits of combining approaches from all eras
examined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 13th International Conference on Artificial Intelligence
  in Music, Sound, Art and Design (EvoMUSART) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge
  Embedded Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Cioflan, Lukas Cavigelli, Manuele Rusci, Miguel de Prado, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyword spotting accuracy degrades when neural networks are exposed to noisy
environments. On-site adaptation to previously unseen noise is crucial to
recovering accuracy loss, and on-device learning is required to ensure that the
adaptation process happens entirely on the edge device. In this work, we
propose a fully on-device domain adaptation system achieving up to 14% accuracy
gains over already-robust keyword spotting models. We enable on-device learning
with less than 10 kB of memory, using only 100 labeled utterances to recover 5%
accuracy after adapting to the complex speech noise. We demonstrate that domain
adaptation can be achieved on ultra-low-power microcontrollers with as little
as 806 mJ in only 14 s on always-on, battery-operated devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 tables, 2 figures. Accepted at IEEE AICAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-sided Acoustic Metascreen for Broadband and Individual Reflection
  and Transmission Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Chen, Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic wave modulation plays a pivotal role in various applications,
including sound-field reconstruction, wireless communication, and particle
manipulation, among others. However, current acoustic metamaterial and
metasurface designs typically focus on controlling either reflection or
transmission waves, often overlooking the coupling between amplitude and phase
of acoustic waves. To fulfill this gap, we propose and experimentally validate
a design enabling complete control of reflected and transmitted acoustic waves
individually across a frequency range of 4 kHz to 8 kHz, allowing arbitrary
combinations of amplitude and phase for reflected and transmitted sound in a
broadband manner. Additionally, we demonstrate the significance of our approach
for sound manipulation by achieving acoustic diffusion, reflection, focusing,
and generating a two-sided 3D hologram at three distinct frequencies. These
findings open an alternative avenue for extensively engineering sound waves,
promising applications in acoustics and related fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMO-SUPERB: An In-depth Look at Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13018v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13018v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibin Wu, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, Jiawei Du, Jyh-Shing Roger Jang, Chi-Chun Lee, Hung-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) is a pivotal technology for human-computer
interaction systems. However, 80.77% of SER papers yield results that cannot be
reproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal
PERformance Benchmark, which aims to enhance open-source initiatives for SER.
EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art
speech self-supervised learning models (SSLMs) for exhaustive evaluation across
six open-source SER datasets. EMO-SUPERB streamlines result sharing via an
online leaderboard, fostering collaboration within a community-driven benchmark
and thereby enhancing the development of SER. On average, 2.58% of annotations
are annotated using natural language. SER relies on classification models and
is unable to process natural languages, leading to the discarding of these
valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural
language annotations, and subsequently re-label the data. By utilizing labels
generated by ChatGPT, we consistently achieve an average relative gain of 3.08%
across all settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>webpage: https://emosuperb.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Audio-textual <span class="highlight-title">Diffusion</span> Model For Converting Speech Signals Into
  Ultrasound Tongue Imaging Data <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Yang, Rongfeng Su, Xiaokang Liu, Nan Yan, Lan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator
movements, such as ultrasound tongue imaging (UTI) data. An issue of existing
AAI methods is only using the personalized acoustic information to derive the
general patterns of tongue motions, and thus the quality of generated UTI data
is limited. To address this issue, this paper proposes an audio-textual
diffusion model for the UTI data generation task. In this model, the inherent
acoustic characteristics of individuals related to the tongue motion details
are encoded by using wav2vec 2.0, while the ASR transcriptions related to the
universality of tongue motions are encoded by using BERT. UTI data are then
generated by using a diffusion module. Experimental results showed that the
proposed diffusion model could generate high-quality UTI data with clear tongue
contour that is crucial for the linguistic analysis and clinical assessment.
The project can be found on the
website\footnote{https://yangyudong2020.github.io/wav2uti/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP2024 Accept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Microphone Array Calibration using Hybrid TDOA Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjie Zhang, Jiang Wang, He Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asynchronous Microphone array calibration is a prerequisite for most audition
robot applications. In practice, the calibration requires estimating microphone
positions, time offsets, clock drift rates, and sound event locations
simultaneously. The existing method proposed Graph-based Simultaneous
Localisation and Mapping (Graph-SLAM) utilizing common TDOA, time difference of
arrival between two microphones (TDOA-M), and odometry measurement, however, it
heavily depends on the initial value. In this paper, we propose a novel TDOA,
time difference of arrival between adjacent sound events (TDOA-S), combine it
with TDOA-M, called hybrid TDOA, and add odometry measurement to construct
Graph-SLAM and use the Gauss-Newton (GN) method to solve. TDOA-S is simple and
efficient because it eliminates time offset without generating new variables.
Simulation and real-world experiment results consistently show that our method
is independent of microphone number, insensitive to initial values, and has
better calibration accuracy and stability under various TDOA noises. In
addition, the simulation result demonstrates that our method has a lower
Cram\'er-Rao lower bound (CRLB) for microphone parameters, which explains the
advantages of my method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StoRM: A <span class="highlight-title">Diffusion</span>-based Stochastic Re<span class="highlight-title">generation</span> Model for Speech
  Enhancement and Dereverberation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Marie Lemercier, Julius Richter, Simon Welker, Timo Gerkmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown a great ability at bridging the performance gap
between predictive and generative approaches for speech enhancement. We have
shown that they may even outperform their predictive counterparts for
non-additive corruption types or when they are evaluated on mismatched
conditions. However, diffusion models suffer from a high computational burden,
mainly as they require to run a neural network for each reverse diffusion step,
whereas predictive approaches only require one pass. As diffusion models are
generative approaches they may also produce vocalizing and breathing artifacts
in adverse conditions. In comparison, in such difficult scenarios, predictive
models typically do not produce such artifacts but tend to distort the target
speech instead, thereby degrading the speech quality. In this work, we present
a stochastic regeneration approach where an estimate given by a predictive
model is provided as a guide for further diffusion. We show that the proposed
approach uses the predictive model to remove the vocalizing and breathing
artifacts while producing very high quality samples thanks to the diffusion
model, even in adverse conditions. We further show that this approach enables
to use lighter sampling schemes with fewer diffusion steps without sacrificing
quality, thus lifting the computational burden by an order of magnitude. Source
code and audio examples are available online (https://uhh.de/inf-sp-storm).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE/ACM Transactions on Audio, Speech and Language
  Processing, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech
  Models via Language-Specific Experts <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, Vassilina Nikoulina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whisper is a multitask and multilingual speech model covering 99 languages.
It yields commendable automatic speech recognition (ASR) results in a subset of
its covered languages, but the model still underperforms on a non-negligible
number of under-represented languages, a problem exacerbated in smaller model
versions. In this work, we propose DistilWhisper, an approach able to bridge
the performance gap in ASR for these languages while retaining the advantages
of multitask and multilingual capabilities. Our approach involves two key
strategies: lightweight modular ASR fine-tuning of whisper-small using
language-specific experts, and knowledge distillation from whisper-large-v2.
This dual approach allows us to effectively boost ASR performance while keeping
the robustness inherited from the multitask and multilingual pre-training.
Results demonstrate that our approach is more effective than standard
fine-tuning or LoRA adapters, boosting performance in the targeted languages
for both in- and out-of-domain test sets, while introducing only a negligible
parameter overhead at inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Steganalysis of Diverse Data Types: A review of
  methods, taxonomy, challenges and future directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04522v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04522v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Kheddar, Mustapha Hemis, Yassine Himeur, David Megías, Abbes Amira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steganography and steganalysis are two interrelated aspects of the field of
information security. Steganography seeks to conceal communications, whereas
steganalysis is aimed to either find them or even, if possible, recover the
data they contain. Steganography and steganalysis have attracted a great deal
of interest, particularly from law enforcement. Steganography is often used by
cybercriminals and even terrorists to avoid being captured while in possession
of incriminating evidence, even encrypted, since cryptography is prohibited or
restricted in many countries. Therefore, knowledge of cutting-edge techniques
to uncover concealed information is crucial in exposing illegal acts. Over the
last few years, a number of strong and reliable steganography and steganalysis
techniques have been introduced in the literature. This review paper provides a
comprehensive overview of deep learning-based steganalysis techniques used to
detect hidden information within digital media. The paper covers all types of
cover in steganalysis, including image, audio, and video, and discusses the
most commonly used deep learning techniques. In addition, the paper explores
the use of more advanced deep learning techniques, such as deep transfer
learning (DTL) and deep reinforcement learning (DRL), to enhance the
performance of steganalysis systems. The paper provides a systematic review of
recent research in the field, including data sets and evaluation metrics used
in recent studies. It also presents a detailed analysis of DTL-based
steganalysis approaches and their performance on different data sets. The
review concludes with a discussion on the current state of deep learning-based
steganalysis, challenges, and future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Cross Attention for Audio-Visual Person Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Gnana Praveen, Jahangir Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although person or identity verification has been predominantly explored
using individual modalities such as face and voice, audio-visual fusion has
recently shown immense potential to outperform unimodal approaches. Audio and
visual modalities are often expected to pose strong complementary
relationships, which plays a crucial role in effective audio-visual fusion.
However, they may not always strongly complement each other, they may also
exhibit weak complementary relationships, resulting in poor audio-visual
feature representations. In this paper, we propose a Dynamic Cross-Attention
(DCA) model that can dynamically select the cross-attended or unattended
features on the fly based on the strong or weak complementary relationships,
respectively, across audio and visual modalities. In particular, a conditional
gating layer is designed to evaluate the contribution of the cross-attention
mechanism and choose cross-attended features only when they exhibit strong
complementary relationships, otherwise unattended features. Extensive
experiments are conducted on the Voxceleb1 dataset to demonstrate the
robustness of the proposed model. Results indicate that the proposed model
consistently improves the performance on multiple variants of cross-attention
while outperforming the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to FG2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Person Verification based on Recursive Fusion of Joint
  Cross-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Gnana Praveen, Jahangir Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person or identity verification has been recently gaining a lot of attention
using audio-visual fusion as faces and voices share close associations with
each other. Conventional approaches based on audio-visual fusion rely on
score-level or early feature-level fusion techniques. Though existing
approaches showed improvement over unimodal systems, the potential of
audio-visual fusion for person verification is not fully exploited. In this
paper, we have investigated the prospect of effectively capturing both the
intra- and inter-modal relationships across audio and visual modalities, which
can play a crucial role in significantly improving the fusion performance over
unimodal systems. In particular, we introduce a recursive fusion of a joint
cross-attentional model, where a joint audio-visual feature representation is
employed in the cross-attention framework in a recursive fashion to
progressively refine the feature representations that can efficiently capture
the intra-and inter-modal relationships. To further enhance the audio-visual
feature representations, we have also explored BLSTMs to improve the temporal
modeling of audio-visual feature representations. Extensive experiments are
conducted on the Voxceleb1 dataset to evaluate the proposed model. Results
indicate that the proposed model shows promising improvement in fusion
performance by adeptly capturing the intra-and inter-modal relationships across
audio and visual modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to FG2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Acoustic Scene Mapping Based on Acoustic Features and
  Dimensionality Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idan Cohen, Ofir Lindenbaum, Sharon Gannot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical methods for acoustic scene mapping require the estimation of time
difference of arrival (TDOA) between microphones. Unfortunately, TDOA
estimation is very sensitive to reverberation and additive noise. We introduce
an unsupervised data-driven approach that exploits the natural structure of the
data. Our method builds upon local conformal autoencoders (LOCA) - an offline
deep learning scheme for learning standardized data coordinates from
measurements. Our experimental setup includes a microphone array that measures
the transmitted sound source at multiple locations across the acoustic
enclosure. We demonstrate that LOCA learns a representation that is isometric
to the spatial locations of the microphones. The performance of our method is
evaluated using a series of realistic simulations and compared with other
dimensionality-reduction schemes. We further assess the influence of
reverberation on the results of LOCA and show that it demonstrates considerable
robustness.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T02:20:51.759380727Z">
           <a href="https://github.com/Shayne-Ada/arxivpaper/actions">
                <img id="build-timestamp-badge"
                     src="https://img.shields.io/github/workflow/status/Shayne-Ada/arxivpaper/Update?label=2024-03-28 02:20:51 UTC&style=for-the-badge"
                alt="2024-03-28 02:20:51 UTC">
            </a>
            2024-03-28 02:20:51 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
