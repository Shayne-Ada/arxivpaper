{"2024-03-12T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2402.13018v4","updated":"2024-03-12T07:13:02Z","published":"2024-02-20T14:00:53Z","title":"EMO-SUPERB: An In-depth Look at Speech Emotion Recognition","summary":"  Speech emotion recognition (SER) is a pivotal technology for human-computer\ninteraction systems. However, 80.77% of SER papers yield results that cannot be\nreproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal\nPERformance Benchmark, which aims to enhance open-source initiatives for SER.\nEMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art\nspeech self-supervised learning models (SSLMs) for exhaustive evaluation across\nsix open-source SER datasets. EMO-SUPERB streamlines result sharing via an\nonline leaderboard, fostering collaboration within a community-driven benchmark\nand thereby enhancing the development of SER. On average, 2.58% of annotations\nare annotated using natural language. SER relies on classification models and\nis unable to process natural languages, leading to the discarding of these\nvaluable annotations. We prompt ChatGPT to mimic annotators, comprehend natural\nlanguage annotations, and subsequently re-label the data. By utilizing labels\ngenerated by ChatGPT, we consistently achieve an average relative gain of 3.08%\nacross all settings.\n","authors":["Haibin Wu","Huang-Cheng Chou","Kai-Wei Chang","Lucas Goncalves","Jiawei Du","Jyh-Shing Roger Jang","Chi-Chun Lee","Hung-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13018v4.pdf","comment":"webpage: https://emosuperb.github.io/"},{"id":"http://arxiv.org/abs/2403.05820v2","updated":"2024-03-12T11:26:07Z","published":"2024-03-09T06:59:47Z","title":"An Audio-textual Diffusion Model For Converting Speech Signals Into\n  Ultrasound Tongue Imaging Data","summary":"  Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator\nmovements, such as ultrasound tongue imaging (UTI) data. An issue of existing\nAAI methods is only using the personalized acoustic information to derive the\ngeneral patterns of tongue motions, and thus the quality of generated UTI data\nis limited. To address this issue, this paper proposes an audio-textual\ndiffusion model for the UTI data generation task. In this model, the inherent\nacoustic characteristics of individuals related to the tongue motion details\nare encoded by using wav2vec 2.0, while the ASR transcriptions related to the\nuniversality of tongue motions are encoded by using BERT. UTI data are then\ngenerated by using a diffusion module. Experimental results showed that the\nproposed diffusion model could generate high-quality UTI data with clear tongue\ncontour that is crucial for the linguistic analysis and clinical assessment.\nThe project can be found on the\nwebsite\\footnote{https://yangyudong2020.github.io/wav2uti/\n","authors":["Yudong Yang","Rongfeng Su","Xiaokang Liu","Nan Yan","Lan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05820v2.pdf","comment":"ICASSP2024 Accept"},{"id":"http://arxiv.org/abs/2403.05791v2","updated":"2024-03-12T14:32:25Z","published":"2024-03-09T04:37:34Z","title":"Asynchronous Microphone Array Calibration using Hybrid TDOA Information","summary":"  Asynchronous Microphone array calibration is a prerequisite for most audition\nrobot applications. In practice, the calibration requires estimating microphone\npositions, time offsets, clock drift rates, and sound event locations\nsimultaneously. The existing method proposed Graph-based Simultaneous\nLocalisation and Mapping (Graph-SLAM) utilizing common TDOA, time difference of\narrival between two microphones (TDOA-M), and odometry measurement, however, it\nheavily depends on the initial value. In this paper, we propose a novel TDOA,\ntime difference of arrival between adjacent sound events (TDOA-S), combine it\nwith TDOA-M, called hybrid TDOA, and add odometry measurement to construct\nGraph-SLAM and use the Gauss-Newton (GN) method to solve. TDOA-S is simple and\nefficient because it eliminates time offset without generating new variables.\nSimulation and real-world experiment results consistently show that our method\nis independent of microphone number, insensitive to initial values, and has\nbetter calibration accuracy and stability under various TDOA noises. In\naddition, the simulation result demonstrates that our method has a lower\nCram\\'er-Rao lower bound (CRLB) for microphone parameters, which explains the\nadvantages of my method.\n","authors":["Chengjie Zhang","Jiang Wang","He Kong"],"pdf_url":"https://arxiv.org/pdf/2403.05791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07802v1","updated":"2024-03-12T16:41:31Z","published":"2024-03-12T16:41:31Z","title":"Boosting keyword spotting through on-device learnable user speech\n  characteristics","summary":"  Keyword spotting systems for always-on TinyML-constrained applications\nrequire on-site tuning to boost the accuracy of offline trained classifiers\nwhen deployed in unseen inference conditions. Adapting to the speech\npeculiarities of target users requires many in-domain samples, often\nunavailable in real-world scenarios. Furthermore, current on-device learning\ntechniques rely on computationally intensive and memory-hungry backbone update\nschemes, unfit for always-on, battery-powered devices. In this work, we propose\na novel on-device learning architecture, composed of a pretrained backbone and\na user-aware embedding learning the user's speech characteristics. The\nso-generated features are fused and used to classify the input utterance. For\ndomain shifts generated by unseen speakers, we measure error rate reductions of\nup to 19% from 30.1% to 24.3% based on the 35-class problem of the Google\nSpeech Commands dataset, through the inexpensive update of the user\nprojections. We moreover demonstrate the few-shot learning capabilities of our\nproposed architecture in sample- and class-scarce learning conditions. With\n23.7 kparameters and 1 MFLOP per epoch required for on-device training, our\nsystem is feasible for TinyML applications aimed at battery-powered\nmicrocontrollers.\n","authors":["Cristian Cioflan","Lukas Cavigelli","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.07802v1.pdf","comment":"5 pages, 3 tables, 2 figures. Accepted as a full paper by the tinyML\n  Research Symposium 2024"},{"id":"http://arxiv.org/abs/2212.11851v2","updated":"2024-03-12T15:31:01Z","published":"2022-12-22T16:35:42Z","title":"StoRM: A Diffusion-based Stochastic Regeneration Model for Speech\n  Enhancement and Dereverberation","summary":"  Diffusion models have shown a great ability at bridging the performance gap\nbetween predictive and generative approaches for speech enhancement. We have\nshown that they may even outperform their predictive counterparts for\nnon-additive corruption types or when they are evaluated on mismatched\nconditions. However, diffusion models suffer from a high computational burden,\nmainly as they require to run a neural network for each reverse diffusion step,\nwhereas predictive approaches only require one pass. As diffusion models are\ngenerative approaches they may also produce vocalizing and breathing artifacts\nin adverse conditions. In comparison, in such difficult scenarios, predictive\nmodels typically do not produce such artifacts but tend to distort the target\nspeech instead, thereby degrading the speech quality. In this work, we present\na stochastic regeneration approach where an estimate given by a predictive\nmodel is provided as a guide for further diffusion. We show that the proposed\napproach uses the predictive model to remove the vocalizing and breathing\nartifacts while producing very high quality samples thanks to the diffusion\nmodel, even in adverse conditions. We further show that this approach enables\nto use lighter sampling schemes with fewer diffusion steps without sacrificing\nquality, thus lifting the computational burden by an order of magnitude. Source\ncode and audio examples are available online (https://uhh.de/inf-sp-storm).\n","authors":["Jean-Marie Lemercier","Julius Richter","Simon Welker","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2212.11851v2.pdf","comment":"Published in IEEE/ACM Transactions on Audio, Speech and Language\n  Processing, 2023"},{"id":"http://arxiv.org/abs/2311.01070v3","updated":"2024-03-12T14:50:30Z","published":"2023-11-02T08:37:30Z","title":"Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech\n  Models via Language-Specific Experts","summary":"  Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still underperforms on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we propose DistilWhisper, an approach able to bridge\nthe performance gap in ASR for these languages while retaining the advantages\nof multitask and multilingual capabilities. Our approach involves two key\nstrategies: lightweight modular ASR fine-tuning of whisper-small using\nlanguage-specific experts, and knowledge distillation from whisper-large-v2.\nThis dual approach allows us to effectively boost ASR performance while keeping\nthe robustness inherited from the multitask and multilingual pre-training.\nResults demonstrate that our approach is more effective than standard\nfine-tuning or LoRA adapters, boosting performance in the targeted languages\nfor both in- and out-of-domain test sets, while introducing only a negligible\nparameter overhead at inference.\n","authors":["Thomas Palmeira Ferraz","Marcely Zanon Boito","Caroline Brun","Vassilina Nikoulina"],"pdf_url":"https://arxiv.org/pdf/2311.01070v3.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.07675v1","updated":"2024-03-12T14:11:29Z","published":"2024-03-12T14:11:29Z","title":"Multichannel Long-Term Streaming Neural Speech Enhancement for Static\n  and Moving Speakers","summary":"  In this work, we extend our previously proposed offline SpatialNet for\nlong-term streaming multichannel speech enhancement in both static and moving\nspeaker scenarios. SpatialNet exploits spatial information, such as the\nspatial/steering direction of speech, for discriminating between target speech\nand interferences, and achieved outstanding performance. The core of SpatialNet\nis a narrow-band self-attention module used for learning the temporal dynamic\nof spatial vectors. Towards long-term streaming speech enhancement, we propose\nto replace the offline self-attention network with online networks that have\nlinear inference complexity w.r.t signal length and meanwhile maintain the\ncapability of learning long-term information. Three variants are developed\nbased on (i) masked self-attention, (ii) Retention, a self-attention variant\nwith linear inference complexity, and (iii) Mamba, a\nstructured-state-space-based RNN-like network. Moreover, we investigate the\nlength extrapolation ability of different networks, namely test on signals that\nare much longer than training signals, and propose a short-signal training plus\nlong-signal fine-tuning strategy, which largely improves the length\nextrapolation ability of the networks within limited training time. Overall,\nthe proposed online SpatialNet achieves outstanding speech enhancement\nperformance for long audio streams, and for both static and moving speakers.\nThe proposed method will be open-sourced in\nhttps://github.com/Audio-WestlakeU/NBSS.\n","authors":["Changsheng Quan","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2403.07675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07661v1","updated":"2024-03-12T13:50:58Z","published":"2024-03-12T13:50:58Z","title":"Gender-ambiguous voice generation through feminine speaking style\n  transfer in male voices","summary":"  Recently, and under the umbrella of Responsible AI, efforts have been made to\ndevelop gender-ambiguous synthetic speech to represent with a single voice all\nindividuals in the gender spectrum. However, research efforts have completely\noverlooked the speaking style despite differences found among binary and\nnon-binary populations. In this work, we synthesise gender-ambiguous speech by\ncombining the timbre of a male speaker with the manner of speech of a female\nspeaker using voice morphing and pitch shifting towards the male-female\nboundary. Subjective evaluations indicate that the ambiguity of the morphed\nsamples that convey the female speech style is higher than those that undergo\npure pitch transformations suggesting that the speaking style can be a\ncontributing factor in creating gender-ambiguous speech. To our knowledge, this\nis the first study that explicitly uses the transfer of the speaking style to\ncreate gender-ambiguous voices.\n","authors":["Maria Koutsogiannaki","Shafel Mc Dowall","Ioannis Agiomyrgiannakis"],"pdf_url":"https://arxiv.org/pdf/2403.07661v1.pdf","comment":"5 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2308.04522v3","updated":"2024-03-12T00:16:38Z","published":"2023-08-08T18:37:24Z","title":"Deep Learning for Steganalysis of Diverse Data Types: A review of\n  methods, taxonomy, challenges and future directions","summary":"  Steganography and steganalysis are two interrelated aspects of the field of\ninformation security. Steganography seeks to conceal communications, whereas\nsteganalysis is aimed to either find them or even, if possible, recover the\ndata they contain. Steganography and steganalysis have attracted a great deal\nof interest, particularly from law enforcement. Steganography is often used by\ncybercriminals and even terrorists to avoid being captured while in possession\nof incriminating evidence, even encrypted, since cryptography is prohibited or\nrestricted in many countries. Therefore, knowledge of cutting-edge techniques\nto uncover concealed information is crucial in exposing illegal acts. Over the\nlast few years, a number of strong and reliable steganography and steganalysis\ntechniques have been introduced in the literature. This review paper provides a\ncomprehensive overview of deep learning-based steganalysis techniques used to\ndetect hidden information within digital media. The paper covers all types of\ncover in steganalysis, including image, audio, and video, and discusses the\nmost commonly used deep learning techniques. In addition, the paper explores\nthe use of more advanced deep learning techniques, such as deep transfer\nlearning (DTL) and deep reinforcement learning (DRL), to enhance the\nperformance of steganalysis systems. The paper provides a systematic review of\nrecent research in the field, including data sets and evaluation metrics used\nin recent studies. It also presents a detailed analysis of DTL-based\nsteganalysis approaches and their performance on different data sets. The\nreview concludes with a discussion on the current state of deep learning-based\nsteganalysis, challenges, and future research directions.\n","authors":["Hamza Kheddar","Mustapha Hemis","Yassine Himeur","David Megías","Abbes Amira"],"pdf_url":"https://arxiv.org/pdf/2308.04522v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04661v2","updated":"2024-03-12T20:52:02Z","published":"2024-03-07T17:07:51Z","title":"Dynamic Cross Attention for Audio-Visual Person Verification","summary":"  Although person or identity verification has been predominantly explored\nusing individual modalities such as face and voice, audio-visual fusion has\nrecently shown immense potential to outperform unimodal approaches. Audio and\nvisual modalities are often expected to pose strong complementary\nrelationships, which plays a crucial role in effective audio-visual fusion.\nHowever, they may not always strongly complement each other, they may also\nexhibit weak complementary relationships, resulting in poor audio-visual\nfeature representations. In this paper, we propose a Dynamic Cross-Attention\n(DCA) model that can dynamically select the cross-attended or unattended\nfeatures on the fly based on the strong or weak complementary relationships,\nrespectively, across audio and visual modalities. In particular, a conditional\ngating layer is designed to evaluate the contribution of the cross-attention\nmechanism and choose cross-attended features only when they exhibit strong\ncomplementary relationships, otherwise unattended features. Extensive\nexperiments are conducted on the Voxceleb1 dataset to demonstrate the\nrobustness of the proposed model. Results indicate that the proposed model\nconsistently improves the performance on multiple variants of cross-attention\nwhile outperforming the state-of-the-art methods.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.04661v2.pdf","comment":"Accepted to FG2024"},{"id":"http://arxiv.org/abs/2403.04654v2","updated":"2024-03-12T20:50:27Z","published":"2024-03-07T16:57:45Z","title":"Audio-Visual Person Verification based on Recursive Fusion of Joint\n  Cross-Attention","summary":"  Person or identity verification has been recently gaining a lot of attention\nusing audio-visual fusion as faces and voices share close associations with\neach other. Conventional approaches based on audio-visual fusion rely on\nscore-level or early feature-level fusion techniques. Though existing\napproaches showed improvement over unimodal systems, the potential of\naudio-visual fusion for person verification is not fully exploited. In this\npaper, we have investigated the prospect of effectively capturing both the\nintra- and inter-modal relationships across audio and visual modalities, which\ncan play a crucial role in significantly improving the fusion performance over\nunimodal systems. In particular, we introduce a recursive fusion of a joint\ncross-attentional model, where a joint audio-visual feature representation is\nemployed in the cross-attention framework in a recursive fashion to\nprogressively refine the feature representations that can efficiently capture\nthe intra-and inter-modal relationships. To further enhance the audio-visual\nfeature representations, we have also explored BLSTMs to improve the temporal\nmodeling of audio-visual feature representations. Extensive experiments are\nconducted on the Voxceleb1 dataset to evaluate the proposed model. Results\nindicate that the proposed model shows promising improvement in fusion\nperformance by adeptly capturing the intra-and inter-modal relationships across\naudio and visual modalities.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.04654v2.pdf","comment":"Accepted to FG2024"},{"id":"http://arxiv.org/abs/2301.00448v2","updated":"2024-03-12T18:48:40Z","published":"2023-01-01T17:46:09Z","title":"Unsupervised Acoustic Scene Mapping Based on Acoustic Features and\n  Dimensionality Reduction","summary":"  Classical methods for acoustic scene mapping require the estimation of time\ndifference of arrival (TDOA) between microphones. Unfortunately, TDOA\nestimation is very sensitive to reverberation and additive noise. We introduce\nan unsupervised data-driven approach that exploits the natural structure of the\ndata. Our method builds upon local conformal autoencoders (LOCA) - an offline\ndeep learning scheme for learning standardized data coordinates from\nmeasurements. Our experimental setup includes a microphone array that measures\nthe transmitted sound source at multiple locations across the acoustic\nenclosure. We demonstrate that LOCA learns a representation that is isometric\nto the spatial locations of the microphones. The performance of our method is\nevaluated using a series of realistic simulations and compared with other\ndimensionality-reduction schemes. We further assess the influence of\nreverberation on the results of LOCA and show that it demonstrates considerable\nrobustness.\n","authors":["Idan Cohen","Ofir Lindenbaum","Sharon Gannot"],"pdf_url":"https://arxiv.org/pdf/2301.00448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07995v1","updated":"2024-03-12T18:03:08Z","published":"2024-03-12T18:03:08Z","title":"Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic\n  Music Generation","summary":"  Modelling musical structure is vital yet challenging for artificial\nintelligence systems that generate symbolic music compositions. This literature\nreview dissects the evolution of techniques for incorporating coherent\nstructure, from symbolic approaches to foundational and transformative deep\nlearning methods that harness the power of computation and data across a wide\nvariety of training paradigms. In the later stages, we review an emerging\ntechnique which we refer to as \"sub-task decomposition\" that involves\ndecomposing music generation into separate high-level structural planning and\ncontent creation stages. Such systems incorporate some form of musical\nknowledge or neuro-symbolic methods by extracting melodic skeletons or\nstructural templates to guide the generation. Progress is evident in capturing\nmotifs and repetitions across all three eras reviewed, yet modelling the\nnuanced development of themes across extended compositions in the style of\nhuman composers remains difficult. We outline several key future directions to\nrealize the synergistic benefits of combining approaches from all eras\nexamined.\n","authors":["Keshav Bhandari","Simon Colton"],"pdf_url":"https://arxiv.org/pdf/2403.07995v1.pdf","comment":"Accepted to 13th International Conference on Artificial Intelligence\n  in Music, Sound, Art and Design (EvoMUSART) 2024"},{"id":"http://arxiv.org/abs/2403.10549v1","updated":"2024-03-12T19:54:35Z","published":"2024-03-12T19:54:35Z","title":"On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge\n  Embedded Systems","summary":"  Keyword spotting accuracy degrades when neural networks are exposed to noisy\nenvironments. On-site adaptation to previously unseen noise is crucial to\nrecovering accuracy loss, and on-device learning is required to ensure that the\nadaptation process happens entirely on the edge device. In this work, we\npropose a fully on-device domain adaptation system achieving up to 14% accuracy\ngains over already-robust keyword spotting models. We enable on-device learning\nwith less than 10 kB of memory, using only 100 labeled utterances to recover 5%\naccuracy after adapting to the complex speech noise. We demonstrate that domain\nadaptation can be achieved on ultra-low-power microcontrollers with as little\nas 806 mJ in only 14 s on always-on, battery-operated devices.\n","authors":["Cristian Cioflan","Lukas Cavigelli","Manuele Rusci","Miguel de Prado","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.10549v1.pdf","comment":"5 pages, 2 tables, 2 figures. Accepted at IEEE AICAS 2024"},{"id":"http://arxiv.org/abs/2403.10548v1","updated":"2024-03-12T16:46:57Z","published":"2024-03-12T16:46:57Z","title":"Two-sided Acoustic Metascreen for Broadband and Individual Reflection\n  and Transmission Control","summary":"  Acoustic wave modulation plays a pivotal role in various applications,\nincluding sound-field reconstruction, wireless communication, and particle\nmanipulation, among others. However, current acoustic metamaterial and\nmetasurface designs typically focus on controlling either reflection or\ntransmission waves, often overlooking the coupling between amplitude and phase\nof acoustic waves. To fulfill this gap, we propose and experimentally validate\na design enabling complete control of reflected and transmitted acoustic waves\nindividually across a frequency range of 4 kHz to 8 kHz, allowing arbitrary\ncombinations of amplitude and phase for reflected and transmitted sound in a\nbroadband manner. Additionally, we demonstrate the significance of our approach\nfor sound manipulation by achieving acoustic diffusion, reflection, focusing,\nand generating a two-sided 3D hologram at three distinct frequencies. These\nfindings open an alternative avenue for extensively engineering sound waves,\npromising applications in acoustics and related fields.\n","authors":["Ao Chen","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10548v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2402.13018v4","updated":"2024-03-12T07:13:02Z","published":"2024-02-20T14:00:53Z","title":"EMO-SUPERB: An In-depth Look at Speech Emotion Recognition","summary":"  Speech emotion recognition (SER) is a pivotal technology for human-computer\ninteraction systems. However, 80.77% of SER papers yield results that cannot be\nreproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal\nPERformance Benchmark, which aims to enhance open-source initiatives for SER.\nEMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art\nspeech self-supervised learning models (SSLMs) for exhaustive evaluation across\nsix open-source SER datasets. EMO-SUPERB streamlines result sharing via an\nonline leaderboard, fostering collaboration within a community-driven benchmark\nand thereby enhancing the development of SER. On average, 2.58% of annotations\nare annotated using natural language. SER relies on classification models and\nis unable to process natural languages, leading to the discarding of these\nvaluable annotations. We prompt ChatGPT to mimic annotators, comprehend natural\nlanguage annotations, and subsequently re-label the data. By utilizing labels\ngenerated by ChatGPT, we consistently achieve an average relative gain of 3.08%\nacross all settings.\n","authors":["Haibin Wu","Huang-Cheng Chou","Kai-Wei Chang","Lucas Goncalves","Jiawei Du","Jyh-Shing Roger Jang","Chi-Chun Lee","Hung-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13018v4.pdf","comment":"webpage: https://emosuperb.github.io/"},{"id":"http://arxiv.org/abs/2403.05820v2","updated":"2024-03-12T11:26:07Z","published":"2024-03-09T06:59:47Z","title":"An Audio-textual Diffusion Model For Converting Speech Signals Into\n  Ultrasound Tongue Imaging Data","summary":"  Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator\nmovements, such as ultrasound tongue imaging (UTI) data. An issue of existing\nAAI methods is only using the personalized acoustic information to derive the\ngeneral patterns of tongue motions, and thus the quality of generated UTI data\nis limited. To address this issue, this paper proposes an audio-textual\ndiffusion model for the UTI data generation task. In this model, the inherent\nacoustic characteristics of individuals related to the tongue motion details\nare encoded by using wav2vec 2.0, while the ASR transcriptions related to the\nuniversality of tongue motions are encoded by using BERT. UTI data are then\ngenerated by using a diffusion module. Experimental results showed that the\nproposed diffusion model could generate high-quality UTI data with clear tongue\ncontour that is crucial for the linguistic analysis and clinical assessment.\nThe project can be found on the\nwebsite\\footnote{https://yangyudong2020.github.io/wav2uti/\n","authors":["Yudong Yang","Rongfeng Su","Xiaokang Liu","Nan Yan","Lan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05820v2.pdf","comment":"ICASSP2024 Accept"},{"id":"http://arxiv.org/abs/2403.05791v2","updated":"2024-03-12T14:32:25Z","published":"2024-03-09T04:37:34Z","title":"Asynchronous Microphone Array Calibration using Hybrid TDOA Information","summary":"  Asynchronous Microphone array calibration is a prerequisite for most audition\nrobot applications. In practice, the calibration requires estimating microphone\npositions, time offsets, clock drift rates, and sound event locations\nsimultaneously. The existing method proposed Graph-based Simultaneous\nLocalisation and Mapping (Graph-SLAM) utilizing common TDOA, time difference of\narrival between two microphones (TDOA-M), and odometry measurement, however, it\nheavily depends on the initial value. In this paper, we propose a novel TDOA,\ntime difference of arrival between adjacent sound events (TDOA-S), combine it\nwith TDOA-M, called hybrid TDOA, and add odometry measurement to construct\nGraph-SLAM and use the Gauss-Newton (GN) method to solve. TDOA-S is simple and\nefficient because it eliminates time offset without generating new variables.\nSimulation and real-world experiment results consistently show that our method\nis independent of microphone number, insensitive to initial values, and has\nbetter calibration accuracy and stability under various TDOA noises. In\naddition, the simulation result demonstrates that our method has a lower\nCram\\'er-Rao lower bound (CRLB) for microphone parameters, which explains the\nadvantages of my method.\n","authors":["Chengjie Zhang","Jiang Wang","He Kong"],"pdf_url":"https://arxiv.org/pdf/2403.05791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07802v1","updated":"2024-03-12T16:41:31Z","published":"2024-03-12T16:41:31Z","title":"Boosting keyword spotting through on-device learnable user speech\n  characteristics","summary":"  Keyword spotting systems for always-on TinyML-constrained applications\nrequire on-site tuning to boost the accuracy of offline trained classifiers\nwhen deployed in unseen inference conditions. Adapting to the speech\npeculiarities of target users requires many in-domain samples, often\nunavailable in real-world scenarios. Furthermore, current on-device learning\ntechniques rely on computationally intensive and memory-hungry backbone update\nschemes, unfit for always-on, battery-powered devices. In this work, we propose\na novel on-device learning architecture, composed of a pretrained backbone and\na user-aware embedding learning the user's speech characteristics. The\nso-generated features are fused and used to classify the input utterance. For\ndomain shifts generated by unseen speakers, we measure error rate reductions of\nup to 19% from 30.1% to 24.3% based on the 35-class problem of the Google\nSpeech Commands dataset, through the inexpensive update of the user\nprojections. We moreover demonstrate the few-shot learning capabilities of our\nproposed architecture in sample- and class-scarce learning conditions. With\n23.7 kparameters and 1 MFLOP per epoch required for on-device training, our\nsystem is feasible for TinyML applications aimed at battery-powered\nmicrocontrollers.\n","authors":["Cristian Cioflan","Lukas Cavigelli","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.07802v1.pdf","comment":"5 pages, 3 tables, 2 figures. Accepted as a full paper by the tinyML\n  Research Symposium 2024"},{"id":"http://arxiv.org/abs/2403.07767v1","updated":"2024-03-12T15:54:32Z","published":"2024-03-12T15:54:32Z","title":"Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech\n  Recognition Datasets","summary":"  Paralinguistic traits like cognitive load and emotion are increasingly\nrecognized as pivotal areas in speech recognition research, often examined\nthrough specialized datasets like CLSE and IEMOCAP. However, the integrity of\nthese datasets is seldom scrutinized for text-dependency. This paper critically\nevaluates the prevalent assumption that machine learning models trained on such\ndatasets genuinely learn to identify paralinguistic traits, rather than merely\ncapturing lexical features. By examining the lexical overlap in these datasets\nand testing the performance of machine learning models, we expose significant\ntext-dependency in trait-labeling. Our results suggest that some machine\nlearning models, especially large pre-trained models like HuBERT, might\ninadvertently focus on lexical characteristics rather than the intended\nparalinguistic features. The study serves as a call to action for the research\ncommunity to reevaluate the reliability of existing datasets and methodologies,\nensuring that machine learning models genuinely learn what they are designed to\nrecognize.\n","authors":["Jan Pešán","Santosh Kesiraju","Lukáš Burget","Jan ''Honza'' Černocký"],"pdf_url":"https://arxiv.org/pdf/2403.07767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11851v2","updated":"2024-03-12T15:31:01Z","published":"2022-12-22T16:35:42Z","title":"StoRM: A Diffusion-based Stochastic Regeneration Model for Speech\n  Enhancement and Dereverberation","summary":"  Diffusion models have shown a great ability at bridging the performance gap\nbetween predictive and generative approaches for speech enhancement. We have\nshown that they may even outperform their predictive counterparts for\nnon-additive corruption types or when they are evaluated on mismatched\nconditions. However, diffusion models suffer from a high computational burden,\nmainly as they require to run a neural network for each reverse diffusion step,\nwhereas predictive approaches only require one pass. As diffusion models are\ngenerative approaches they may also produce vocalizing and breathing artifacts\nin adverse conditions. In comparison, in such difficult scenarios, predictive\nmodels typically do not produce such artifacts but tend to distort the target\nspeech instead, thereby degrading the speech quality. In this work, we present\na stochastic regeneration approach where an estimate given by a predictive\nmodel is provided as a guide for further diffusion. We show that the proposed\napproach uses the predictive model to remove the vocalizing and breathing\nartifacts while producing very high quality samples thanks to the diffusion\nmodel, even in adverse conditions. We further show that this approach enables\nto use lighter sampling schemes with fewer diffusion steps without sacrificing\nquality, thus lifting the computational burden by an order of magnitude. Source\ncode and audio examples are available online (https://uhh.de/inf-sp-storm).\n","authors":["Jean-Marie Lemercier","Julius Richter","Simon Welker","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2212.11851v2.pdf","comment":"Published in IEEE/ACM Transactions on Audio, Speech and Language\n  Processing, 2023"},{"id":"http://arxiv.org/abs/2311.01070v3","updated":"2024-03-12T14:50:30Z","published":"2023-11-02T08:37:30Z","title":"Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech\n  Models via Language-Specific Experts","summary":"  Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still underperforms on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we propose DistilWhisper, an approach able to bridge\nthe performance gap in ASR for these languages while retaining the advantages\nof multitask and multilingual capabilities. Our approach involves two key\nstrategies: lightweight modular ASR fine-tuning of whisper-small using\nlanguage-specific experts, and knowledge distillation from whisper-large-v2.\nThis dual approach allows us to effectively boost ASR performance while keeping\nthe robustness inherited from the multitask and multilingual pre-training.\nResults demonstrate that our approach is more effective than standard\nfine-tuning or LoRA adapters, boosting performance in the targeted languages\nfor both in- and out-of-domain test sets, while introducing only a negligible\nparameter overhead at inference.\n","authors":["Thomas Palmeira Ferraz","Marcely Zanon Boito","Caroline Brun","Vassilina Nikoulina"],"pdf_url":"https://arxiv.org/pdf/2311.01070v3.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.07675v1","updated":"2024-03-12T14:11:29Z","published":"2024-03-12T14:11:29Z","title":"Multichannel Long-Term Streaming Neural Speech Enhancement for Static\n  and Moving Speakers","summary":"  In this work, we extend our previously proposed offline SpatialNet for\nlong-term streaming multichannel speech enhancement in both static and moving\nspeaker scenarios. SpatialNet exploits spatial information, such as the\nspatial/steering direction of speech, for discriminating between target speech\nand interferences, and achieved outstanding performance. The core of SpatialNet\nis a narrow-band self-attention module used for learning the temporal dynamic\nof spatial vectors. Towards long-term streaming speech enhancement, we propose\nto replace the offline self-attention network with online networks that have\nlinear inference complexity w.r.t signal length and meanwhile maintain the\ncapability of learning long-term information. Three variants are developed\nbased on (i) masked self-attention, (ii) Retention, a self-attention variant\nwith linear inference complexity, and (iii) Mamba, a\nstructured-state-space-based RNN-like network. Moreover, we investigate the\nlength extrapolation ability of different networks, namely test on signals that\nare much longer than training signals, and propose a short-signal training plus\nlong-signal fine-tuning strategy, which largely improves the length\nextrapolation ability of the networks within limited training time. Overall,\nthe proposed online SpatialNet achieves outstanding speech enhancement\nperformance for long audio streams, and for both static and moving speakers.\nThe proposed method will be open-sourced in\nhttps://github.com/Audio-WestlakeU/NBSS.\n","authors":["Changsheng Quan","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2403.07675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07661v1","updated":"2024-03-12T13:50:58Z","published":"2024-03-12T13:50:58Z","title":"Gender-ambiguous voice generation through feminine speaking style\n  transfer in male voices","summary":"  Recently, and under the umbrella of Responsible AI, efforts have been made to\ndevelop gender-ambiguous synthetic speech to represent with a single voice all\nindividuals in the gender spectrum. However, research efforts have completely\noverlooked the speaking style despite differences found among binary and\nnon-binary populations. In this work, we synthesise gender-ambiguous speech by\ncombining the timbre of a male speaker with the manner of speech of a female\nspeaker using voice morphing and pitch shifting towards the male-female\nboundary. Subjective evaluations indicate that the ambiguity of the morphed\nsamples that convey the female speech style is higher than those that undergo\npure pitch transformations suggesting that the speaking style can be a\ncontributing factor in creating gender-ambiguous speech. To our knowledge, this\nis the first study that explicitly uses the transfer of the speaking style to\ncreate gender-ambiguous voices.\n","authors":["Maria Koutsogiannaki","Shafel Mc Dowall","Ioannis Agiomyrgiannakis"],"pdf_url":"https://arxiv.org/pdf/2403.07661v1.pdf","comment":"5 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2403.07579v1","updated":"2024-03-12T12:07:56Z","published":"2024-03-12T12:07:56Z","title":"On HRTF Notch Frequency Prediction Using Anthropometric Features and\n  Neural Networks","summary":"  High fidelity spatial audio often performs better when produced using a\npersonalized head-related transfer function (HRTF). However, the direct\nacquisition of HRTFs is cumbersome and requires specialized equipment. Thus,\nmany personalization methods estimate HRTF features from easily obtained\nanthropometric features of the pinna, head, and torso. The first HRTF notch\nfrequency (N1) is known to be a dominant feature in elevation localization, and\nthus a useful feature for HRTF personalization. This paper describes the\nprediction of N1 frequency from pinna anthropometry using a neural model.\nPrediction is performed separately on three databases, both simulated and\nmeasured, and then by domain mixing in-between the databases. The model\nsuccessfully predicts N1 frequency for individual databases and by domain\nmixing between some databases. Prediction errors are better or comparable to\nthose previously reported, showing significant improvement when acquired over a\nlarge database and with a larger output range.\n","authors":["Lior Arbel","Ishwarya Ananthabhotla","Zamir Ben-Hur","David Lou Alon","Boaz Rafaely"],"pdf_url":"https://arxiv.org/pdf/2403.07579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04522v3","updated":"2024-03-12T00:16:38Z","published":"2023-08-08T18:37:24Z","title":"Deep Learning for Steganalysis of Diverse Data Types: A review of\n  methods, taxonomy, challenges and future directions","summary":"  Steganography and steganalysis are two interrelated aspects of the field of\ninformation security. Steganography seeks to conceal communications, whereas\nsteganalysis is aimed to either find them or even, if possible, recover the\ndata they contain. Steganography and steganalysis have attracted a great deal\nof interest, particularly from law enforcement. Steganography is often used by\ncybercriminals and even terrorists to avoid being captured while in possession\nof incriminating evidence, even encrypted, since cryptography is prohibited or\nrestricted in many countries. Therefore, knowledge of cutting-edge techniques\nto uncover concealed information is crucial in exposing illegal acts. Over the\nlast few years, a number of strong and reliable steganography and steganalysis\ntechniques have been introduced in the literature. This review paper provides a\ncomprehensive overview of deep learning-based steganalysis techniques used to\ndetect hidden information within digital media. The paper covers all types of\ncover in steganalysis, including image, audio, and video, and discusses the\nmost commonly used deep learning techniques. In addition, the paper explores\nthe use of more advanced deep learning techniques, such as deep transfer\nlearning (DTL) and deep reinforcement learning (DRL), to enhance the\nperformance of steganalysis systems. The paper provides a systematic review of\nrecent research in the field, including data sets and evaluation metrics used\nin recent studies. It also presents a detailed analysis of DTL-based\nsteganalysis approaches and their performance on different data sets. The\nreview concludes with a discussion on the current state of deep learning-based\nsteganalysis, challenges, and future research directions.\n","authors":["Hamza Kheddar","Mustapha Hemis","Yassine Himeur","David Megías","Abbes Amira"],"pdf_url":"https://arxiv.org/pdf/2308.04522v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04661v2","updated":"2024-03-12T20:52:02Z","published":"2024-03-07T17:07:51Z","title":"Dynamic Cross Attention for Audio-Visual Person Verification","summary":"  Although person or identity verification has been predominantly explored\nusing individual modalities such as face and voice, audio-visual fusion has\nrecently shown immense potential to outperform unimodal approaches. Audio and\nvisual modalities are often expected to pose strong complementary\nrelationships, which plays a crucial role in effective audio-visual fusion.\nHowever, they may not always strongly complement each other, they may also\nexhibit weak complementary relationships, resulting in poor audio-visual\nfeature representations. In this paper, we propose a Dynamic Cross-Attention\n(DCA) model that can dynamically select the cross-attended or unattended\nfeatures on the fly based on the strong or weak complementary relationships,\nrespectively, across audio and visual modalities. In particular, a conditional\ngating layer is designed to evaluate the contribution of the cross-attention\nmechanism and choose cross-attended features only when they exhibit strong\ncomplementary relationships, otherwise unattended features. Extensive\nexperiments are conducted on the Voxceleb1 dataset to demonstrate the\nrobustness of the proposed model. Results indicate that the proposed model\nconsistently improves the performance on multiple variants of cross-attention\nwhile outperforming the state-of-the-art methods.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.04661v2.pdf","comment":"Accepted to FG2024"},{"id":"http://arxiv.org/abs/2403.04654v2","updated":"2024-03-12T20:50:27Z","published":"2024-03-07T16:57:45Z","title":"Audio-Visual Person Verification based on Recursive Fusion of Joint\n  Cross-Attention","summary":"  Person or identity verification has been recently gaining a lot of attention\nusing audio-visual fusion as faces and voices share close associations with\neach other. Conventional approaches based on audio-visual fusion rely on\nscore-level or early feature-level fusion techniques. Though existing\napproaches showed improvement over unimodal systems, the potential of\naudio-visual fusion for person verification is not fully exploited. In this\npaper, we have investigated the prospect of effectively capturing both the\nintra- and inter-modal relationships across audio and visual modalities, which\ncan play a crucial role in significantly improving the fusion performance over\nunimodal systems. In particular, we introduce a recursive fusion of a joint\ncross-attentional model, where a joint audio-visual feature representation is\nemployed in the cross-attention framework in a recursive fashion to\nprogressively refine the feature representations that can efficiently capture\nthe intra-and inter-modal relationships. To further enhance the audio-visual\nfeature representations, we have also explored BLSTMs to improve the temporal\nmodeling of audio-visual feature representations. Extensive experiments are\nconducted on the Voxceleb1 dataset to evaluate the proposed model. Results\nindicate that the proposed model shows promising improvement in fusion\nperformance by adeptly capturing the intra-and inter-modal relationships across\naudio and visual modalities.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.04654v2.pdf","comment":"Accepted to FG2024"},{"id":"http://arxiv.org/abs/2301.00448v2","updated":"2024-03-12T18:48:40Z","published":"2023-01-01T17:46:09Z","title":"Unsupervised Acoustic Scene Mapping Based on Acoustic Features and\n  Dimensionality Reduction","summary":"  Classical methods for acoustic scene mapping require the estimation of time\ndifference of arrival (TDOA) between microphones. Unfortunately, TDOA\nestimation is very sensitive to reverberation and additive noise. We introduce\nan unsupervised data-driven approach that exploits the natural structure of the\ndata. Our method builds upon local conformal autoencoders (LOCA) - an offline\ndeep learning scheme for learning standardized data coordinates from\nmeasurements. Our experimental setup includes a microphone array that measures\nthe transmitted sound source at multiple locations across the acoustic\nenclosure. We demonstrate that LOCA learns a representation that is isometric\nto the spatial locations of the microphones. The performance of our method is\nevaluated using a series of realistic simulations and compared with other\ndimensionality-reduction schemes. We further assess the influence of\nreverberation on the results of LOCA and show that it demonstrates considerable\nrobustness.\n","authors":["Idan Cohen","Ofir Lindenbaum","Sharon Gannot"],"pdf_url":"https://arxiv.org/pdf/2301.00448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07995v1","updated":"2024-03-12T18:03:08Z","published":"2024-03-12T18:03:08Z","title":"Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic\n  Music Generation","summary":"  Modelling musical structure is vital yet challenging for artificial\nintelligence systems that generate symbolic music compositions. This literature\nreview dissects the evolution of techniques for incorporating coherent\nstructure, from symbolic approaches to foundational and transformative deep\nlearning methods that harness the power of computation and data across a wide\nvariety of training paradigms. In the later stages, we review an emerging\ntechnique which we refer to as \"sub-task decomposition\" that involves\ndecomposing music generation into separate high-level structural planning and\ncontent creation stages. Such systems incorporate some form of musical\nknowledge or neuro-symbolic methods by extracting melodic skeletons or\nstructural templates to guide the generation. Progress is evident in capturing\nmotifs and repetitions across all three eras reviewed, yet modelling the\nnuanced development of themes across extended compositions in the style of\nhuman composers remains difficult. We outline several key future directions to\nrealize the synergistic benefits of combining approaches from all eras\nexamined.\n","authors":["Keshav Bhandari","Simon Colton"],"pdf_url":"https://arxiv.org/pdf/2403.07995v1.pdf","comment":"Accepted to 13th International Conference on Artificial Intelligence\n  in Music, Sound, Art and Design (EvoMUSART) 2024"},{"id":"http://arxiv.org/abs/2403.10549v1","updated":"2024-03-12T19:54:35Z","published":"2024-03-12T19:54:35Z","title":"On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge\n  Embedded Systems","summary":"  Keyword spotting accuracy degrades when neural networks are exposed to noisy\nenvironments. On-site adaptation to previously unseen noise is crucial to\nrecovering accuracy loss, and on-device learning is required to ensure that the\nadaptation process happens entirely on the edge device. In this work, we\npropose a fully on-device domain adaptation system achieving up to 14% accuracy\ngains over already-robust keyword spotting models. We enable on-device learning\nwith less than 10 kB of memory, using only 100 labeled utterances to recover 5%\naccuracy after adapting to the complex speech noise. We demonstrate that domain\nadaptation can be achieved on ultra-low-power microcontrollers with as little\nas 806 mJ in only 14 s on always-on, battery-operated devices.\n","authors":["Cristian Cioflan","Lukas Cavigelli","Manuele Rusci","Miguel de Prado","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.10549v1.pdf","comment":"5 pages, 2 tables, 2 figures. Accepted at IEEE AICAS 2024"},{"id":"http://arxiv.org/abs/2403.10548v1","updated":"2024-03-12T16:46:57Z","published":"2024-03-12T16:46:57Z","title":"Two-sided Acoustic Metascreen for Broadband and Individual Reflection\n  and Transmission Control","summary":"  Acoustic wave modulation plays a pivotal role in various applications,\nincluding sound-field reconstruction, wireless communication, and particle\nmanipulation, among others. However, current acoustic metamaterial and\nmetasurface designs typically focus on controlling either reflection or\ntransmission waves, often overlooking the coupling between amplitude and phase\nof acoustic waves. To fulfill this gap, we propose and experimentally validate\na design enabling complete control of reflected and transmitted acoustic waves\nindividually across a frequency range of 4 kHz to 8 kHz, allowing arbitrary\ncombinations of amplitude and phase for reflected and transmitted sound in a\nbroadband manner. Additionally, we demonstrate the significance of our approach\nfor sound manipulation by achieving acoustic diffusion, reflection, focusing,\nand generating a two-sided 3D hologram at three distinct frequencies. These\nfindings open an alternative avenue for extensively engineering sound waves,\npromising applications in acoustics and related fields.\n","authors":["Ao Chen","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10548v1.pdf","comment":null}]},"2024-03-13T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.06487v2","updated":"2024-03-13T00:41:36Z","published":"2024-03-11T07:50:29Z","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","summary":"  This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).\n","authors":["Koji Inoue","Bing'er Jiang","Erik Ekstedt","Tatsuya Kawahara","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2403.06487v2.pdf","comment":"This paper has been accepted for presentation at The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation (LREC-COLING 2024) and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2403.03522v2","updated":"2024-03-13T09:50:40Z","published":"2024-03-06T08:03:05Z","title":"Non-verbal information in spontaneous speech -- towards a new framework\n  of analysis","summary":"  Non-verbal signals in speech are encoded by prosody and carry information\nthat ranges from conversation action to attitude and emotion. Despite its\nimportance, the principles that govern prosodic structure are not yet\nadequately understood. This paper offers an analytical schema and a\ntechnological proof-of-concept for the categorization of prosodic signals and\ntheir association with meaning. The schema interprets surface-representations\nof multi-layered prosodic events. As a first step towards implementation, we\npresent a classification process that disentangles prosodic phenomena of three\norders. It relies on fine-tuning a pre-trained speech recognition model,\nenabling the simultaneous multi-class/multi-label detection. It generalizes\nover a large variety of spontaneous data, performing on a par with, or superior\nto, human annotation. In addition to a standardized formalization of prosody,\ndisentangling prosodic patterns can direct a theory of communication and speech\norganization. A welcome by-product is an interpretation of prosody that will\nenhance speech- and language-related technologies.\n","authors":["Tirza Biron","Moshe Barboy","Eran Ben-Artzy","Alona Golubchik","Yanir Marmor","Smadar Szekely","Yaron Winter","David Harel"],"pdf_url":"https://arxiv.org/pdf/2403.03522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08738v1","updated":"2024-03-13T17:42:03Z","published":"2024-03-13T17:42:03Z","title":"Improving Acoustic Word Embeddings through Correspondence Training of\n  Self-supervised Speech Representations","summary":"  Acoustic word embeddings (AWEs) are vector representations of spoken words.\nAn effective method for obtaining AWEs is the Correspondence Auto-Encoder\n(CAE). In the past, the CAE method has been associated with traditional MFCC\nfeatures. Representations obtained from self-supervised learning (SSL)-based\nspeech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many\ndownstream tasks. However, they have not been well studied in the context of\nlearning AWEs. This work explores the effectiveness of CAE with SSL-based\nspeech representations to obtain improved AWEs. Additionally, the capabilities\nof SSL-based speech models are explored in cross-lingual scenarios for\nobtaining AWEs. Experiments are conducted on five languages: Polish,\nPortuguese, Spanish, French, and English. HuBERT-based CAE model achieves the\nbest results for word discrimination in all languages, despite Hu-BERT being\npre-trained on English only. Also, the HuBERT-based CAE model works well in\ncross-lingual settings. It outperforms MFCC-based CAE models trained on the\ntarget languages when trained on one source language and tested on target\nlanguages.\n","authors":["Amit Meghanani","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2403.08738v1.pdf","comment":"Accepted to EACL 2024 Main Conference, Long paper"},{"id":"http://arxiv.org/abs/2403.08654v1","updated":"2024-03-13T16:08:59Z","published":"2024-03-13T16:08:59Z","title":"An Efficient End-to-End Approach to Noise Invariant Speech Features via\n  Multi-Task Learning","summary":"  Self-supervised speech representation learning enables the extraction of\nmeaningful features from raw waveforms. These features can then be efficiently\nused across multiple downstream tasks. However, two significant issues arise\nwhen considering the deployment of such methods ``in-the-wild\": (i) Their large\nsize, which can be prohibitive for edge applications; and (ii) their robustness\nto detrimental factors, such as noise and/or reverberation, that can heavily\ndegrade the performance of such systems. In this work, we propose\nRobustDistiller, a novel knowledge distillation mechanism that tackles both\nproblems jointly. Simultaneously to the distillation recipe, we apply a\nmulti-task learning objective to encourage the network to learn noise-invariant\nrepresentations by denoising the input. The proposed mechanism is evaluated on\ntwelve different downstream tasks. It outperforms several benchmarks regardless\nof noise type, or noise and reverberation levels. Experimental results show\nthat the new Student model with 23M parameters can achieve results comparable\nto the Teacher model with 95M parameters. Lastly, we show that the proposed\nrecipe can be applied to other distillation methodologies, such as the recent\nDPWavLM. For reproducibility, code and model checkpoints will be made available\nat \\mbox{\\url{https://github.com/Hguimaraes/robustdistiller}}.\n","authors":["Heitor R. Guimarães","Arthur Pimentel","Anderson R. Avila","Mehdi Rezagholizadeh","Boxing Chen","Tiago H. Falk"],"pdf_url":"https://arxiv.org/pdf/2403.08654v1.pdf","comment":"Under review on IEEE Transactions on Audio, Speech, and Language\n  Processing (2024)"},{"id":"http://arxiv.org/abs/2403.08559v1","updated":"2024-03-13T14:10:10Z","published":"2024-03-13T14:10:10Z","title":"End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier\n  Models","summary":"  This paper describes a data-driven approach to creating real-time neural\nnetwork models of guitar amplifiers, recreating the amplifiers' sonic response\nto arbitrary inputs at the full range of controls present on the physical\ndevice. While the focus on the paper is on the data collection pipeline, we\ndemonstrate the effectiveness of this conditioned black-box approach by\ntraining an LSTM model to the task, and comparing its performance to an offline\nwhite-box SPICE circuit simulation. Our listening test results demonstrate that\nthe neural amplifier modeling approach can match the subjective performance of\na high-quality SPICE model, all while using an automated, non-intrusive data\ncollection process, and an end-to-end trainable, real-time feasible neural\nnetwork model.\n","authors":["Lauri Juvela","Eero-Pekka Damskägg","Aleksi Peussa","Jaakko Mäkinen","Thomas Sherson","Stylianos I. Mimilakis","Athanasios Gotsopoulos"],"pdf_url":"https://arxiv.org/pdf/2403.08559v1.pdf","comment":"Presented at ICASSP 2023 - 2023 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)"},{"id":"http://arxiv.org/abs/2403.08525v1","updated":"2024-03-13T13:33:35Z","published":"2024-03-13T13:33:35Z","title":"From Weak to Strong Sound Event Labels using Adaptive Change-Point\n  Detection and Active Learning","summary":"  In this work we propose an audio recording segmentation method based on an\nadaptive change point detection (A-CPD) for machine guided weak label\nannotation of audio recording segments. The goal is to maximize the amount of\ninformation gained about the temporal activation's of the target sounds. For\neach unlabeled audio recording, we use a prediction model to derive a\nprobability curve used to guide annotation. The prediction model is initially\npre-trained on available annotated sound event data with classes that are\ndisjoint from the classes in the unlabeled dataset. The prediction model then\ngradually adapts to the annotations provided by the annotator in an active\nlearning loop. The queries used to guide the weak label annotator towards\nstrong labels are derived using change point detection on these probabilities.\nWe show that it is possible to derive strong labels of high quality even with a\nlimited annotation budget, and show favorable results for A-CPD when compared\nto two baseline query strategies.\n","authors":["John Martinsson","Olof Mogren","Maria Sandsten","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2403.08525v1.pdf","comment":"Under review at EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2308.09262v3","updated":"2024-03-13T13:15:11Z","published":"2023-08-18T02:36:21Z","title":"Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality\n  Assessment Model","summary":"  This study proposes a multi-task pseudo-label learning (MPL)-based\nnon-intrusive speech quality assessment model called MTQ-Net. MPL consists of\ntwo stages: obtaining pseudo-label scores from a pretrained model and\nperforming multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS),\nNoise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The\npretrained MOSA-Net model is utilized to estimate three pseudo labels:\nperceptual evaluation of speech quality (PESQ), short-time objective\nintelligibility (STOI), and speech distortion index (SDI). Multi-task learning\nis then employed to train MTQ-Net by combining a supervised loss (derived from\nthe difference between the estimated score and the ground-truth label) and a\nsemi-supervised loss (derived from the difference between the estimated score\nand the pseudo label), where the Huber loss is employed as the loss function.\nExperimental results first demonstrate the advantages of MPL compared to\ntraining a model from scratch and using a direct knowledge transfer mechanism.\nSecond, the benefit of the Huber loss for improving the predictive ability of\nMTQ-Net is verified. Finally, the MTQ-Net with the MPL approach exhibits higher\noverall predictive power compared to other SSL-based speech assessment models.\n","authors":["Ryandhimas E. Zezario","Bo-Ren Brian Bai","Chiou-Shann Fuh","Hsin-Min Wang","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2308.09262v3.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.11976v2","updated":"2024-03-13T08:32:51Z","published":"2023-09-21T11:21:52Z","title":"Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation\n  Using Simulated Data and a Teacher Model","summary":"  Previous methods for predicting room acoustic parameters and speech quality\nmetrics have focused on the single-channel case, where room acoustics and Mean\nOpinion Score (MOS) are predicted for a single recording device. However,\nquality-based device selection for rooms with multiple recording devices may\nbenefit from a multi-channel approach where the descriptive metrics are\npredicted for multiple devices in parallel. Following our hypothesis that a\nmodel may benefit from multi-channel training, we develop a multi-channel model\nfor joint MOS and room acoustics prediction (MOSRA) for five channels in\nparallel. The lack of multi-channel audio data with ground truth labels\nnecessitated the creation of simulated data using an acoustic simulator with\nroom acoustic labels extracted from the generated impulse responses and labels\nfor MOS generated in a student-teacher setup using a wav2vec2-based MOS\nprediction model. Our experiments show that the multi-channel model improves\nthe prediction of the direct-to-reverberation ratio, clarity, and speech\ntransmission index over the single-channel model with roughly 5$\\times$ less\ncomputation while suffering minimal losses in the performance of the other\nmetrics.\n","authors":["Jozef Coldenhoff","Andrew Harper","Paul Kendrick","Tijana Stojkovic","Milos Cernak"],"pdf_url":"https://arxiv.org/pdf/2309.11976v2.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.08187v1","updated":"2024-03-13T02:20:05Z","published":"2024-03-13T02:20:05Z","title":"Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of\n  Speech Sound Disorders in Korean children","summary":"  This study presents a model of automatic speech recognition (ASR) designed to\ndiagnose pronunciation issues in children with speech sound disorders (SSDs) to\nreplace manual transcriptions in clinical procedures. Since ASR models trained\nfor general purposes primarily predict input speech into real words, employing\na well-known high-performance ASR model for evaluating pronunciation in\nchildren with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to\nrecognize speech as pronounced rather than as existing words. The model was\nfine-tuned with a speech dataset from 137 children with inadequate speech\nproduction pronouncing 73 Korean words selected for actual clinical diagnosis.\nThe model's predictions of the pronunciations of the words matched the human\nannotations with about 90% accuracy. While the model still requires improvement\nin recognizing unclear pronunciation, this study demonstrates that ASR models\ncan streamline complex pronunciation error diagnostic procedures in clinical\nfields.\n","authors":["Taekyung Ahn","Yeonjung Hong","Younggon Im","Do Hyung Kim","Dayoung Kang","Joo Won Jeong","Jae Won Kim","Min Jung Kim","Ah-ra Cho","Dae-Hyun Jang","Hosung Nam"],"pdf_url":"https://arxiv.org/pdf/2403.08187v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.08164v1","updated":"2024-03-13T01:27:57Z","published":"2024-03-13T01:27:57Z","title":"EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight\n  Text-to-Speech","summary":"  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved\nhigh-quality speech synthesis results. Recurrent neural networks have become a\nstandard modeling technique for sequential data in TTS systems and are widely\nused. However, training a TTS model which includes RNN components requires\npowerful GPU performance and takes a long time. In contrast, CNN-based sequence\nsynthesis techniques can significantly reduce the parameters and training time\nof a TTS model while guaranteeing a certain performance due to their high\nparallelism, which alleviate these economic costs of training. In this paper,\nwe propose a lightweight TTS system based on deep convolutional neural\nnetworks, which is a two-stage training end-to-end TTS model and does not\nemploy any recurrent units. Our model consists of two stages: Text2Spectrum and\nSSRN. The former is used to encode phonemes into a coarse mel spectrogram and\nthe latter is used to synthesize the complete spectrum from the coarse mel\nspectrogram. Meanwhile, we improve the robustness of our model by a series of\ndata augmentations, such as noise suppression, time warping, frequency masking\nand time masking, for solving the low resource mongolian problem. Experiments\nshow that our model can reduce the training time and parameters while ensuring\nthe quality and naturalness of the synthesized speech compared to using\nmainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for\nvalidation, which significantly reduces training time while maintaining a\ncertain accuracy.\n","authors":["Ziqi Liang","Haoxiang Shi","Jiawei Wang","Keda Lu"],"pdf_url":"https://arxiv.org/pdf/2403.08164v1.pdf","comment":"Accepted by the 27th IEEE International Conference on Computer\n  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:\n  substantial text overlap with arXiv:2211.01948"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.06487v2","updated":"2024-03-13T00:41:36Z","published":"2024-03-11T07:50:29Z","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","summary":"  This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).\n","authors":["Koji Inoue","Bing'er Jiang","Erik Ekstedt","Tatsuya Kawahara","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2403.06487v2.pdf","comment":"This paper has been accepted for presentation at The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation (LREC-COLING 2024) and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2403.03522v2","updated":"2024-03-13T09:50:40Z","published":"2024-03-06T08:03:05Z","title":"Non-verbal information in spontaneous speech -- towards a new framework\n  of analysis","summary":"  Non-verbal signals in speech are encoded by prosody and carry information\nthat ranges from conversation action to attitude and emotion. Despite its\nimportance, the principles that govern prosodic structure are not yet\nadequately understood. This paper offers an analytical schema and a\ntechnological proof-of-concept for the categorization of prosodic signals and\ntheir association with meaning. The schema interprets surface-representations\nof multi-layered prosodic events. As a first step towards implementation, we\npresent a classification process that disentangles prosodic phenomena of three\norders. It relies on fine-tuning a pre-trained speech recognition model,\nenabling the simultaneous multi-class/multi-label detection. It generalizes\nover a large variety of spontaneous data, performing on a par with, or superior\nto, human annotation. In addition to a standardized formalization of prosody,\ndisentangling prosodic patterns can direct a theory of communication and speech\norganization. A welcome by-product is an interpretation of prosody that will\nenhance speech- and language-related technologies.\n","authors":["Tirza Biron","Moshe Barboy","Eran Ben-Artzy","Alona Golubchik","Yanir Marmor","Smadar Szekely","Yaron Winter","David Harel"],"pdf_url":"https://arxiv.org/pdf/2403.03522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08738v1","updated":"2024-03-13T17:42:03Z","published":"2024-03-13T17:42:03Z","title":"Improving Acoustic Word Embeddings through Correspondence Training of\n  Self-supervised Speech Representations","summary":"  Acoustic word embeddings (AWEs) are vector representations of spoken words.\nAn effective method for obtaining AWEs is the Correspondence Auto-Encoder\n(CAE). In the past, the CAE method has been associated with traditional MFCC\nfeatures. Representations obtained from self-supervised learning (SSL)-based\nspeech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many\ndownstream tasks. However, they have not been well studied in the context of\nlearning AWEs. This work explores the effectiveness of CAE with SSL-based\nspeech representations to obtain improved AWEs. Additionally, the capabilities\nof SSL-based speech models are explored in cross-lingual scenarios for\nobtaining AWEs. Experiments are conducted on five languages: Polish,\nPortuguese, Spanish, French, and English. HuBERT-based CAE model achieves the\nbest results for word discrimination in all languages, despite Hu-BERT being\npre-trained on English only. Also, the HuBERT-based CAE model works well in\ncross-lingual settings. It outperforms MFCC-based CAE models trained on the\ntarget languages when trained on one source language and tested on target\nlanguages.\n","authors":["Amit Meghanani","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2403.08738v1.pdf","comment":"Accepted to EACL 2024 Main Conference, Long paper"},{"id":"http://arxiv.org/abs/2403.08654v1","updated":"2024-03-13T16:08:59Z","published":"2024-03-13T16:08:59Z","title":"An Efficient End-to-End Approach to Noise Invariant Speech Features via\n  Multi-Task Learning","summary":"  Self-supervised speech representation learning enables the extraction of\nmeaningful features from raw waveforms. These features can then be efficiently\nused across multiple downstream tasks. However, two significant issues arise\nwhen considering the deployment of such methods ``in-the-wild\": (i) Their large\nsize, which can be prohibitive for edge applications; and (ii) their robustness\nto detrimental factors, such as noise and/or reverberation, that can heavily\ndegrade the performance of such systems. In this work, we propose\nRobustDistiller, a novel knowledge distillation mechanism that tackles both\nproblems jointly. Simultaneously to the distillation recipe, we apply a\nmulti-task learning objective to encourage the network to learn noise-invariant\nrepresentations by denoising the input. The proposed mechanism is evaluated on\ntwelve different downstream tasks. It outperforms several benchmarks regardless\nof noise type, or noise and reverberation levels. Experimental results show\nthat the new Student model with 23M parameters can achieve results comparable\nto the Teacher model with 95M parameters. Lastly, we show that the proposed\nrecipe can be applied to other distillation methodologies, such as the recent\nDPWavLM. For reproducibility, code and model checkpoints will be made available\nat \\mbox{\\url{https://github.com/Hguimaraes/robustdistiller}}.\n","authors":["Heitor R. Guimarães","Arthur Pimentel","Anderson R. Avila","Mehdi Rezagholizadeh","Boxing Chen","Tiago H. Falk"],"pdf_url":"https://arxiv.org/pdf/2403.08654v1.pdf","comment":"Under review on IEEE Transactions on Audio, Speech, and Language\n  Processing (2024)"},{"id":"http://arxiv.org/abs/2403.08559v1","updated":"2024-03-13T14:10:10Z","published":"2024-03-13T14:10:10Z","title":"End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier\n  Models","summary":"  This paper describes a data-driven approach to creating real-time neural\nnetwork models of guitar amplifiers, recreating the amplifiers' sonic response\nto arbitrary inputs at the full range of controls present on the physical\ndevice. While the focus on the paper is on the data collection pipeline, we\ndemonstrate the effectiveness of this conditioned black-box approach by\ntraining an LSTM model to the task, and comparing its performance to an offline\nwhite-box SPICE circuit simulation. Our listening test results demonstrate that\nthe neural amplifier modeling approach can match the subjective performance of\na high-quality SPICE model, all while using an automated, non-intrusive data\ncollection process, and an end-to-end trainable, real-time feasible neural\nnetwork model.\n","authors":["Lauri Juvela","Eero-Pekka Damskägg","Aleksi Peussa","Jaakko Mäkinen","Thomas Sherson","Stylianos I. Mimilakis","Athanasios Gotsopoulos"],"pdf_url":"https://arxiv.org/pdf/2403.08559v1.pdf","comment":"Presented at ICASSP 2023 - 2023 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)"},{"id":"http://arxiv.org/abs/2308.09262v3","updated":"2024-03-13T13:15:11Z","published":"2023-08-18T02:36:21Z","title":"Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality\n  Assessment Model","summary":"  This study proposes a multi-task pseudo-label learning (MPL)-based\nnon-intrusive speech quality assessment model called MTQ-Net. MPL consists of\ntwo stages: obtaining pseudo-label scores from a pretrained model and\nperforming multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS),\nNoise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The\npretrained MOSA-Net model is utilized to estimate three pseudo labels:\nperceptual evaluation of speech quality (PESQ), short-time objective\nintelligibility (STOI), and speech distortion index (SDI). Multi-task learning\nis then employed to train MTQ-Net by combining a supervised loss (derived from\nthe difference between the estimated score and the ground-truth label) and a\nsemi-supervised loss (derived from the difference between the estimated score\nand the pseudo label), where the Huber loss is employed as the loss function.\nExperimental results first demonstrate the advantages of MPL compared to\ntraining a model from scratch and using a direct knowledge transfer mechanism.\nSecond, the benefit of the Huber loss for improving the predictive ability of\nMTQ-Net is verified. Finally, the MTQ-Net with the MPL approach exhibits higher\noverall predictive power compared to other SSL-based speech assessment models.\n","authors":["Ryandhimas E. Zezario","Bo-Ren Brian Bai","Chiou-Shann Fuh","Hsin-Min Wang","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2308.09262v3.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.11976v2","updated":"2024-03-13T08:32:51Z","published":"2023-09-21T11:21:52Z","title":"Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation\n  Using Simulated Data and a Teacher Model","summary":"  Previous methods for predicting room acoustic parameters and speech quality\nmetrics have focused on the single-channel case, where room acoustics and Mean\nOpinion Score (MOS) are predicted for a single recording device. However,\nquality-based device selection for rooms with multiple recording devices may\nbenefit from a multi-channel approach where the descriptive metrics are\npredicted for multiple devices in parallel. Following our hypothesis that a\nmodel may benefit from multi-channel training, we develop a multi-channel model\nfor joint MOS and room acoustics prediction (MOSRA) for five channels in\nparallel. The lack of multi-channel audio data with ground truth labels\nnecessitated the creation of simulated data using an acoustic simulator with\nroom acoustic labels extracted from the generated impulse responses and labels\nfor MOS generated in a student-teacher setup using a wav2vec2-based MOS\nprediction model. Our experiments show that the multi-channel model improves\nthe prediction of the direct-to-reverberation ratio, clarity, and speech\ntransmission index over the single-channel model with roughly 5$\\times$ less\ncomputation while suffering minimal losses in the performance of the other\nmetrics.\n","authors":["Jozef Coldenhoff","Andrew Harper","Paul Kendrick","Tijana Stojkovic","Milos Cernak"],"pdf_url":"https://arxiv.org/pdf/2309.11976v2.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.08196v1","updated":"2024-03-13T02:41:53Z","published":"2024-03-13T02:41:53Z","title":"SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech\n  Recognition Evaluation","summary":"  In the wake of the surging tide of deep learning over the past decade,\nAutomatic Speech Recognition (ASR) has garnered substantial attention, leading\nto the emergence of numerous publicly accessible ASR systems that are actively\nbeing integrated into our daily lives. Nonetheless, the impartial and\nreplicable evaluation of these ASR systems encounters challenges due to various\ncrucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a\ngeneral-purpose, open-source platform designed for ASR evaluation. With this\nplatform: (i) We report a comprehensive benchmark, unveiling the current\nstate-of-the-art panorama for ASR systems, covering both open-source models and\nindustrial commercial services. (ii) We quantize how distinct nuances in the\nscoring pipeline influence the final benchmark outcomes. These include nuances\nrelated to capitalization, punctuation, interjection, contraction, synonym\nusage, compound words, etc. These issues have gained prominence in the context\nof the transition towards an End-to-End future. (iii) We propose a practical\nmodification to the conventional Token-Error-Rate (TER) evaluation metric, with\ninspirations from Kolmogorov complexity and Normalized Information Distance\n(NID). This adaptation, called modified-TER (mTER), achieves proper\nnormalization and symmetrical treatment of reference and hypothesis. By\nleveraging this platform as a large-scale testing ground, this study\ndemonstrates the robustness and backward compatibility of mTER when compared to\nTER. The SpeechColab Leaderboard is accessible at\nhttps://github.com/SpeechColab/Leaderboard\n","authors":["Jiayu Du","Jinpeng Li","Guoguo Chen","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08187v1","updated":"2024-03-13T02:20:05Z","published":"2024-03-13T02:20:05Z","title":"Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of\n  Speech Sound Disorders in Korean children","summary":"  This study presents a model of automatic speech recognition (ASR) designed to\ndiagnose pronunciation issues in children with speech sound disorders (SSDs) to\nreplace manual transcriptions in clinical procedures. Since ASR models trained\nfor general purposes primarily predict input speech into real words, employing\na well-known high-performance ASR model for evaluating pronunciation in\nchildren with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to\nrecognize speech as pronounced rather than as existing words. The model was\nfine-tuned with a speech dataset from 137 children with inadequate speech\nproduction pronouncing 73 Korean words selected for actual clinical diagnosis.\nThe model's predictions of the pronunciations of the words matched the human\nannotations with about 90% accuracy. While the model still requires improvement\nin recognizing unclear pronunciation, this study demonstrates that ASR models\ncan streamline complex pronunciation error diagnostic procedures in clinical\nfields.\n","authors":["Taekyung Ahn","Yeonjung Hong","Younggon Im","Do Hyung Kim","Dayoung Kang","Joo Won Jeong","Jae Won Kim","Min Jung Kim","Ah-ra Cho","Dae-Hyun Jang","Hosung Nam"],"pdf_url":"https://arxiv.org/pdf/2403.08187v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.08164v1","updated":"2024-03-13T01:27:57Z","published":"2024-03-13T01:27:57Z","title":"EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight\n  Text-to-Speech","summary":"  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved\nhigh-quality speech synthesis results. Recurrent neural networks have become a\nstandard modeling technique for sequential data in TTS systems and are widely\nused. However, training a TTS model which includes RNN components requires\npowerful GPU performance and takes a long time. In contrast, CNN-based sequence\nsynthesis techniques can significantly reduce the parameters and training time\nof a TTS model while guaranteeing a certain performance due to their high\nparallelism, which alleviate these economic costs of training. In this paper,\nwe propose a lightweight TTS system based on deep convolutional neural\nnetworks, which is a two-stage training end-to-end TTS model and does not\nemploy any recurrent units. Our model consists of two stages: Text2Spectrum and\nSSRN. The former is used to encode phonemes into a coarse mel spectrogram and\nthe latter is used to synthesize the complete spectrum from the coarse mel\nspectrogram. Meanwhile, we improve the robustness of our model by a series of\ndata augmentations, such as noise suppression, time warping, frequency masking\nand time masking, for solving the low resource mongolian problem. Experiments\nshow that our model can reduce the training time and parameters while ensuring\nthe quality and naturalness of the synthesized speech compared to using\nmainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for\nvalidation, which significantly reduces training time while maintaining a\ncertain accuracy.\n","authors":["Ziqi Liang","Haoxiang Shi","Jiawei Wang","Keda Lu"],"pdf_url":"https://arxiv.org/pdf/2403.08164v1.pdf","comment":"Accepted by the 27th IEEE International Conference on Computer\n  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:\n  substantial text overlap with arXiv:2211.01948"},{"id":"http://arxiv.org/abs/2403.08525v1","updated":"2024-03-13T13:33:35Z","published":"2024-03-13T13:33:35Z","title":"From Weak to Strong Sound Event Labels using Adaptive Change-Point\n  Detection and Active Learning","summary":"  In this work we propose an audio recording segmentation method based on an\nadaptive change point detection (A-CPD) for machine guided weak label\nannotation of audio recording segments. The goal is to maximize the amount of\ninformation gained about the temporal activation's of the target sounds. For\neach unlabeled audio recording, we use a prediction model to derive a\nprobability curve used to guide annotation. The prediction model is initially\npre-trained on available annotated sound event data with classes that are\ndisjoint from the classes in the unlabeled dataset. The prediction model then\ngradually adapts to the annotations provided by the annotator in an active\nlearning loop. The queries used to guide the weak label annotator towards\nstrong labels are derived using change point detection on these probabilities.\nWe show that it is possible to derive strong labels of high quality even with a\nlimited annotation budget, and show favorable results for A-CPD when compared\nto two baseline query strategies.\n","authors":["John Martinsson","Olof Mogren","Maria Sandsten","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2403.08525v1.pdf","comment":"Under review at EUSIPCO 2024"}]},"2024-03-14T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2402.07485v4","updated":"2024-03-14T13:39:45Z","published":"2024-02-12T08:51:06Z","title":"MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and\n  Instruction Tuning","summary":"  In the realm of audio-language pre-training (ALP), the challenge of achieving\ncross-modal alignment is significant. Moreover, the integration of audio inputs\nwith diverse distributions and task variations poses challenges in developing\ngeneric audio-language models. In this study, we introduce MINT, a novel ALP\nframework boosting audio-language models through multi-target pre-training and\ninstruction tuning. MINT leverages the strength of frozen pre-trained audio\nencoders and large language models (LLMs) to improve audio-language\npre-training, enabling effective transferablility to both audio-text\nunderstanding and generation tasks. To address the modality gap, we propose\nBridge-Net, a lightweight trainable module that enhances cross-modality\nalignment and the model's ability to follow instructions for a variety of\naudio-text tasks. Bridge-Net is pivotal within MINT, initially enhancing\naudio-language representation learning through a multi-target pre-training\napproach. Subsequently, Bridge-Net further boosts audio-to-language generative\nlearning by integrating a frozen language model with instruction tuning. This\nintegration empowers MINT to extract features in a flexible and effective\nmanner, specifically tailored to the provided instructions for diverse tasks.\nExperimental results demonstrate that MINT attains superior performance across\nvarious audio-language understanding and generation tasks, highlighting its\nrobust generalization capabilities even in zero-shot scenarios.\n","authors":["Hang Zhao","Yifei Xin","Zhesong Yu","Bilei Zhu","Lu Lu","Zejun Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07485v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09598v1","updated":"2024-03-14T17:39:14Z","published":"2024-03-14T17:39:14Z","title":"Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds","summary":"  Multi-label imbalanced classification poses a significant challenge in\nmachine learning, particularly evident in bioacoustics where animal sounds\noften co-occur, and certain sounds are much less frequent than others. This\npaper focuses on the specific case of classifying anuran species sounds using\nthe dataset AnuraSet, that contains both class imbalance and multi-label\nexamples. To address these challenges, we introduce Mixture of Mixups (Mix2), a\nframework that leverages mixing regularization methods Mixup, Manifold Mixup,\nand MultiMix. Experimental results show that these methods, individually, may\nlead to suboptimal results; however, when applied randomly, with one selected\nat each training iteration, they prove effective in addressing the mentioned\nchallenges, particularly for rare classes with few occurrences. Further\nanalysis reveals that Mix2 is also proficient in classifying sounds across\nvarious levels of class co-occurrences.\n","authors":["Ilyass Moummad","Nicolas Farrugia","Romain Serizel","Jeremy Froidevaux","Vincent Lostanlen"],"pdf_url":"https://arxiv.org/pdf/2403.09598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09579v1","updated":"2024-03-14T17:13:37Z","published":"2024-03-14T17:13:37Z","title":"uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with\n  Unsupervised Audio Mixtures","summary":"  Masked Autoencoders (MAEs) learn rich low-level representations from\nunlabeled data but require substantial labeled data to effectively adapt to\ndownstream tasks. Conversely, Instance Discrimination (ID) emphasizes\nhigh-level semantics, offering a potential solution to alleviate annotation\nrequirements in MAEs. Although combining these two approaches can address\ndownstream tasks with limited labeled data, naively integrating ID into MAEs\nleads to extended training times and high computational costs. To address this\nchallenge, we introduce uaMix-MAE, an efficient ID tuning strategy that\nleverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE\naligns the representations of pretrained MAEs, thereby facilitating effective\nadaptation to task-specific semantics. To optimize the model with small amounts\nof unlabeled data, we propose an audio mixing technique that manipulates audio\nsamples in both input and virtual label spaces. Experiments in low/few-shot\nsettings demonstrate that \\modelname achieves 4-6% accuracy improvements over\nvarious benchmarks when tuned with limited unlabeled data, such as\nAudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE\n","authors":["Afrina Tabassum","Dung Tran","Trung Dang","Ismini Lourentzou","Kazuhito Koishida"],"pdf_url":"https://arxiv.org/pdf/2403.09579v1.pdf","comment":"5 pages, 6 figures, 4 tables. To appear in ICASSP'2024"},{"id":"http://arxiv.org/abs/2403.09455v1","updated":"2024-03-14T14:53:54Z","published":"2024-03-14T14:53:54Z","title":"The Neural-SRP method for positional sound source localization","summary":"  Steered Response Power (SRP) is a widely used method for the task of sound\nsource localization using microphone arrays, showing satisfactory localization\nperformance on many practical scenarios. However, its performance is diminished\nunder highly reverberant environments. Although Deep Neural Networks (DNNs)\nhave been previously proposed to overcome this limitation, most are trained for\na specific number of microphones with fixed spatial coordinates. This restricts\ntheir practical application on scenarios frequently observed in wireless\nacoustic sensor networks, where each application has an ad-hoc microphone\ntopology. We propose Neural-SRP, a DNN which combines the flexibility of SRP\nwith the performance gains of DNNs. We train our network using simulated data\nand transfer learning, and evaluate our approach on recorded and simulated\ndata. Results verify that Neural-SRP's localization performance significantly\noutperforms the baselines.\n","authors":["Eric Grinstein","Toon van Waterschoot","Mike Brookes","Patrick A. Naylor"],"pdf_url":"https://arxiv.org/pdf/2403.09455v1.pdf","comment":"Presented at Asilomar Conference on Signals, Systems, and Computers"},{"id":"http://arxiv.org/abs/2403.09451v1","updated":"2024-03-14T14:49:40Z","published":"2024-03-14T14:49:40Z","title":"M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in\n  Cognitive Load Assessment","summary":"  This paper introduces the M&M model, a novel multimodal-multitask learning\nframework, applied to the AVCAffe dataset for cognitive load assessment (CLA).\nM&M uniquely integrates audiovisual cues through a dual-pathway architecture,\nfeaturing specialized streams for audio and video inputs. A key innovation lies\nin its cross-modality multihead attention mechanism, fusing the different\nmodalities for synchronized multitasking. Another notable feature is the\nmodel's three specialized branches, each tailored to a specific cognitive load\nlabel, enabling nuanced, task-specific analysis. While it shows modest\nperformance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a\npromising framework for integrated multimodal processing. This work paves the\nway for future enhancements in multimodal-multitask learning systems,\nemphasizing the fusion of diverse data types for complex task handling.\n","authors":["Long Nguyen-Phuoc","Renald Gaboriau","Dimitri Delacroix","Laurent Navarro"],"pdf_url":"https://arxiv.org/pdf/2403.09451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09407v1","updated":"2024-03-14T13:59:04Z","published":"2024-03-14T13:59:04Z","title":"LM2D: Lyrics- and Music-Driven Dance Synthesis","summary":"  Dance typically involves professional choreography with complex movements\nthat follow a musical rhythm and can also be influenced by lyrical content. The\nintegration of lyrics in addition to the auditory dimension, enriches the\nfoundational tone and makes motion generation more amenable to its semantic\nmeanings. However, existing dance synthesis methods tend to model motions only\nconditioned on audio signals. In this work, we make two contributions to bridge\nthis gap. First, we propose LM2D, a novel probabilistic architecture that\nincorporates a multimodal diffusion model with consistency distillation,\ndesigned to create dance conditioned on both music and lyrics in one diffusion\ngeneration step. Second, we introduce the first 3D dance-motion dataset that\nencompasses both music and lyrics, obtained with pose estimation technologies.\nWe evaluate our model against music-only baseline models with objective metrics\nand human evaluations, including dancers and choreographers. The results\ndemonstrate LM2D is able to produce realistic and diverse dance matching both\nlyrics and music. A video summary can be accessed at:\nhttps://youtu.be/4XCgvYookvA.\n","authors":["Wenjie Yin","Xuejiao Zhao","Yi Yu","Hang Yin","Danica Kragic","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2403.09407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09321v1","updated":"2024-03-14T12:10:47Z","published":"2024-03-14T12:10:47Z","title":"A Practical Guide to Spectrogram Analysis for Audio Signal Processing","summary":"  The paper summarizes spectrogram and gives practical application of\nspectrogram in signal processing. For analysis, finger-snapping is recorded\nwith a sampling rate of 441000 Hz and 96000 Hz. The effects of the number of\nsegments on the Power Spectral Density (PSD) and spectrogram are analyzed and\nvisualized.\n","authors":["Zulfidin Khodzhaev"],"pdf_url":"https://arxiv.org/pdf/2403.09321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09298v1","updated":"2024-03-14T11:37:02Z","published":"2024-03-14T11:37:02Z","title":"More than words: Advancements and challenges in speech recognition for\n  singing","summary":"  This paper addresses the challenges and advancements in speech recognition\nfor singing, a domain distinctly different from standard speech recognition.\nSinging encompasses unique challenges, including extensive pitch variations,\ndiverse vocal styles, and background music interference. We explore key areas\nsuch as phoneme recognition, language identification in songs, keyword\nspotting, and full lyrics transcription. I will describe some of my own\nexperiences when performing research on these tasks just as they were starting\nto gain traction, but will also show how recent developments in deep learning\nand large-scale datasets have propelled progress in this field. My goal is to\nilluminate the complexities of applying speech recognition to singing, evaluate\ncurrent capabilities, and outline future research directions.\n","authors":["Anna Kruspe"],"pdf_url":"https://arxiv.org/pdf/2403.09298v1.pdf","comment":"Conference on Electronic Speech Signal Processing (ESSV) 2024,\n  Keynote"},{"id":"http://arxiv.org/abs/2310.15399v3","updated":"2024-03-14T02:14:04Z","published":"2023-10-23T23:01:33Z","title":"GESI: Gammachirp Envelope Similarity Index for Predicting\n  Intelligibility of Simulated Hearing Loss Sounds","summary":"  We propose an objective intelligibility measure (OIM), called the Gammachirp\nEnvelope Similarity Index (GESI), which can predict the speech intelligibility\n(SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners.\nGESI is an intrusive method that computes the SI metric using the gammachirp\nfilterbank (GCFB), the modulation filterbank, and the extended cosine\nsimilarity measure. The unique features of GESI are that i) it reflects the\nhearing impaired (HI) listener's HL that appears in the audiogram and is caused\nby active and passive cochlear dysfunction, ii) it provides a single goodness\nmetric, as in the widely used STOI and ESTOI, that can be used immediately to\nevaluate SE algorithms, and iii) it provides a simple control parameter to\naccept the level asymmetry of the reference and test sounds and to deal with\nindividual listening conditions and environments. We evaluated GESI and the\nconventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using\nfour SI experiments on words of male and female speech sounds in both\nlaboratory and remote environments. GESI was shown to outperform the other OIMs\nin the evaluations. GESI could be used to improve SE algorithms in assistive\nlistening devices for individual HI listeners.\n","authors":["Ayako Yamamoto","Toshio Irino","Fuki Miyazaki","Honoka Tamaru"],"pdf_url":"https://arxiv.org/pdf/2310.15399v3.pdf","comment":"This paper was submitted to JASA on March 14, 2024"},{"id":"http://arxiv.org/abs/2403.09030v1","updated":"2024-03-14T01:46:30Z","published":"2024-03-14T01:46:30Z","title":"An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from\n  Acoustic Signals","summary":"  This study aimed to develop a deep learning model for the classification of\nbearing faults in wind turbine generators from acoustic signals. A\nconvolutional LSTM model was successfully constructed and trained by using\naudio data from five predefined fault types for both training and validation.\nTo create the dataset, raw audio signal data was collected and processed in\nframes to capture time and frequency domain information. The model exhibited\noutstanding accuracy on training samples and demonstrated excellent\ngeneralization ability during validation, indicating its proficiency of\ngeneralization capability. On the test samples, the model achieved remarkable\nclassification performance, with an overall accuracy exceeding 99.5%, and a\nfalse positive rate of less than 1% for normal status. The findings of this\nstudy provide essential support for the diagnosis and maintenance of bearing\nfaults in wind turbine generators, with the potential to enhance the\nreliability and efficiency of wind power generation.\n","authors":["Zhao Wang","Xiaomeng Li","Na Li","Longlong Shu"],"pdf_url":"https://arxiv.org/pdf/2403.09030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06487v3","updated":"2024-03-14T23:59:59Z","published":"2024-03-11T07:50:29Z","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","summary":"  This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).\n","authors":["Koji Inoue","Bing'er Jiang","Erik Ekstedt","Tatsuya Kawahara","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2403.06487v3.pdf","comment":"This paper has been accepted for presentation at The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation (LREC-COLING 2024) and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2402.14982v2","updated":"2024-03-14T22:43:13Z","published":"2024-02-22T21:44:58Z","title":"Human Brain Exhibits Distinct Patterns When Listening to Fake Versus\n  Real Audio: Preliminary Evidence","summary":"  In this paper we study the variations in human brain activity when listening\nto real and fake audio. Our preliminary results suggest that the\nrepresentations learned by a state-of-the-art deepfake audio detection\nalgorithm, do not exhibit clear distinct patterns between real and fake audio.\nIn contrast, human brain activity, as measured by EEG, displays distinct\npatterns when individuals are exposed to fake versus real audio. This\npreliminary evidence enables future research directions in areas such as\ndeepfake audio detection.\n","authors":["Mahsa Salehi","Kalin Stefanov","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2402.14982v2.pdf","comment":"9 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.09789v1","updated":"2024-03-14T18:19:29Z","published":"2024-03-14T18:19:29Z","title":"Audiosockets: A Python socket package for Real-Time Audio Processing","summary":"  There are many packages in Python which allow one to perform real-time\nprocessing on audio data. Unfortunately, due to the synchronous nature of the\nlanguage, there lacks a framework which allows for distributed parallel\nprocessing of the data without requiring a large programming overhead and in\nwhich the data acquisition is not blocked by subsequent processing operations.\nThis work improves on packages used for audio data collection with a\nlight-weight backend and a simple interface that allows for distributed\nprocessing through a socket-based structure. This is intended for real-time\naudio machine learning and data processing in Python with a quick deployment of\nmultiple parallel operations on the same data, allowing users to spend less\ntime debugging and more time developing.\n","authors":["Nicolas Shu","David V. Anderson"],"pdf_url":"https://arxiv.org/pdf/2403.09789v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.10565v1","updated":"2024-03-14T14:57:16Z","published":"2024-03-14T14:57:16Z","title":"PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux\n  pour la détection du trouble de stress post-traumatique","summary":"  In order to provide a more objective and quicker way to diagnose\npost-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two\nunimodal convolutional neural networks and which gives low detection error\nrate. By taking only videos and audios as inputs, the model could be used in\nthe configuration of teleconsultation sessions, in the optimization of patient\njourneys or for human-robot interaction.\n","authors":["Long Nguyen-Phuoc","Renald Gaboriau","Dimitri Delacroix","Laurent Navarro"],"pdf_url":"https://arxiv.org/pdf/2403.10565v1.pdf","comment":"in French language. GRETSI 2023"},{"id":"http://arxiv.org/abs/2403.09753v1","updated":"2024-03-14T12:07:37Z","published":"2024-03-14T12:07:37Z","title":"SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification\n  of Spoken Numbers in Different Languages","summary":"  Benchmarking plays a pivotal role in assessing and enhancing the performance\nof compact deep learning models designed for execution on resource-constrained\ndevices, such as microcontrollers. Our study introduces a novel, entirely\nartificially generated benchmarking dataset tailored for speech recognition,\nrepresenting a core challenge in the field of tiny deep learning. SpokeN-100\nconsists of spoken numbers from 0 to 99 spoken by 32 different speakers in four\ndifferent languages, namely English, Mandarin, German and French, resulting in\n12,800 audio samples. We determine auditory features and use UMAP (Uniform\nManifold Approximation and Projection for Dimension Reduction) as a\ndimensionality reduction method to show the diversity and richness of the\ndataset. To highlight the use case of the dataset, we introduce two benchmark\ntasks: given an audio sample, classify (i) the used language and/or (ii) the\nspoken number. We optimized state-of-the-art deep neural networks and performed\nan evolutionary neural architecture search to find tiny architectures optimized\nfor the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent\nthe first benchmark data achieved for SpokeN-100.\n","authors":["René Groh","Nina Goes","Andreas M. Kist"],"pdf_url":"https://arxiv.org/pdf/2403.09753v1.pdf","comment":"Accepted as a full paper by the tinyML Research Symposium 2024"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2402.07485v4","updated":"2024-03-14T13:39:45Z","published":"2024-02-12T08:51:06Z","title":"MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and\n  Instruction Tuning","summary":"  In the realm of audio-language pre-training (ALP), the challenge of achieving\ncross-modal alignment is significant. Moreover, the integration of audio inputs\nwith diverse distributions and task variations poses challenges in developing\ngeneric audio-language models. In this study, we introduce MINT, a novel ALP\nframework boosting audio-language models through multi-target pre-training and\ninstruction tuning. MINT leverages the strength of frozen pre-trained audio\nencoders and large language models (LLMs) to improve audio-language\npre-training, enabling effective transferablility to both audio-text\nunderstanding and generation tasks. To address the modality gap, we propose\nBridge-Net, a lightweight trainable module that enhances cross-modality\nalignment and the model's ability to follow instructions for a variety of\naudio-text tasks. Bridge-Net is pivotal within MINT, initially enhancing\naudio-language representation learning through a multi-target pre-training\napproach. Subsequently, Bridge-Net further boosts audio-to-language generative\nlearning by integrating a frozen language model with instruction tuning. This\nintegration empowers MINT to extract features in a flexible and effective\nmanner, specifically tailored to the provided instructions for diverse tasks.\nExperimental results demonstrate that MINT attains superior performance across\nvarious audio-language understanding and generation tasks, highlighting its\nrobust generalization capabilities even in zero-shot scenarios.\n","authors":["Hang Zhao","Yifei Xin","Zhesong Yu","Bilei Zhu","Lu Lu","Zejun Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07485v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09598v1","updated":"2024-03-14T17:39:14Z","published":"2024-03-14T17:39:14Z","title":"Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds","summary":"  Multi-label imbalanced classification poses a significant challenge in\nmachine learning, particularly evident in bioacoustics where animal sounds\noften co-occur, and certain sounds are much less frequent than others. This\npaper focuses on the specific case of classifying anuran species sounds using\nthe dataset AnuraSet, that contains both class imbalance and multi-label\nexamples. To address these challenges, we introduce Mixture of Mixups (Mix2), a\nframework that leverages mixing regularization methods Mixup, Manifold Mixup,\nand MultiMix. Experimental results show that these methods, individually, may\nlead to suboptimal results; however, when applied randomly, with one selected\nat each training iteration, they prove effective in addressing the mentioned\nchallenges, particularly for rare classes with few occurrences. Further\nanalysis reveals that Mix2 is also proficient in classifying sounds across\nvarious levels of class co-occurrences.\n","authors":["Ilyass Moummad","Nicolas Farrugia","Romain Serizel","Jeremy Froidevaux","Vincent Lostanlen"],"pdf_url":"https://arxiv.org/pdf/2403.09598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09579v1","updated":"2024-03-14T17:13:37Z","published":"2024-03-14T17:13:37Z","title":"uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with\n  Unsupervised Audio Mixtures","summary":"  Masked Autoencoders (MAEs) learn rich low-level representations from\nunlabeled data but require substantial labeled data to effectively adapt to\ndownstream tasks. Conversely, Instance Discrimination (ID) emphasizes\nhigh-level semantics, offering a potential solution to alleviate annotation\nrequirements in MAEs. Although combining these two approaches can address\ndownstream tasks with limited labeled data, naively integrating ID into MAEs\nleads to extended training times and high computational costs. To address this\nchallenge, we introduce uaMix-MAE, an efficient ID tuning strategy that\nleverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE\naligns the representations of pretrained MAEs, thereby facilitating effective\nadaptation to task-specific semantics. To optimize the model with small amounts\nof unlabeled data, we propose an audio mixing technique that manipulates audio\nsamples in both input and virtual label spaces. Experiments in low/few-shot\nsettings demonstrate that \\modelname achieves 4-6% accuracy improvements over\nvarious benchmarks when tuned with limited unlabeled data, such as\nAudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE\n","authors":["Afrina Tabassum","Dung Tran","Trung Dang","Ismini Lourentzou","Kazuhito Koishida"],"pdf_url":"https://arxiv.org/pdf/2403.09579v1.pdf","comment":"5 pages, 6 figures, 4 tables. To appear in ICASSP'2024"},{"id":"http://arxiv.org/abs/2403.09527v1","updated":"2024-03-14T16:10:34Z","published":"2024-03-14T16:10:34Z","title":"WavCraft: Audio Editing and Generation with Natural Language Prompts","summary":"  We introduce WavCraft, a collective system that leverages large language\nmodels (LLMs) to connect diverse task-specific models for audio content\ncreation and editing. Specifically, WavCraft describes the content of raw sound\nmaterials in natural language and prompts the LLM conditioned on audio\ndescriptions and users' requests. WavCraft leverages the in-context learning\nability of the LLM to decomposes users' instructions into several tasks and\ntackle each task collaboratively with audio expert modules. Through task\ndecomposition along with a set of task-specific models, WavCraft follows the\ninput instruction to create or edit audio content with more details and\nrationales, facilitating users' control. In addition, WavCraft is able to\ncooperate with users via dialogue interaction and even produce the audio\ncontent without explicit user commands. Experiments demonstrate that WavCraft\nyields a better performance than existing methods, especially when adjusting\nthe local regions of audio clips. Moreover, WavCraft can follow complex\ninstructions to edit and even create audio content on the top of input\nrecordings, facilitating audio producers in a broader range of applications.\nOur implementation and demos are available at\nhttps://github.com/JinhuaLiang/WavCraft.\n","authors":["Jinhua Liang","Huan Zhang","Haohe Liu","Yin Cao","Qiuqiang Kong","Xubo Liu","Wenwu Wang","Mark D. Plumbley","Huy Phan","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2403.09527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09524v1","updated":"2024-03-14T16:10:17Z","published":"2024-03-14T16:10:17Z","title":"Physics-Informed Neural Network for Volumetric Sound field\n  Reconstruction of Speech Signals","summary":"  Recent developments in acoustic signal processing have seen the integration\nof deep learning methodologies, alongside the continued prominence of classical\nwave expansion-based approaches, particularly in sound field reconstruction.\nPhysics-Informed Neural Networks (PINNs) have emerged as a novel framework,\nbridging the gap between data-driven and model-based techniques for addressing\nphysical phenomena governed by partial differential equations. This paper\nintroduces a PINN-based approach for the recovery of arbitrary volumetric\nacoustic fields. The network incorporates the wave equation to impose a\nregularization on signal reconstruction in the time domain. This methodology\nenables the network to learn the underlying physics of sound propagation and\nallows for the complete characterization of the sound field based on a limited\nset of observations. The proposed method's efficacy is validated through\nexperiments involving speech signals in a real-world environment, considering\nvarying numbers of available measurements. Moreover, a comparative analysis is\nundertaken against state-of-the-art frequency-domain and time-domain\nreconstruction methods from existing literature, highlighting the increased\naccuracy across the various measurement configurations.\n","authors":["Marco Olivieri","Xenofon Karakonstantis","Mirco Pezzoli","Fabio Antonacci","Augusto Sarti","Efren Fernandez-Grande"],"pdf_url":"https://arxiv.org/pdf/2403.09524v1.pdf","comment":"Submitted to EURASIP Journal on Audio, Speech, and Music Processing"},{"id":"http://arxiv.org/abs/2403.09455v1","updated":"2024-03-14T14:53:54Z","published":"2024-03-14T14:53:54Z","title":"The Neural-SRP method for positional sound source localization","summary":"  Steered Response Power (SRP) is a widely used method for the task of sound\nsource localization using microphone arrays, showing satisfactory localization\nperformance on many practical scenarios. However, its performance is diminished\nunder highly reverberant environments. Although Deep Neural Networks (DNNs)\nhave been previously proposed to overcome this limitation, most are trained for\na specific number of microphones with fixed spatial coordinates. This restricts\ntheir practical application on scenarios frequently observed in wireless\nacoustic sensor networks, where each application has an ad-hoc microphone\ntopology. We propose Neural-SRP, a DNN which combines the flexibility of SRP\nwith the performance gains of DNNs. We train our network using simulated data\nand transfer learning, and evaluate our approach on recorded and simulated\ndata. Results verify that Neural-SRP's localization performance significantly\noutperforms the baselines.\n","authors":["Eric Grinstein","Toon van Waterschoot","Mike Brookes","Patrick A. Naylor"],"pdf_url":"https://arxiv.org/pdf/2403.09455v1.pdf","comment":"Presented at Asilomar Conference on Signals, Systems, and Computers"},{"id":"http://arxiv.org/abs/2403.09451v1","updated":"2024-03-14T14:49:40Z","published":"2024-03-14T14:49:40Z","title":"M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in\n  Cognitive Load Assessment","summary":"  This paper introduces the M&M model, a novel multimodal-multitask learning\nframework, applied to the AVCAffe dataset for cognitive load assessment (CLA).\nM&M uniquely integrates audiovisual cues through a dual-pathway architecture,\nfeaturing specialized streams for audio and video inputs. A key innovation lies\nin its cross-modality multihead attention mechanism, fusing the different\nmodalities for synchronized multitasking. Another notable feature is the\nmodel's three specialized branches, each tailored to a specific cognitive load\nlabel, enabling nuanced, task-specific analysis. While it shows modest\nperformance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a\npromising framework for integrated multimodal processing. This work paves the\nway for future enhancements in multimodal-multitask learning systems,\nemphasizing the fusion of diverse data types for complex task handling.\n","authors":["Long Nguyen-Phuoc","Renald Gaboriau","Dimitri Delacroix","Laurent Navarro"],"pdf_url":"https://arxiv.org/pdf/2403.09451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09407v1","updated":"2024-03-14T13:59:04Z","published":"2024-03-14T13:59:04Z","title":"LM2D: Lyrics- and Music-Driven Dance Synthesis","summary":"  Dance typically involves professional choreography with complex movements\nthat follow a musical rhythm and can also be influenced by lyrical content. The\nintegration of lyrics in addition to the auditory dimension, enriches the\nfoundational tone and makes motion generation more amenable to its semantic\nmeanings. However, existing dance synthesis methods tend to model motions only\nconditioned on audio signals. In this work, we make two contributions to bridge\nthis gap. First, we propose LM2D, a novel probabilistic architecture that\nincorporates a multimodal diffusion model with consistency distillation,\ndesigned to create dance conditioned on both music and lyrics in one diffusion\ngeneration step. Second, we introduce the first 3D dance-motion dataset that\nencompasses both music and lyrics, obtained with pose estimation technologies.\nWe evaluate our model against music-only baseline models with objective metrics\nand human evaluations, including dancers and choreographers. The results\ndemonstrate LM2D is able to produce realistic and diverse dance matching both\nlyrics and music. A video summary can be accessed at:\nhttps://youtu.be/4XCgvYookvA.\n","authors":["Wenjie Yin","Xuejiao Zhao","Yi Yu","Hang Yin","Danica Kragic","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2403.09407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09321v1","updated":"2024-03-14T12:10:47Z","published":"2024-03-14T12:10:47Z","title":"A Practical Guide to Spectrogram Analysis for Audio Signal Processing","summary":"  The paper summarizes spectrogram and gives practical application of\nspectrogram in signal processing. For analysis, finger-snapping is recorded\nwith a sampling rate of 441000 Hz and 96000 Hz. The effects of the number of\nsegments on the Power Spectral Density (PSD) and spectrogram are analyzed and\nvisualized.\n","authors":["Zulfidin Khodzhaev"],"pdf_url":"https://arxiv.org/pdf/2403.09321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09298v1","updated":"2024-03-14T11:37:02Z","published":"2024-03-14T11:37:02Z","title":"More than words: Advancements and challenges in speech recognition for\n  singing","summary":"  This paper addresses the challenges and advancements in speech recognition\nfor singing, a domain distinctly different from standard speech recognition.\nSinging encompasses unique challenges, including extensive pitch variations,\ndiverse vocal styles, and background music interference. We explore key areas\nsuch as phoneme recognition, language identification in songs, keyword\nspotting, and full lyrics transcription. I will describe some of my own\nexperiences when performing research on these tasks just as they were starting\nto gain traction, but will also show how recent developments in deep learning\nand large-scale datasets have propelled progress in this field. My goal is to\nilluminate the complexities of applying speech recognition to singing, evaluate\ncurrent capabilities, and outline future research directions.\n","authors":["Anna Kruspe"],"pdf_url":"https://arxiv.org/pdf/2403.09298v1.pdf","comment":"Conference on Electronic Speech Signal Processing (ESSV) 2024,\n  Keynote"},{"id":"http://arxiv.org/abs/2310.15399v3","updated":"2024-03-14T02:14:04Z","published":"2023-10-23T23:01:33Z","title":"GESI: Gammachirp Envelope Similarity Index for Predicting\n  Intelligibility of Simulated Hearing Loss Sounds","summary":"  We propose an objective intelligibility measure (OIM), called the Gammachirp\nEnvelope Similarity Index (GESI), which can predict the speech intelligibility\n(SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners.\nGESI is an intrusive method that computes the SI metric using the gammachirp\nfilterbank (GCFB), the modulation filterbank, and the extended cosine\nsimilarity measure. The unique features of GESI are that i) it reflects the\nhearing impaired (HI) listener's HL that appears in the audiogram and is caused\nby active and passive cochlear dysfunction, ii) it provides a single goodness\nmetric, as in the widely used STOI and ESTOI, that can be used immediately to\nevaluate SE algorithms, and iii) it provides a simple control parameter to\naccept the level asymmetry of the reference and test sounds and to deal with\nindividual listening conditions and environments. We evaluated GESI and the\nconventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using\nfour SI experiments on words of male and female speech sounds in both\nlaboratory and remote environments. GESI was shown to outperform the other OIMs\nin the evaluations. GESI could be used to improve SE algorithms in assistive\nlistening devices for individual HI listeners.\n","authors":["Ayako Yamamoto","Toshio Irino","Fuki Miyazaki","Honoka Tamaru"],"pdf_url":"https://arxiv.org/pdf/2310.15399v3.pdf","comment":"This paper was submitted to JASA on March 14, 2024"},{"id":"http://arxiv.org/abs/2403.09030v1","updated":"2024-03-14T01:46:30Z","published":"2024-03-14T01:46:30Z","title":"An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from\n  Acoustic Signals","summary":"  This study aimed to develop a deep learning model for the classification of\nbearing faults in wind turbine generators from acoustic signals. A\nconvolutional LSTM model was successfully constructed and trained by using\naudio data from five predefined fault types for both training and validation.\nTo create the dataset, raw audio signal data was collected and processed in\nframes to capture time and frequency domain information. The model exhibited\noutstanding accuracy on training samples and demonstrated excellent\ngeneralization ability during validation, indicating its proficiency of\ngeneralization capability. On the test samples, the model achieved remarkable\nclassification performance, with an overall accuracy exceeding 99.5%, and a\nfalse positive rate of less than 1% for normal status. The findings of this\nstudy provide essential support for the diagnosis and maintenance of bearing\nfaults in wind turbine generators, with the potential to enhance the\nreliability and efficiency of wind power generation.\n","authors":["Zhao Wang","Xiaomeng Li","Na Li","Longlong Shu"],"pdf_url":"https://arxiv.org/pdf/2403.09030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06487v3","updated":"2024-03-14T23:59:59Z","published":"2024-03-11T07:50:29Z","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","summary":"  This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).\n","authors":["Koji Inoue","Bing'er Jiang","Erik Ekstedt","Tatsuya Kawahara","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2403.06487v3.pdf","comment":"This paper has been accepted for presentation at The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation (LREC-COLING 2024) and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2402.14982v2","updated":"2024-03-14T22:43:13Z","published":"2024-02-22T21:44:58Z","title":"Human Brain Exhibits Distinct Patterns When Listening to Fake Versus\n  Real Audio: Preliminary Evidence","summary":"  In this paper we study the variations in human brain activity when listening\nto real and fake audio. Our preliminary results suggest that the\nrepresentations learned by a state-of-the-art deepfake audio detection\nalgorithm, do not exhibit clear distinct patterns between real and fake audio.\nIn contrast, human brain activity, as measured by EEG, displays distinct\npatterns when individuals are exposed to fake versus real audio. This\npreliminary evidence enables future research directions in areas such as\ndeepfake audio detection.\n","authors":["Mahsa Salehi","Kalin Stefanov","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2402.14982v2.pdf","comment":"9 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.09789v1","updated":"2024-03-14T18:19:29Z","published":"2024-03-14T18:19:29Z","title":"Audiosockets: A Python socket package for Real-Time Audio Processing","summary":"  There are many packages in Python which allow one to perform real-time\nprocessing on audio data. Unfortunately, due to the synchronous nature of the\nlanguage, there lacks a framework which allows for distributed parallel\nprocessing of the data without requiring a large programming overhead and in\nwhich the data acquisition is not blocked by subsequent processing operations.\nThis work improves on packages used for audio data collection with a\nlight-weight backend and a simple interface that allows for distributed\nprocessing through a socket-based structure. This is intended for real-time\naudio machine learning and data processing in Python with a quick deployment of\nmultiple parallel operations on the same data, allowing users to spend less\ntime debugging and more time developing.\n","authors":["Nicolas Shu","David V. Anderson"],"pdf_url":"https://arxiv.org/pdf/2403.09789v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.10565v1","updated":"2024-03-14T14:57:16Z","published":"2024-03-14T14:57:16Z","title":"PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux\n  pour la détection du trouble de stress post-traumatique","summary":"  In order to provide a more objective and quicker way to diagnose\npost-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two\nunimodal convolutional neural networks and which gives low detection error\nrate. By taking only videos and audios as inputs, the model could be used in\nthe configuration of teleconsultation sessions, in the optimization of patient\njourneys or for human-robot interaction.\n","authors":["Long Nguyen-Phuoc","Renald Gaboriau","Dimitri Delacroix","Laurent Navarro"],"pdf_url":"https://arxiv.org/pdf/2403.10565v1.pdf","comment":"in French language. GRETSI 2023"},{"id":"http://arxiv.org/abs/2403.09753v1","updated":"2024-03-14T12:07:37Z","published":"2024-03-14T12:07:37Z","title":"SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification\n  of Spoken Numbers in Different Languages","summary":"  Benchmarking plays a pivotal role in assessing and enhancing the performance\nof compact deep learning models designed for execution on resource-constrained\ndevices, such as microcontrollers. Our study introduces a novel, entirely\nartificially generated benchmarking dataset tailored for speech recognition,\nrepresenting a core challenge in the field of tiny deep learning. SpokeN-100\nconsists of spoken numbers from 0 to 99 spoken by 32 different speakers in four\ndifferent languages, namely English, Mandarin, German and French, resulting in\n12,800 audio samples. We determine auditory features and use UMAP (Uniform\nManifold Approximation and Projection for Dimension Reduction) as a\ndimensionality reduction method to show the diversity and richness of the\ndataset. To highlight the use case of the dataset, we introduce two benchmark\ntasks: given an audio sample, classify (i) the used language and/or (ii) the\nspoken number. We optimized state-of-the-art deep neural networks and performed\nan evolutionary neural architecture search to find tiny architectures optimized\nfor the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent\nthe first benchmark data achieved for SpokeN-100.\n","authors":["René Groh","Nina Goes","Andreas M. Kist"],"pdf_url":"https://arxiv.org/pdf/2403.09753v1.pdf","comment":"Accepted as a full paper by the tinyML Research Symposium 2024"}]},"2024-03-15T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.07661v2","updated":"2024-03-15T13:21:31Z","published":"2024-03-12T13:50:58Z","title":"Gender-ambiguous voice generation through feminine speaking style\n  transfer in male voices","summary":"  Recently, and under the umbrella of Responsible AI, efforts have been made to\ndevelop gender-ambiguous synthetic speech to represent with a single voice all\nindividuals in the gender spectrum. However, research efforts have completely\noverlooked the speaking style despite differences found among binary and\nnon-binary populations. In this work, we synthesise gender-ambiguous speech by\ncombining the timbre of a male speaker with the manner of speech of a female\nspeaker using voice morphing and pitch shifting towards the male-female\nboundary. Subjective evaluations indicate that the ambiguity of the morphed\nsamples that convey the female speech style is higher than those that undergo\nplain pitch transformations suggesting that the speaking style can be a\ncontributing factor in creating gender-ambiguous speech. To our knowledge, this\nis the first study that explicitly uses the transfer of the speaking style to\ncreate gender-ambiguous voices.\n","authors":["Maria Koutsogiannaki","Shafel Mc Dowall","Ioannis Agiomyrgiannakis"],"pdf_url":"https://arxiv.org/pdf/2403.07661v2.pdf","comment":"submitted to Interspeech"},{"id":"http://arxiv.org/abs/2309.08005v2","updated":"2024-03-15T20:21:36Z","published":"2023-09-14T19:29:12Z","title":"Efficient Face Detection with Audio-Based Region Proposals for\n  Human-Robot Interactions","summary":"  Efficient face detection is critical to provide natural human-robot\ninteractions. However, computer vision tends to involve a large computational\nload due to the amount of data (i.e. pixels) that needs to be processed in a\nshort amount of time. This is undesirable on robotics platforms where multiple\nprocesses need to run in parallel and where the processing power is limited by\nportability constraints. Existing solutions often involve reducing image\nquality which can negatively impact processing. The literature also reports\nmethods to generate regions of interest in images from pixel data. Although it\nis a promising idea, these methods often involve heavy vision algorithms. In\nthis paper, we evaluate how audio can be used to generate regions of interest\nin optical images to reduce the number of pixels to process with computer\nvision. Thereby, we propose a unique attention mechanism to localize a speech\nsource and evaluate its impact on an existing face detection algorithm. Our\nresults show that the attention mechanism reduces the computational load and\noffers an interesting trade-off between speed and accuracy. The proposed\npipeline is flexible and can be easily adapted to other applications such as\nrobot surveillance, video conferences or smart glasses.\n","authors":["William Aris","François Grondin"],"pdf_url":"https://arxiv.org/pdf/2309.08005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10518v1","updated":"2024-03-15T17:59:33Z","published":"2024-03-15T17:59:33Z","title":"Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation\n  Guided by the Characteristic Dance Primitives","summary":"  We propose Lodge, a network capable of generating extremely long dance\nsequences conditioned on given music. We design Lodge as a two-stage coarse to\nfine diffusion architecture, and propose the characteristic dance primitives\nthat possess significant expressiveness as intermediate representations between\ntwo diffusion models. The first stage is global diffusion, which focuses on\ncomprehending the coarse-level music-dance correlation and production\ncharacteristic dance primitives. In contrast, the second-stage is the local\ndiffusion, which parallelly generates detailed motion sequences under the\nguidance of the dance primitives and choreographic rules. In addition, we\npropose a Foot Refine Block to optimize the contact between the feet and the\nground, enhancing the physical realism of the motion. Our approach can\nparallelly generate dance sequences of extremely long length, striking a\nbalance between global choreographic patterns and local motion quality and\nexpressiveness. Extensive experiments validate the efficacy of our method.\n","authors":["Ronghui Li","YuXiang Zhang","Yachao Zhang","Hongwen Zhang","Jie Guo","Yan Zhang","Yebin Liu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.10518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10493v1","updated":"2024-03-15T17:27:42Z","published":"2024-03-15T17:27:42Z","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","summary":"  Diffusion-based audio and music generation models commonly generate music by\nconstructing an image representation of audio (e.g., a mel-spectrogram) and\nthen converting it to audio using a phase reconstruction model or vocoder.\nTypical vocoders, however, produce monophonic audio at lower resolutions (e.g.,\n16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an\nefficient high-fidelity stereophonic vocoder. Our method employs a cascade of\nthree generative adversarial networks (GANs) that convert low-resolution\nmel-spectrograms to audio, upsamples to high-resolution audio via bandwidth\nexpansion, and upmixes to stereophonic audio. Compared to previous work, we\npropose 1) a unified GAN-based generator and discriminator architecture and\ntraining procedure for each stage of our cascade, 2) a new fast, near\ndownsampling-compatible bandwidth extension module, and 3) a new fast\ndownmix-compatible mono-to-stereo upmixer that ensures the preservation of\nmonophonic content in the output. We evaluate our approach using both objective\nand subjective listening tests and find our approach yields comparable or\nbetter audio quality, better spatialization control, and significantly faster\ninference speed compared to past work. Sound examples are at\nhttps://MusicHiFi.github.io/web/.\n","authors":["Ge Zhu","Juan-Pablo Caceres","Zhiyao Duan","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2403.10493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10488v1","updated":"2024-03-15T17:23:38Z","published":"2024-03-15T17:23:38Z","title":"Joint Multimodal Transformer for Dimensional Emotional Recognition in\n  the Wild","summary":"  Audiovisual emotion recognition (ER) in videos has immense potential over\nunimodal performance. It effectively leverages the inter- and intra-modal\ndependencies between visual and auditory modalities. This work proposes a novel\naudio-visual emotion recognition system utilizing a joint multimodal\ntransformer architecture with key-based cross-attention. This framework aims to\nexploit the complementary nature of audio and visual cues (facial expressions\nand vocal patterns) in videos, leading to superior performance compared to\nsolely relying on a single modality. The proposed model leverages separate\nbackbones for capturing intra-modal temporal dependencies within each modality\n(audio and visual). Subsequently, a joint multimodal transformer architecture\nintegrates the individual modality embeddings, enabling the model to\neffectively capture inter-modal (between audio and visual) and intra-modal\n(within each modality) relationships. Extensive evaluations on the challenging\nAffwild2 dataset demonstrate that the proposed model significantly outperforms\nbaseline and state-of-the-art methods in ER tasks.\n","authors":["Paul Waligora","Osama Zeeshan","Haseeb Aslam","Soufiane Belharbi","Alessandro Lameiras Koerich","Marco Pedersoli","Simon Bacon","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2403.10488v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.10380v1","updated":"2024-03-15T15:10:40Z","published":"2024-03-15T15:10:40Z","title":"BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics","summary":"  Deep learning (DL) models have emerged as a powerful tool in avian\nbioacoustics to diagnose environmental health and biodiversity. However,\ninconsistencies in research pose notable challenges hindering progress in this\ndomain. Reliable DL models need to analyze bird calls flexibly across various\nspecies and environments to fully harness the potential of bioacoustics in a\ncost-effective passive acoustic monitoring scenario. Data fragmentation and\nopacity across studies complicate a comprehensive evaluation of general model\nperformance. To overcome these challenges, we present the BirdSet benchmark, a\nunified framework consolidating research efforts with a holistic approach for\nclassifying bird vocalizations in avian bioacoustics. BirdSet harmonizes\nopen-source bird recordings into a curated dataset collection. This unified\napproach provides an in-depth understanding of model performance and identifies\npotential shortcomings across different tasks. By establishing baseline results\nof current models, BirdSet aims to facilitate comparability, guide subsequent\ndata collection, and increase accessibility for newcomers to avian\nbioacoustics.\n","authors":["Lukas Rauch","Raphael Schwinger","Moritz Wirth","René Heinrich","Jonas Lange","Stefan Kahl","Bernhard Sick","Sven Tomforde","Christoph Scholz"],"pdf_url":"https://arxiv.org/pdf/2403.10380v1.pdf","comment":"Work in progress, to be submitted @DMLR next month"},{"id":"http://arxiv.org/abs/2403.10329v1","updated":"2024-03-15T14:16:46Z","published":"2024-03-15T14:16:46Z","title":"Multi-Source Localization and Data Association for Time-Difference of\n  Arrival Measurements","summary":"  In this work, we consider the problem of localizing multiple signal sources\nbased on time-difference of arrival (TDOA) measurements. In the blind setting,\nin which the source signals are not known, the localization task is challenging\ndue to the data association problem. That is, it is not known which of the TDOA\nmeasurements correspond to the same source. Herein, we propose to perform joint\nlocalization and data association by means of an optimal transport formulation.\nThe method operates by finding optimal groupings of TDOA measurements and\nassociating these with candidate source locations. To allow for computationally\nfeasible localization in three-dimensional space, an efficient set of candidate\nlocations is constructed using a minimal multilateration solver based on\nminimal sets of receiver pairs. In numerical simulations, we demonstrate that\nthe proposed method is robust both to measurement noise and TDOA detection\nerrors. Furthermore, it is shown that the data association provided by the\nproposed method allows for statistically efficient estimates of the source\nlocations.\n","authors":["Gabrielle Flood","Filip Elvander"],"pdf_url":"https://arxiv.org/pdf/2403.10329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14512v7","updated":"2024-03-15T13:10:26Z","published":"2023-08-28T11:58:07Z","title":"A time-causal and time-recursive analogue of the Gabor transform","summary":"  This paper presents a time-causal analogue of the Gabor filter, as well as a\nboth time-causal and time-recursive analogue of the Gabor transform, where the\nproposed time-causal representations obey both temporal scale covariance and a\ncascade property with a simplifying kernel over temporal scales. The motivation\nbehind these constructions is to enable theoretically well-founded\ntime-frequency analysis over multiple temporal scales for real-time situations,\nor for physical or biological modelling situations, when the future cannot be\naccessed, and the non-causal access to future in Gabor filtering is therefore\nnot viable for a time-frequency analysis of the system.\n  We develop the theory for these representations, obtained by replacing the\nGaussian kernel in Gabor filtering with a time-causal kernel, referred to as\nthe time-causal limit kernel, which guarantees simplification properties from\nfiner to coarser levels of scales in a time-causal situation, similar as the\nGaussian kernel can be shown to guarantee over a non-causal temporal domain. In\nthese ways, the proposed time-frequency representations guarantee well-founded\ntreatment over multiple scales, in situations when the characteristic scales in\nthe signals, or physical or biological phenomena, to be analyzed may vary\nsubstantially, and additionally all steps in the time-frequency analysis have\nto be fully time-causal.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2308.14512v7.pdf","comment":"23 pages, 7 figures, 1 algorithm"},{"id":"http://arxiv.org/abs/2403.10146v1","updated":"2024-03-15T09:47:17Z","published":"2024-03-15T09:47:17Z","title":"Multiscale Matching Driven by Cross-Modal Similarity Consistency for\n  Audio-Text Retrieval","summary":"  Audio-text retrieval (ATR), which retrieves a relevant caption given an audio\nclip (A2T) and vice versa (T2A), has recently attracted much research\nattention. Existing methods typically aggregate information from each modality\ninto a single vector for matching, but this sacrifices local details and can\nhardly capture intricate relationships within and between modalities.\nFurthermore, current ATR datasets lack comprehensive alignment information, and\nsimple binary contrastive learning labels overlook the measurement of\nfine-grained semantic differences between samples. To counter these challenges,\nwe present a novel ATR framework that comprehensively captures the matching\nrelationships of multimodal information from different perspectives and finer\ngranularities. Specifically, a fine-grained alignment method is introduced,\nachieving a more detail-oriented matching through a multiscale process from\nlocal to global levels to capture meticulous cross-modal relationships. In\naddition, we pioneer the application of cross-modal similarity consistency,\nleveraging intra-modal similarity relationships as soft supervision to boost\nmore intricate alignment. Extensive experiments validate the effectiveness of\nour approach, outperforming previous methods by significant margins of at least\n3.9% (T2A) / 6.9% (A2T) R@1 on the AudioCaps dataset and 2.9% (T2A) / 5.4%\n(A2T) R@1 on the Clotho dataset.\n","authors":["Qian Wang","Jia-Chen Gu","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2403.10146v1.pdf","comment":"5 pages, accepted to ICASSP2024"},{"id":"http://arxiv.org/abs/2403.10024v1","updated":"2024-03-15T05:13:38Z","published":"2024-03-15T05:13:38Z","title":"MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate\n  Instrument Leakage","summary":"  This paper presents enhancements to the MT3 model, a state-of-the-art (SOTA)\ntoken-based multi-instrument automatic music transcription (AMT) model. Despite\nSOTA performance, MT3 has the issue of instrument leakage, where transcriptions\nare fragmented across different instruments. To mitigate this, we propose\nMR-MT3, with enhancements including a memory retention mechanism, prior token\nsampling, and token shuffling are proposed. These methods are evaluated on the\nSlakh2100 dataset, demonstrating improved onset F1 scores and reduced\ninstrument leakage. In addition to the conventional multi-instrument\ntranscription F1 score, new metrics such as the instrument leakage ratio and\nthe instrument detection F1 score are introduced for a more comprehensive\nassessment of transcription quality. The study also explores the issue of\ndomain overfitting by evaluating MT3 on single-instrument monophonic datasets\nsuch as ComMU and NSynth. The findings, along with the source code, are shared\nto facilitate future work aimed at refining token-based multi-instrument AMT\nmodels.\n","authors":["Hao Hao Tan","Kin Wai Cheuk","Taemin Cho","Wei-Hsiang Liao","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2403.10024v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.09527v2","updated":"2024-03-15T03:35:31Z","published":"2024-03-14T16:10:34Z","title":"WavCraft: Audio Editing and Generation with Natural Language Prompts","summary":"  We introduce WavCraft, a collective system that leverages large language\nmodels (LLMs) to connect diverse task-specific models for audio content\ncreation and editing. Specifically, WavCraft describes the content of raw sound\nmaterials in natural language and prompts the LLM conditioned on audio\ndescriptions and users' requests. WavCraft leverages the in-context learning\nability of the LLM to decomposes users' instructions into several tasks and\ntackle each task collaboratively with audio expert modules. Through task\ndecomposition along with a set of task-specific models, WavCraft follows the\ninput instruction to create or edit audio content with more details and\nrationales, facilitating users' control. In addition, WavCraft is able to\ncooperate with users via dialogue interaction and even produce the audio\ncontent without explicit user commands. Experiments demonstrate that WavCraft\nyields a better performance than existing methods, especially when adjusting\nthe local regions of audio clips. Moreover, WavCraft can follow complex\ninstructions to edit and even create audio content on the top of input\nrecordings, facilitating audio producers in a broader range of applications.\nOur implementation and demos are available at\nhttps://github.com/JinhuaLiang/WavCraft.\n","authors":["Jinhua Liang","Huan Zhang","Haohe Liu","Yin Cao","Qiuqiang Kong","Xubo Liu","Wenwu Wang","Mark D. Plumbley","Huy Phan","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2403.09527v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07661v2","updated":"2024-03-15T13:21:31Z","published":"2024-03-12T13:50:58Z","title":"Gender-ambiguous voice generation through feminine speaking style\n  transfer in male voices","summary":"  Recently, and under the umbrella of Responsible AI, efforts have been made to\ndevelop gender-ambiguous synthetic speech to represent with a single voice all\nindividuals in the gender spectrum. However, research efforts have completely\noverlooked the speaking style despite differences found among binary and\nnon-binary populations. In this work, we synthesise gender-ambiguous speech by\ncombining the timbre of a male speaker with the manner of speech of a female\nspeaker using voice morphing and pitch shifting towards the male-female\nboundary. Subjective evaluations indicate that the ambiguity of the morphed\nsamples that convey the female speech style is higher than those that undergo\nplain pitch transformations suggesting that the speaking style can be a\ncontributing factor in creating gender-ambiguous speech. To our knowledge, this\nis the first study that explicitly uses the transfer of the speaking style to\ncreate gender-ambiguous voices.\n","authors":["Maria Koutsogiannaki","Shafel Mc Dowall","Ioannis Agiomyrgiannakis"],"pdf_url":"https://arxiv.org/pdf/2403.07661v2.pdf","comment":"submitted to Interspeech"},{"id":"http://arxiv.org/abs/2309.08005v2","updated":"2024-03-15T20:21:36Z","published":"2023-09-14T19:29:12Z","title":"Efficient Face Detection with Audio-Based Region Proposals for\n  Human-Robot Interactions","summary":"  Efficient face detection is critical to provide natural human-robot\ninteractions. However, computer vision tends to involve a large computational\nload due to the amount of data (i.e. pixels) that needs to be processed in a\nshort amount of time. This is undesirable on robotics platforms where multiple\nprocesses need to run in parallel and where the processing power is limited by\nportability constraints. Existing solutions often involve reducing image\nquality which can negatively impact processing. The literature also reports\nmethods to generate regions of interest in images from pixel data. Although it\nis a promising idea, these methods often involve heavy vision algorithms. In\nthis paper, we evaluate how audio can be used to generate regions of interest\nin optical images to reduce the number of pixels to process with computer\nvision. Thereby, we propose a unique attention mechanism to localize a speech\nsource and evaluate its impact on an existing face detection algorithm. Our\nresults show that the attention mechanism reduces the computational load and\noffers an interesting trade-off between speed and accuracy. The proposed\npipeline is flexible and can be easily adapted to other applications such as\nrobot surveillance, video conferences or smart glasses.\n","authors":["William Aris","François Grondin"],"pdf_url":"https://arxiv.org/pdf/2309.08005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10518v1","updated":"2024-03-15T17:59:33Z","published":"2024-03-15T17:59:33Z","title":"Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation\n  Guided by the Characteristic Dance Primitives","summary":"  We propose Lodge, a network capable of generating extremely long dance\nsequences conditioned on given music. We design Lodge as a two-stage coarse to\nfine diffusion architecture, and propose the characteristic dance primitives\nthat possess significant expressiveness as intermediate representations between\ntwo diffusion models. The first stage is global diffusion, which focuses on\ncomprehending the coarse-level music-dance correlation and production\ncharacteristic dance primitives. In contrast, the second-stage is the local\ndiffusion, which parallelly generates detailed motion sequences under the\nguidance of the dance primitives and choreographic rules. In addition, we\npropose a Foot Refine Block to optimize the contact between the feet and the\nground, enhancing the physical realism of the motion. Our approach can\nparallelly generate dance sequences of extremely long length, striking a\nbalance between global choreographic patterns and local motion quality and\nexpressiveness. Extensive experiments validate the efficacy of our method.\n","authors":["Ronghui Li","YuXiang Zhang","Yachao Zhang","Hongwen Zhang","Jie Guo","Yan Zhang","Yebin Liu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.10518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10493v1","updated":"2024-03-15T17:27:42Z","published":"2024-03-15T17:27:42Z","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","summary":"  Diffusion-based audio and music generation models commonly generate music by\nconstructing an image representation of audio (e.g., a mel-spectrogram) and\nthen converting it to audio using a phase reconstruction model or vocoder.\nTypical vocoders, however, produce monophonic audio at lower resolutions (e.g.,\n16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an\nefficient high-fidelity stereophonic vocoder. Our method employs a cascade of\nthree generative adversarial networks (GANs) that convert low-resolution\nmel-spectrograms to audio, upsamples to high-resolution audio via bandwidth\nexpansion, and upmixes to stereophonic audio. Compared to previous work, we\npropose 1) a unified GAN-based generator and discriminator architecture and\ntraining procedure for each stage of our cascade, 2) a new fast, near\ndownsampling-compatible bandwidth extension module, and 3) a new fast\ndownmix-compatible mono-to-stereo upmixer that ensures the preservation of\nmonophonic content in the output. We evaluate our approach using both objective\nand subjective listening tests and find our approach yields comparable or\nbetter audio quality, better spatialization control, and significantly faster\ninference speed compared to past work. Sound examples are at\nhttps://MusicHiFi.github.io/web/.\n","authors":["Ge Zhu","Juan-Pablo Caceres","Zhiyao Duan","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2403.10493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10488v1","updated":"2024-03-15T17:23:38Z","published":"2024-03-15T17:23:38Z","title":"Joint Multimodal Transformer for Dimensional Emotional Recognition in\n  the Wild","summary":"  Audiovisual emotion recognition (ER) in videos has immense potential over\nunimodal performance. It effectively leverages the inter- and intra-modal\ndependencies between visual and auditory modalities. This work proposes a novel\naudio-visual emotion recognition system utilizing a joint multimodal\ntransformer architecture with key-based cross-attention. This framework aims to\nexploit the complementary nature of audio and visual cues (facial expressions\nand vocal patterns) in videos, leading to superior performance compared to\nsolely relying on a single modality. The proposed model leverages separate\nbackbones for capturing intra-modal temporal dependencies within each modality\n(audio and visual). Subsequently, a joint multimodal transformer architecture\nintegrates the individual modality embeddings, enabling the model to\neffectively capture inter-modal (between audio and visual) and intra-modal\n(within each modality) relationships. Extensive evaluations on the challenging\nAffwild2 dataset demonstrate that the proposed model significantly outperforms\nbaseline and state-of-the-art methods in ER tasks.\n","authors":["Paul Waligora","Osama Zeeshan","Haseeb Aslam","Soufiane Belharbi","Alessandro Lameiras Koerich","Marco Pedersoli","Simon Bacon","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2403.10488v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.10428v1","updated":"2024-03-15T16:00:27Z","published":"2024-03-15T16:00:27Z","title":"How to train your ears: Auditory-model emulation for large-dynamic-range\n  inputs and mild-to-severe hearing losses","summary":"  Advanced auditory models are useful in designing signal-processing algorithms\nfor hearing-loss compensation or speech enhancement. Such auditory models\nprovide rich and detailed descriptions of the auditory pathway, and might allow\nfor individualization of signal-processing strategies, based on physiological\nmeasurements. However, these auditory models are often computationally\ndemanding, requiring significant time to compute. To address this issue,\nprevious studies have explored the use of deep neural networks to emulate\nauditory models and reduce inference time. While these deep neural networks\noffer impressive efficiency gains in terms of computational time, they may\nsuffer from uneven emulation performance as a function of auditory-model\nfrequency-channels and input sound pressure level, making them unsuitable for\nmany tasks. In this study, we demonstrate that the conventional\nmachine-learning optimization objective used in existing state-of-the-art\nmethods is the primary source of this limitation. Specifically, the\noptimization objective fails to account for the frequency- and\nlevel-dependencies of the auditory model, caused by a large input dynamic range\nand different types of hearing losses emulated by the auditory model. To\novercome this limitation, we propose a new optimization objective that\nexplicitly embeds the frequency- and level-dependencies of the auditory model.\nOur results show that this new optimization objective significantly improves\nthe emulation performance of deep neural networks across relevant input sound\nlevels and auditory-model frequency channels, without increasing the\ncomputational load during inference. Addressing these limitations is essential\nfor advancing the application of auditory models in signal-processing tasks,\nensuring their efficacy in diverse scenarios.\n","authors":["Peter Leer","Jesper Jensen","Zheng-Hua Tan","Jan Østergaard","Lars Bramsløw"],"pdf_url":"https://arxiv.org/pdf/2403.10428v1.pdf","comment":"Accepted by IEEE/ACM Transactions on Audio, Speech and Language\n  Processing. This version is the authors' version and may vary from the final\n  publication in details"},{"id":"http://arxiv.org/abs/2403.10420v1","updated":"2024-03-15T15:55:19Z","published":"2024-03-15T15:55:19Z","title":"Neural Networks Hear You Loud And Clear: Hearing Loss Compensation Using\n  Deep Neural Networks","summary":"  This article investigates the use of deep neural networks (DNNs) for\nhearing-loss compensation. Hearing loss is a prevalent issue affecting millions\nof people worldwide, and conventional hearing aids have limitations in\nproviding satisfactory compensation. DNNs have shown remarkable performance in\nvarious auditory tasks, including speech recognition, speaker identification,\nand music classification. In this study, we propose a DNN-based approach for\nhearing-loss compensation, which is trained on the outputs of hearing-impaired\nand normal-hearing DNN-based auditory models in response to speech signals.\nFirst, we introduce a framework for emulating auditory models using DNNs,\nfocusing on an auditory-nerve model in the auditory pathway. We propose a\nlinearization of the DNN-based approach, which we use to analyze the DNN-based\nhearing-loss compensation. Additionally we develop a simple approach to choose\nthe acoustic center frequencies of the auditory model used for the compensation\nstrategy. Finally, we evaluate the DNN-based hearing-loss compensation\nstrategies using listening tests with hearing impaired listeners. The results\ndemonstrate that the proposed approach results in feasible hearing-loss\ncompensation strategies. Our proposed approach was shown to provide an increase\nin speech intelligibility and was found to outperform a conventional approach\nin terms of perceived speech quality.\n","authors":["Peter Leer","Jesper Jensen","Laurel Carney","Zheng-Hua Tan","Jan Østergaard","Lars Bramsløw"],"pdf_url":"https://arxiv.org/pdf/2403.10420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10380v1","updated":"2024-03-15T15:10:40Z","published":"2024-03-15T15:10:40Z","title":"BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics","summary":"  Deep learning (DL) models have emerged as a powerful tool in avian\nbioacoustics to diagnose environmental health and biodiversity. However,\ninconsistencies in research pose notable challenges hindering progress in this\ndomain. Reliable DL models need to analyze bird calls flexibly across various\nspecies and environments to fully harness the potential of bioacoustics in a\ncost-effective passive acoustic monitoring scenario. Data fragmentation and\nopacity across studies complicate a comprehensive evaluation of general model\nperformance. To overcome these challenges, we present the BirdSet benchmark, a\nunified framework consolidating research efforts with a holistic approach for\nclassifying bird vocalizations in avian bioacoustics. BirdSet harmonizes\nopen-source bird recordings into a curated dataset collection. This unified\napproach provides an in-depth understanding of model performance and identifies\npotential shortcomings across different tasks. By establishing baseline results\nof current models, BirdSet aims to facilitate comparability, guide subsequent\ndata collection, and increase accessibility for newcomers to avian\nbioacoustics.\n","authors":["Lukas Rauch","Raphael Schwinger","Moritz Wirth","René Heinrich","Jonas Lange","Stefan Kahl","Bernhard Sick","Sven Tomforde","Christoph Scholz"],"pdf_url":"https://arxiv.org/pdf/2403.10380v1.pdf","comment":"Work in progress, to be submitted @DMLR next month"},{"id":"http://arxiv.org/abs/2403.10329v1","updated":"2024-03-15T14:16:46Z","published":"2024-03-15T14:16:46Z","title":"Multi-Source Localization and Data Association for Time-Difference of\n  Arrival Measurements","summary":"  In this work, we consider the problem of localizing multiple signal sources\nbased on time-difference of arrival (TDOA) measurements. In the blind setting,\nin which the source signals are not known, the localization task is challenging\ndue to the data association problem. That is, it is not known which of the TDOA\nmeasurements correspond to the same source. Herein, we propose to perform joint\nlocalization and data association by means of an optimal transport formulation.\nThe method operates by finding optimal groupings of TDOA measurements and\nassociating these with candidate source locations. To allow for computationally\nfeasible localization in three-dimensional space, an efficient set of candidate\nlocations is constructed using a minimal multilateration solver based on\nminimal sets of receiver pairs. In numerical simulations, we demonstrate that\nthe proposed method is robust both to measurement noise and TDOA detection\nerrors. Furthermore, it is shown that the data association provided by the\nproposed method allows for statistically efficient estimates of the source\nlocations.\n","authors":["Gabrielle Flood","Filip Elvander"],"pdf_url":"https://arxiv.org/pdf/2403.10329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14512v7","updated":"2024-03-15T13:10:26Z","published":"2023-08-28T11:58:07Z","title":"A time-causal and time-recursive analogue of the Gabor transform","summary":"  This paper presents a time-causal analogue of the Gabor filter, as well as a\nboth time-causal and time-recursive analogue of the Gabor transform, where the\nproposed time-causal representations obey both temporal scale covariance and a\ncascade property with a simplifying kernel over temporal scales. The motivation\nbehind these constructions is to enable theoretically well-founded\ntime-frequency analysis over multiple temporal scales for real-time situations,\nor for physical or biological modelling situations, when the future cannot be\naccessed, and the non-causal access to future in Gabor filtering is therefore\nnot viable for a time-frequency analysis of the system.\n  We develop the theory for these representations, obtained by replacing the\nGaussian kernel in Gabor filtering with a time-causal kernel, referred to as\nthe time-causal limit kernel, which guarantees simplification properties from\nfiner to coarser levels of scales in a time-causal situation, similar as the\nGaussian kernel can be shown to guarantee over a non-causal temporal domain. In\nthese ways, the proposed time-frequency representations guarantee well-founded\ntreatment over multiple scales, in situations when the characteristic scales in\nthe signals, or physical or biological phenomena, to be analyzed may vary\nsubstantially, and additionally all steps in the time-frequency analysis have\nto be fully time-causal.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2308.14512v7.pdf","comment":"23 pages, 7 figures, 1 algorithm"},{"id":"http://arxiv.org/abs/2403.10271v1","updated":"2024-03-15T13:03:24Z","published":"2024-03-15T13:03:24Z","title":"SuperME: Supervised and Mixture-to-Mixture Co-Learning for Speech\n  Enhancement and Robust ASR","summary":"  The current dominant approach for neural speech enhancement is based on\nsupervised learning by using simulated training data. The trained models,\nhowever, often exhibit limited generalizability to real-recorded data. To\naddress this, we investigate training models directly on real target-domain\ndata, and propose two algorithms, mixture-to-mixture (M2M) training and a\nco-learning algorithm that improves M2M with the help of supervised algorithms.\nWhen paired close-talk and far-field mixtures are available for training, M2M\nrealizes speech enhancement by training a deep neural network (DNN) to produce\nspeech and noise estimates in a way such that they can be linearly filtered to\nreconstruct the close-talk and far-field mixtures. This way, the DNN can be\ntrained directly on real mixtures, and can leverage close-talk mixtures as a\nweak supervision to enhance far-field mixtures. To improve M2M, we combine it\nwith supervised approaches to co-train the DNN, where mini-batches of real\nclose-talk and far-field mixture pairs and mini-batches of simulated mixture\nand clean speech pairs are alternately fed to the DNN, and the loss functions\nare respectively (a) the mixture reconstruction loss on the real close-talk and\nfar-field mixtures and (b) the regular enhancement loss on the simulated clean\nspeech and noise. We find that, this way, the DNN can learn from real and\nsimulated data to achieve better generalization to real data. We name this\nalgorithm SuperME, $\\underline{super}$vised and\n$\\underline{m}$ixture-to-mixtur$\\underline{e}$ co-learning. Evaluation results\non the CHiME-4 dataset show its effectiveness and potential.\n","authors":["Zhong-Qiu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10271v1.pdf","comment":"in submission"},{"id":"http://arxiv.org/abs/2403.10146v1","updated":"2024-03-15T09:47:17Z","published":"2024-03-15T09:47:17Z","title":"Multiscale Matching Driven by Cross-Modal Similarity Consistency for\n  Audio-Text Retrieval","summary":"  Audio-text retrieval (ATR), which retrieves a relevant caption given an audio\nclip (A2T) and vice versa (T2A), has recently attracted much research\nattention. Existing methods typically aggregate information from each modality\ninto a single vector for matching, but this sacrifices local details and can\nhardly capture intricate relationships within and between modalities.\nFurthermore, current ATR datasets lack comprehensive alignment information, and\nsimple binary contrastive learning labels overlook the measurement of\nfine-grained semantic differences between samples. To counter these challenges,\nwe present a novel ATR framework that comprehensively captures the matching\nrelationships of multimodal information from different perspectives and finer\ngranularities. Specifically, a fine-grained alignment method is introduced,\nachieving a more detail-oriented matching through a multiscale process from\nlocal to global levels to capture meticulous cross-modal relationships. In\naddition, we pioneer the application of cross-modal similarity consistency,\nleveraging intra-modal similarity relationships as soft supervision to boost\nmore intricate alignment. Extensive experiments validate the effectiveness of\nour approach, outperforming previous methods by significant margins of at least\n3.9% (T2A) / 6.9% (A2T) R@1 on the AudioCaps dataset and 2.9% (T2A) / 5.4%\n(A2T) R@1 on the Clotho dataset.\n","authors":["Qian Wang","Jia-Chen Gu","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2403.10146v1.pdf","comment":"5 pages, accepted to ICASSP2024"},{"id":"http://arxiv.org/abs/2208.10922v2","updated":"2024-03-15T08:48:04Z","published":"2022-08-23T12:49:01Z","title":"StyleTalker: One-shot Style-based Audio-driven Talking Head Video\n  Generation","summary":"  We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.\n","authors":["Dongchan Min","Minyoung Song","Eunji Ko","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2208.10922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10024v1","updated":"2024-03-15T05:13:38Z","published":"2024-03-15T05:13:38Z","title":"MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate\n  Instrument Leakage","summary":"  This paper presents enhancements to the MT3 model, a state-of-the-art (SOTA)\ntoken-based multi-instrument automatic music transcription (AMT) model. Despite\nSOTA performance, MT3 has the issue of instrument leakage, where transcriptions\nare fragmented across different instruments. To mitigate this, we propose\nMR-MT3, with enhancements including a memory retention mechanism, prior token\nsampling, and token shuffling are proposed. These methods are evaluated on the\nSlakh2100 dataset, demonstrating improved onset F1 scores and reduced\ninstrument leakage. In addition to the conventional multi-instrument\ntranscription F1 score, new metrics such as the instrument leakage ratio and\nthe instrument detection F1 score are introduced for a more comprehensive\nassessment of transcription quality. The study also explores the issue of\ndomain overfitting by evaluating MT3 on single-instrument monophonic datasets\nsuch as ComMU and NSynth. The findings, along with the source code, are shared\nto facilitate future work aimed at refining token-based multi-instrument AMT\nmodels.\n","authors":["Hao Hao Tan","Kin Wai Cheuk","Taemin Cho","Wei-Hsiang Liao","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2403.10024v1.pdf","comment":null}]},"2024-03-18T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.11706v1","updated":"2024-03-18T12:08:01Z","published":"2024-03-18T12:08:01Z","title":"Generalized Multi-Source Inference for Text Conditioned Music Diffusion\n  Models","summary":"  Multi-Source Diffusion Models (MSDM) allow for compositional musical\ngeneration tasks: generating a set of coherent sources, creating\naccompaniments, and performing source separation. Despite their versatility,\nthey require estimating the joint distribution over the sources, necessitating\npre-separated musical data, which is rarely available, and fixing the number\nand type of sources at training time. This paper generalizes MSDM to arbitrary\ntime-domain diffusion models conditioned on text embeddings. These models do\nnot require separated data as they are trained on mixtures, can parameterize an\narbitrary number of sources, and allow for rich semantic control. We propose an\ninference procedure enabling the coherent generation of sources and\naccompaniments. Additionally, we adapt the Dirac separator of MSDM to perform\nsource separation. We experiment with diffusion models trained on Slakh2100 and\nMTG-Jamendo, showcasing competitive generation and separation results in a\nrelaxed data setting.\n","authors":["Emilian Postolache","Giorgio Mariani","Luca Cosmo","Emmanouil Benetos","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2403.11706v1.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.00502v2","updated":"2024-03-18T10:32:01Z","published":"2023-12-01T11:06:00Z","title":"A Comprehensive Evaluation of Augmentations for Robust OOD\n  Self-Supervised Contrastive Phonocardiogram Representation Learning","summary":"  Despite the recent increase in research activity, deep-learning models have\nnot yet been widely accepted in several real-world settings, such as medicine.\nThe shortage of high-quality annotated data often hinders the development of\nrobust and generalizable models, which do not suffer from degraded\neffectiveness when presented with newly-collected, out-of-distribution (OOD)\ndatasets. Contrastive Self-Supervised Learning (SSL) offers a potential\nsolution to labeled data scarcity, as it takes advantage of unlabeled data to\nincrease model effectiveness and robustness. In this research, we propose\napplying contrastive SSL for detecting abnormalities in 1D phonocardiogram\n(PCG) samples by learning a generalized representation of the signal.\nSpecifically, we perform an extensive comparative evaluation of a wide range of\naudio-based augmentations, evaluate trained classifiers on multiple datasets\nacross different downstream tasks, and finally report on the impact of each\naugmentation in model training. We experimentally demonstrate that, depending\non its training distribution, the effectiveness of a fully-supervised model can\ndegrade up to 32% when evaluated on unseen data, while SSL models only lose up\nto 10% or even improve in some cases. We argue and experimentally demonstrate\nthat, contrastive SSL pretraining can assist in providing robust classifiers\nwhich can generalize to unseen, OOD data, without relying on time- and\nlabor-intensive annotation processes by medical experts. Furthermore, the\nproposed extensive evaluation protocol sheds light on the most promising and\nappropriate augmentations for robust PCG signal processing, by calculating\ntheir effect size on model training. Finally, we provide researchers and\npractitioners with a roadmap towards producing robust models for PCG\nclassification, in addition to an open-source codebase for developing novel\napproaches.\n","authors":["Aristotelis Ballas","Vasileios Papapanagiotou","Christos Diou"],"pdf_url":"https://arxiv.org/pdf/2312.00502v2.pdf","comment":"PREPRINT Manuscript under review"},{"id":"http://arxiv.org/abs/2403.11626v1","updated":"2024-03-18T09:58:43Z","published":"2024-03-18T09:58:43Z","title":"QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation","summary":"  The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.\n","authors":["Zhizhen Zhou","Yejing Huo","Guoheng Huang","An Zeng","Xuhang Chen","Lian Huang","Zinuo Li"],"pdf_url":"https://arxiv.org/pdf/2403.11626v1.pdf","comment":"Accepted by The Visual Computer Journal"},{"id":"http://arxiv.org/abs/2307.07218v3","updated":"2024-03-18T09:29:37Z","published":"2023-07-14T08:21:25Z","title":"Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis","summary":"  Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech\nprompts, which significantly reduces the data and computation requirements for\nvoice cloning by skipping the fine-tuning process. However, the prompting\nmechanisms of zero-shot TTS still face challenges in the following aspects: 1)\nprevious works of zero-shot TTS are typically trained with single-sentence\nprompts, which significantly restricts their performance when the data is\nrelatively sufficient during the inference stage. 2) The prosodic information\nin prompts is highly coupled with timbre, making it untransferable to each\nother. This paper introduces Mega-TTS 2, a generic prompting mechanism for\nzero-shot TTS, to tackle the aforementioned challenges. Specifically, we design\na powerful acoustic autoencoder that separately encodes the prosody and timbre\ninformation into the compressed latent space while providing high-quality\nreconstructions. Then, we propose a multi-reference timbre encoder and a\nprosody latent language model (P-LLM) to extract useful information from\nmulti-sentence prompts. We further leverage the probabilities derived from\nmultiple P-LLM outputs to produce transferable and controllable prosody.\nExperimental results demonstrate that Mega-TTS 2 could not only synthesize\nidentity-preserving speech with a short prompt of an unseen speaker from\narbitrary sources but consistently outperform the fine-tuning method when the\nvolume of data ranges from 10 seconds to 5 minutes. Furthermore, our method\nenables to transfer various speaking styles to the target timbre in a\nfine-grained and controlled manner. Audio samples can be found in\nhttps://boostprompt.github.io/boostprompt/.\n","authors":["Ziyue Jiang","Jinglin Liu","Yi Ren","Jinzheng He","Zhenhui Ye","Shengpeng Ji","Qian Yang","Chen Zhang","Pengfei Wei","Chunfeng Wang","Xiang Yin","Zejun Ma","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2307.07218v3.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2310.03018v3","updated":"2024-03-18T07:57:58Z","published":"2023-10-04T17:58:11Z","title":"Zero Resource Code-switched Speech Benchmark Using Speech Utterance\n  Pairs For Multiple Spoken Languages","summary":"  We introduce a new zero resource code-switched speech benchmark designed to\ndirectly assess the code-switching capabilities of self-supervised speech\nencoders. We showcase a baseline system of language modeling on discrete units\nto demonstrate how the code-switching abilities of speech encoders can be\nassessed in a zero-resource manner. Our experiments encompass a variety of\nwell-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We\nexamine the impact of pre-training languages and model size on benchmark\nperformance. Notably, though our results demonstrate that speech encoders with\nmultilingual pre-training, exemplified by XLSR, outperform monolingual variants\n(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial\nroom for improvement in their code-switching linguistic abilities.\n","authors":["Kuan-Po Huang","Chih-Kai Yang","Yu-Kuan Fu","Ewan Dunbar","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2310.03018v3.pdf","comment":"Accepted by ICASSP 2024 (v2)"},{"id":"http://arxiv.org/abs/2401.13463v2","updated":"2024-03-18T06:08:31Z","published":"2024-01-24T14:08:38Z","title":"SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken\n  Question Answering","summary":"  Spoken Question Answering (SQA) is essential for machines to reply to user's\nquestion by finding the answer span within a given spoken passage. SQA has been\npreviously achieved without ASR to avoid recognition errors and\nOut-of-Vocabulary (OOV) problems. However, the real-world problem of\nOpen-domain SQA (openSQA), in which the machine needs to first retrieve\npassages that possibly contain the answer from a spoken archive in addition,\nwas never considered. This paper proposes the first known end-to-end framework,\nSpeech Dense Passage Retriever (SpeechDPR), for the retrieval component of the\nopenSQA problem. SpeechDPR learns a sentence-level semantic representation by\ndistilling knowledge from the cascading model of unsupervised ASR (UASR) and\ntext dense retriever (TDR). No manually transcribed speech data is needed.\nInitial experiments showed performance comparable to the cascading model of\nUASR and TDR, and significantly better when UASR was poor, verifying this\napproach is more robust to speech recognition errors.\n","authors":["Chyi-Jiunn Lin","Guan-Ting Lin","Yung-Sung Chuang","Wei-Lun Wu","Shang-Wen Li","Abdelrahman Mohamed","Hung-yi Lee","Lin-shan Lee"],"pdf_url":"https://arxiv.org/pdf/2401.13463v2.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2210.15563v3","updated":"2024-03-18T05:55:34Z","published":"2022-10-27T15:53:38Z","title":"Multimodal Transformer Distillation for Audio-Visual Synchronization","summary":"  Audio-visual synchronization aims to determine whether the mouth movements\nand speech in the video are synchronized. VocaLiST reaches state-of-the-art\nperformance by incorporating multimodal Transformers to model audio-visual\ninteract information. However, it requires high computing resources, making it\nimpractical for real-world applications. This paper proposed an MTDVocaLiST\nmodel, which is trained by our proposed multimodal Transformer distillation\n(MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the\ncross-attention distribution and value-relation in the Transformer of VocaLiST.\nAdditionally, we harness uncertainty weighting to fully exploit the interaction\ninformation across all layers. Our proposed method is effective in two aspects:\nFrom the distillation method perspective, MTD loss outperforms other strong\ndistillation baselines. From the distilled model's performance perspective: 1)\nMTDVocaLiST outperforms similar-size SOTA models, SyncNet, and Perfect Match\nmodels by 15.65% and 3.35%; 2) MTDVocaLiST reduces the model size of VocaLiST\nby 83.52%, yet still maintaining similar performance.\n","authors":["Xuanjun Chen","Haibin Wu","Chung-Che Wang","Hung-yi Lee","Jyh-Shing Roger Jang"],"pdf_url":"https://arxiv.org/pdf/2210.15563v3.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2302.13451v2","updated":"2024-03-18T01:09:44Z","published":"2023-02-27T00:44:22Z","title":"A low latency attention module for streaming self-supervised speech\n  representation learning","summary":"  The transformer is a fundamental building block in deep learning, and the\nattention mechanism is the transformer's core component. Self-supervised speech\nrepresentation learning (SSRL) represents a popular use-case for the\ntransformer architecture. Due to transformers' acausal behavior, the use of\ntransformers for SSRL has been predominantly focused on acausal applications.\nHowever, several media processing problems, such as speech processing, require\nreal-time solutions. In this paper, we present an implementation of the\nattention module that enables training of SSRL architectures with low compute\nand memory requirements, while allowing real-time inference with low and fixed\nlatency. The attention module proposed in this paper includes two components,\nstreaming attention (SA) and low-latency streaming attention (LLSA). The SA\nrepresents our proposal for an efficient streaming SSRL implementation, while\nthe LLSA solves the latency build-up problem of other streaming attention\narchitectures, such as the masked acausal attention (MAA), guaranteeing a\nlatency equal to one layer even when multiple layers are stacked. We present a\ncomparative analysis between the vanilla attention, which we will refer here as\nacausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with\nautomatic speech recognition as downstream task. When training on\nlibrispeech-clean-100 and testing on librispeech-test-clean, our low-latency\nattention module has a word error rate (WER) of 5.84%, which represents a\nsignificant improvement over the MAA (WER = 13.82%). Our implementation also\nreduces the inference latency from 1.92 to 0.16 seconds. The proposed\nlow-latency module preserves many of the benefits of conventional acausal\ntransformers, but also enables latency characteristics that make it applicable\nto real-time streaming applications.\n","authors":["Jianbo Ma","Siqi Pan","Deepak Chandran","Andrea Fanelli","Richard Cartwright"],"pdf_url":"https://arxiv.org/pdf/2302.13451v2.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.12258v1","updated":"2024-03-18T21:14:18Z","published":"2024-03-18T21:14:18Z","title":"A Multi-loudspeaker Binaural Room Impulse Response Dataset with\n  High-Resolution Translational and Rotational Head Coordinates in a Listening\n  Room","summary":"  Data report for the 3D3A Lab Binaural Room Impulse Response (BRIR) Dataset\n(https://doi.org/10.34770/6gc9-5787).\n","authors":["Yue Qiao","Ryan Miguel Gonzales","Edgar Choueiri"],"pdf_url":"https://arxiv.org/pdf/2403.12258v1.pdf","comment":"Submitted to Frontiers in Signal Processing"},{"id":"http://arxiv.org/abs/2403.12000v1","updated":"2024-03-18T17:35:02Z","published":"2024-03-18T17:35:02Z","title":"Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance","summary":"  Deep learning-based probabilistic models of musical data are producing\nincreasingly realistic results and promise to enter creative workflows of many\nkinds. Yet they have been little-studied in a performance setting, where the\nresults of user actions typically ought to feel instantaneous. To enable such\nstudy, we designed Notochord, a deep probabilistic model for sequences of\nstructured events, and trained an instance of it on the Lakh MIDI dataset. Our\nprobabilistic formulation allows interpretable interventions at a sub-event\nlevel, which enables one model to act as a backbone for diverse interactive\nmusical functions including steerable generation, harmonization, machine\nimprovisation, and likelihood-based interfaces. Notochord can generate\npolyphonic and multi-track MIDI, and respond to inputs with latency below ten\nmilliseconds. Training code, model checkpoints and interactive examples are\nprovided as open source software.\n","authors":["Victor Shepardson","Jack Armitage","Thor Magnusson"],"pdf_url":"https://arxiv.org/pdf/2403.12000v1.pdf","comment":"12 pages, 6 figures. Proceedings of the 3rd Conference on AI Music\n  Creativity (2022, September 17)"},{"id":"http://arxiv.org/abs/2403.11879v1","updated":"2024-03-18T15:32:02Z","published":"2024-03-18T15:32:02Z","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton","summary":"  In this study, we propose a methodology for the Emotional Mimicry Intensity\n(EMI) Estimation task within the context of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0\nframework, pre-trained on a comprehensive podcast dataset, to extract a broad\nrange of audio features encompassing both linguistic and paralinguistic\nelements. We enhance feature representation through a fusion technique that\nintegrates individual features with a global mean vector, introducing global\ncontextual insights into our analysis. Additionally, we incorporate a\npre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.\nOur fusion employs a Long Short-Term Memory (LSTM) architecture for efficient\ntemporal analysis of audio data. Utilizing only the provided audio data, our\napproach demonstrates significant improvements over the established baseline.\n","authors":["Tobias Hallmen","Fabian Deuser","Norbert Oswald","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2403.11879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11827v1","updated":"2024-03-18T14:34:16Z","published":"2024-03-18T14:34:16Z","title":"Sound Event Detection and Localization with Distance Estimation","summary":"  Sound Event Detection and Localization (SELD) is a combined task of\nidentifying sound events and their corresponding direction-of-arrival (DOA).\nWhile this task has numerous applications and has been extensively researched\nin recent years, it fails to provide full information about the sound source\nposition. In this paper, we overcome this problem by extending the task to\nSound Event Detection, Localization with Distance Estimation (3D SELD). We\nstudy two ways of integrating distance estimation within the SELD core - a\nmulti-task approach, in which the problem is tackled by a separate model\noutput, and a single-task approach obtained by extending the multi-ACCDOA\nmethod to include distance information. We investigate both methods for the\nAmbisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial\nSoundscapes 2023. Moreover, our study involves experiments on the loss function\nrelated to the distance estimation part. Our results show that it is possible\nto perform 3D SELD without any degradation of performance in sound event\ndetection and DOA estimation.\n","authors":["Daniel Aleksander Krause","Archontis Politis","Annamaria Mesaros"],"pdf_url":"https://arxiv.org/pdf/2403.11827v1.pdf","comment":"This paper has been submitted for the 32nd European Signal Processing\n  Conference EUSIPCO 2024 in Lyon"},{"id":"http://arxiv.org/abs/2403.11780v1","updated":"2024-03-18T13:39:05Z","published":"2024-03-18T13:39:05Z","title":"Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural\n  Language Prompt","summary":"  Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio\nquality and naturalness, yet they lack the capability to control the style\nattributes of the synthesized singing explicitly. We propose Prompt-Singer, the\nfirst SVS method that enables attribute controlling on singer gender, vocal\nrange and volume with natural language. We adopt a model architecture based on\na decoder-only transformer with a multi-scale hierarchy, and design a\nrange-melody decoupled pitch representation that enables text-conditioned vocal\nrange control while keeping melodic accuracy. Furthermore, we explore various\nexperiment settings, including different types of text representations, text\nencoder fine-tuning, and introducing speech data to alleviate data scarcity,\naiming to facilitate further research. Experiments show that our model achieves\nfavorable controlling ability and audio quality. Audio samples are available at\nhttp://prompt-singer.github.io .\n","authors":["Yongqi Wang","Ruofan Hu","Rongjie Huang","Zhiqing Hong","Ruiqi Li","Wenrui Liu","Fuming You","Tao Jin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.11780v1.pdf","comment":"Accepted by NAACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2403.11778v1","updated":"2024-03-18T13:35:10Z","published":"2024-03-18T13:35:10Z","title":"Towards the Development of a Real-Time Deepfake Audio Detection System\n  in Communication Platforms","summary":"  Deepfake audio poses a rising threat in communication platforms,\nnecessitating real-time detection for audio stream integrity. Unlike\ntraditional non-real-time approaches, this study assesses the viability of\nemploying static deepfake audio detection models in real-time communication\nplatforms. An executable software is developed for cross-platform\ncompatibility, enabling real-time execution. Two deepfake audio detection\nmodels based on Resnet and LCNN architectures are implemented using the\nASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof\n2019 challenge baselines. The study proposes strategies and frameworks for\nenhancing these models, paving the way for real-time deepfake audio detection\nin communication platforms. This work contributes to the advancement of audio\nstream security, ensuring robust detection capabilities in dynamic, real-time\ncommunication scenarios.\n","authors":["Jonat John Mathew","Rakin Ahsan","Sae Furukawa","Jagdish Gautham Krishna Kumar","Huzaifa Pallan","Agamjeet Singh Padda","Sara Adamski","Madhu Reddiboina","Arjun Pankajakshan"],"pdf_url":"https://arxiv.org/pdf/2403.11778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11732v1","updated":"2024-03-18T12:42:11Z","published":"2024-03-18T12:42:11Z","title":"Hallucination in Perceptual Metric-Driven Speech Enhancement Networks","summary":"  Within the area of speech enhancement, there is an ongoing interest in the\ncreation of neural systems which explicitly aim to improve the perceptual\nquality of the processed audio. In concert with this is the topic of\nnon-intrusive (i.e. without clean reference) speech quality prediction, for\nwhich neural networks are trained to predict human-assigned quality labels\ndirectly from distorted audio. When combined, these areas allow for the\ncreation of powerful new speech enhancement systems which can leverage large\nreal-world datasets of distorted audio, by taking inference of a pre-trained\nspeech quality predictor as the sole loss function of the speech enhancement\nsystem. This paper aims to identify a potential pitfall with this approach,\nnamely hallucinations which are introduced by the enhancement system `tricking'\nthe speech quality predictor.\n","authors":["George Close","Thomas Hain","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2403.11732v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2302.02257v4","updated":"2024-03-18T11:39:29Z","published":"2023-02-04T23:18:36Z","title":"Multi-Source Diffusion Models for Simultaneous Music Generation and\n  Separation","summary":"  In this work, we define a diffusion-based generative model capable of both\nmusic synthesis and source separation by learning the score of the joint\nprobability density of sources sharing a context. Alongside the classic total\ninference tasks (i.e., generating a mixture, separating the sources), we also\nintroduce and experiment on the partial generation task of source imputation,\nwhere we generate a subset of the sources given the others (e.g., play a piano\ntrack that goes well with the drums). Additionally, we introduce a novel\ninference method for the separation task based on Dirac likelihood functions.\nWe train our model on Slakh2100, a standard dataset for musical source\nseparation, provide qualitative results in the generation settings, and\nshowcase competitive quantitative results in the source separation setting. Our\nmethod is the first example of a single model that can handle both generation\nand separation tasks, thus representing a step toward general audio models.\n","authors":["Giorgio Mariani","Irene Tallini","Emilian Postolache","Michele Mancusi","Luca Cosmo","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2302.02257v4.pdf","comment":"ICLR 2024 oral presentation. Demo page:\n  https://gladia-research-group.github.io/multi-source-diffusion-models/"},{"id":"http://arxiv.org/abs/2309.05472v2","updated":"2024-03-18T10:54:15Z","published":"2023-09-11T14:13:09Z","title":"LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for\n  Self-supervised Representations of French Speech","summary":"  Self-supervised learning (SSL) is at the origin of unprecedented improvements\nin many different domains including computer vision and natural language\nprocessing. Speech processing drastically benefitted from SSL as most of the\ncurrent domain-related tasks are now being approached with pre-trained models.\nThis work introduces LeBenchmark 2.0 an open-source framework for assessing and\nbuilding SSL-equipped French speech technologies. It includes documented,\nlarge-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous\nspeech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to\none billion learnable parameters shared with the community, and an evaluation\nprotocol made of six downstream tasks to complement existing benchmarks.\nLeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for\nspeech with the investigation of frozen versus fine-tuned downstream models,\ntask-agnostic versus task-specific pre-trained models as well as a discussion\non the carbon footprint of large-scale model training. Overall, the newly\nintroduced models trained on 14,000 hours of French speech outperform\nmultilingual and previous LeBenchmark SSL models across the benchmark but also\nrequired up to four times more energy for pre-training.\n","authors":["Titouan Parcollet","Ha Nguyen","Solene Evain","Marcely Zanon Boito","Adrien Pupier","Salima Mdhaffar","Hang Le","Sina Alisamir","Natalia Tomashenko","Marco Dinarelli","Shucong Zhang","Alexandre Allauzen","Maximin Coavoux","Yannick Esteve","Mickael Rouvier","Jerome Goulian","Benjamin Lecouteux","Francois Portet","Solange Rossato","Fabien Ringeval","Didier Schwab","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2309.05472v2.pdf","comment":"Published in Computer Science and Language. Preprint allowed"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.11706v1","updated":"2024-03-18T12:08:01Z","published":"2024-03-18T12:08:01Z","title":"Generalized Multi-Source Inference for Text Conditioned Music Diffusion\n  Models","summary":"  Multi-Source Diffusion Models (MSDM) allow for compositional musical\ngeneration tasks: generating a set of coherent sources, creating\naccompaniments, and performing source separation. Despite their versatility,\nthey require estimating the joint distribution over the sources, necessitating\npre-separated musical data, which is rarely available, and fixing the number\nand type of sources at training time. This paper generalizes MSDM to arbitrary\ntime-domain diffusion models conditioned on text embeddings. These models do\nnot require separated data as they are trained on mixtures, can parameterize an\narbitrary number of sources, and allow for rich semantic control. We propose an\ninference procedure enabling the coherent generation of sources and\naccompaniments. Additionally, we adapt the Dirac separator of MSDM to perform\nsource separation. We experiment with diffusion models trained on Slakh2100 and\nMTG-Jamendo, showcasing competitive generation and separation results in a\nrelaxed data setting.\n","authors":["Emilian Postolache","Giorgio Mariani","Luca Cosmo","Emmanouil Benetos","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2403.11706v1.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.11626v1","updated":"2024-03-18T09:58:43Z","published":"2024-03-18T09:58:43Z","title":"QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation","summary":"  The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.\n","authors":["Zhizhen Zhou","Yejing Huo","Guoheng Huang","An Zeng","Xuhang Chen","Lian Huang","Zinuo Li"],"pdf_url":"https://arxiv.org/pdf/2403.11626v1.pdf","comment":"Accepted by The Visual Computer Journal"},{"id":"http://arxiv.org/abs/2307.07218v3","updated":"2024-03-18T09:29:37Z","published":"2023-07-14T08:21:25Z","title":"Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis","summary":"  Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech\nprompts, which significantly reduces the data and computation requirements for\nvoice cloning by skipping the fine-tuning process. However, the prompting\nmechanisms of zero-shot TTS still face challenges in the following aspects: 1)\nprevious works of zero-shot TTS are typically trained with single-sentence\nprompts, which significantly restricts their performance when the data is\nrelatively sufficient during the inference stage. 2) The prosodic information\nin prompts is highly coupled with timbre, making it untransferable to each\nother. This paper introduces Mega-TTS 2, a generic prompting mechanism for\nzero-shot TTS, to tackle the aforementioned challenges. Specifically, we design\na powerful acoustic autoencoder that separately encodes the prosody and timbre\ninformation into the compressed latent space while providing high-quality\nreconstructions. Then, we propose a multi-reference timbre encoder and a\nprosody latent language model (P-LLM) to extract useful information from\nmulti-sentence prompts. We further leverage the probabilities derived from\nmultiple P-LLM outputs to produce transferable and controllable prosody.\nExperimental results demonstrate that Mega-TTS 2 could not only synthesize\nidentity-preserving speech with a short prompt of an unseen speaker from\narbitrary sources but consistently outperform the fine-tuning method when the\nvolume of data ranges from 10 seconds to 5 minutes. Furthermore, our method\nenables to transfer various speaking styles to the target timbre in a\nfine-grained and controlled manner. Audio samples can be found in\nhttps://boostprompt.github.io/boostprompt/.\n","authors":["Ziyue Jiang","Jinglin Liu","Yi Ren","Jinzheng He","Zhenhui Ye","Shengpeng Ji","Qian Yang","Chen Zhang","Pengfei Wei","Chunfeng Wang","Xiang Yin","Zejun Ma","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2307.07218v3.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.11578v1","updated":"2024-03-18T08:53:04Z","published":"2024-03-18T08:53:04Z","title":"AdaMER-CTC: Connectionist Temporal Classification with Adaptive Maximum\n  Entropy Regularization for Automatic Speech Recognition","summary":"  In Automatic Speech Recognition (ASR) systems, a recurring obstacle is the\ngeneration of narrowly focused output distributions. This phenomenon emerges as\na side effect of Connectionist Temporal Classification (CTC), a robust sequence\nlearning tool that utilizes dynamic programming for sequence mapping. While\nearlier efforts have tried to combine the CTC loss with an entropy maximization\nregularization term to mitigate this issue, they employed a constant weighting\nterm on the regularization during the training, which we find may not be\noptimal. In this work, we introduce Adaptive Maximum Entropy Regularization\n(AdaMER), a technique that can modulate the impact of entropy regularization\nthroughout the training process. This approach not only refines ASR model\ntraining but ensures that as training proceeds, predictions display the desired\nmodel confidence.\n","authors":["SooHwan Eom","Eunseop Yoon","Hee Suk Yoon","Chanwoo Kim","Mark Hasegawa-Johnson","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.11578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03018v3","updated":"2024-03-18T07:57:58Z","published":"2023-10-04T17:58:11Z","title":"Zero Resource Code-switched Speech Benchmark Using Speech Utterance\n  Pairs For Multiple Spoken Languages","summary":"  We introduce a new zero resource code-switched speech benchmark designed to\ndirectly assess the code-switching capabilities of self-supervised speech\nencoders. We showcase a baseline system of language modeling on discrete units\nto demonstrate how the code-switching abilities of speech encoders can be\nassessed in a zero-resource manner. Our experiments encompass a variety of\nwell-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We\nexamine the impact of pre-training languages and model size on benchmark\nperformance. Notably, though our results demonstrate that speech encoders with\nmultilingual pre-training, exemplified by XLSR, outperform monolingual variants\n(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial\nroom for improvement in their code-switching linguistic abilities.\n","authors":["Kuan-Po Huang","Chih-Kai Yang","Yu-Kuan Fu","Ewan Dunbar","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2310.03018v3.pdf","comment":"Accepted by ICASSP 2024 (v2)"},{"id":"http://arxiv.org/abs/2403.11508v1","updated":"2024-03-18T06:26:32Z","published":"2024-03-18T06:26:32Z","title":"Discriminative Neighborhood Smoothing for Generative Anomalous Sound\n  Detection","summary":"  We propose discriminative neighborhood smoothing of generative anomaly scores\nfor anomalous sound detection. While the discriminative approach is known to\nachieve better performance than generative approaches often, we have found that\nit sometimes causes significant performance degradation due to the discrepancy\nbetween the training and test data, making it less robust than the generative\napproach. Our proposed method aims to compensate for the disadvantages of\ngenerative and discriminative approaches by combining them. Generative anomaly\nscores are smoothed using multiple samples with similar discriminative features\nto improve the performance of the generative approach in an ensemble manner\nwhile keeping its robustness. Experimental results show that our proposed\nmethod greatly improves the original generative method, including absolute\nimprovement of 22% in AUC and robustly works, while a discriminative method\nsuffers from the discrepancy.\n","authors":["Takuya Fujimura","Keisuke Imoto","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2403.11508v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2401.13463v2","updated":"2024-03-18T06:08:31Z","published":"2024-01-24T14:08:38Z","title":"SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken\n  Question Answering","summary":"  Spoken Question Answering (SQA) is essential for machines to reply to user's\nquestion by finding the answer span within a given spoken passage. SQA has been\npreviously achieved without ASR to avoid recognition errors and\nOut-of-Vocabulary (OOV) problems. However, the real-world problem of\nOpen-domain SQA (openSQA), in which the machine needs to first retrieve\npassages that possibly contain the answer from a spoken archive in addition,\nwas never considered. This paper proposes the first known end-to-end framework,\nSpeech Dense Passage Retriever (SpeechDPR), for the retrieval component of the\nopenSQA problem. SpeechDPR learns a sentence-level semantic representation by\ndistilling knowledge from the cascading model of unsupervised ASR (UASR) and\ntext dense retriever (TDR). No manually transcribed speech data is needed.\nInitial experiments showed performance comparable to the cascading model of\nUASR and TDR, and significantly better when UASR was poor, verifying this\napproach is more robust to speech recognition errors.\n","authors":["Chyi-Jiunn Lin","Guan-Ting Lin","Yung-Sung Chuang","Wei-Lun Wu","Shang-Wen Li","Abdelrahman Mohamed","Hung-yi Lee","Lin-shan Lee"],"pdf_url":"https://arxiv.org/pdf/2401.13463v2.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2210.15563v3","updated":"2024-03-18T05:55:34Z","published":"2022-10-27T15:53:38Z","title":"Multimodal Transformer Distillation for Audio-Visual Synchronization","summary":"  Audio-visual synchronization aims to determine whether the mouth movements\nand speech in the video are synchronized. VocaLiST reaches state-of-the-art\nperformance by incorporating multimodal Transformers to model audio-visual\ninteract information. However, it requires high computing resources, making it\nimpractical for real-world applications. This paper proposed an MTDVocaLiST\nmodel, which is trained by our proposed multimodal Transformer distillation\n(MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the\ncross-attention distribution and value-relation in the Transformer of VocaLiST.\nAdditionally, we harness uncertainty weighting to fully exploit the interaction\ninformation across all layers. Our proposed method is effective in two aspects:\nFrom the distillation method perspective, MTD loss outperforms other strong\ndistillation baselines. From the distilled model's performance perspective: 1)\nMTDVocaLiST outperforms similar-size SOTA models, SyncNet, and Perfect Match\nmodels by 15.65% and 3.35%; 2) MTDVocaLiST reduces the model size of VocaLiST\nby 83.52%, yet still maintaining similar performance.\n","authors":["Xuanjun Chen","Haibin Wu","Chung-Che Wang","Hung-yi Lee","Jyh-Shing Roger Jang"],"pdf_url":"https://arxiv.org/pdf/2210.15563v3.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.14016v2","updated":"2024-03-18T05:03:14Z","published":"2023-10-21T13:56:23Z","title":"SwG-former: A Sliding-Window Graph Convolutional Network for\n  Simultaneous Spatial-Temporal Information Extraction in Sound Event\n  Localization and Detection","summary":"  Sound event localization and detection (SELD) involves sound event detection\n(SED) and direction of arrival (DoA) estimation tasks. SED mainly relies on\ntemporal dependencies to distinguish different sound classes, while DoA\nestimation depends on spatial correlations to estimate source directions. This\npaper addresses the need to simultaneously extract spatial-temporal information\nin audio signals to improve SELD performance. A novel block, the sliding-window\ngraph-former (SwG-former), is designed to learn temporal context information of\nsound events based on their spatial correlations. The SwG-former block\ntransforms audio signals into a graph representation and constructs graph\nvertices to capture higher abstraction levels for spatial correlations. It uses\ndifferent-sized sliding windows to adapt various sound event durations and\naggregates temporal features with similar spatial information while\nincorporating multi-head self-attention (MHSA) to model global information.\nFurthermore, as the cornerstone of message passing, a robust Conv2dAgg function\nis proposed and embedded into the block to aggregate the features of neighbor\nvertices. As a result, a SwG-former model, which stacks the SwG-former blocks,\ndemonstrates superior performance compared to recent advanced SELD models. The\nSwG-former block is also integrated into the event-independent network version\n2 (EINV2), called SwG-EINV2, which surpasses the state-of-the-art (SOTA)\nmethods under the same acoustic environment.\n","authors":["Weiming Huang","Qinghua Huang","Liyan Ma","Chuan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13451v2","updated":"2024-03-18T01:09:44Z","published":"2023-02-27T00:44:22Z","title":"A low latency attention module for streaming self-supervised speech\n  representation learning","summary":"  The transformer is a fundamental building block in deep learning, and the\nattention mechanism is the transformer's core component. Self-supervised speech\nrepresentation learning (SSRL) represents a popular use-case for the\ntransformer architecture. Due to transformers' acausal behavior, the use of\ntransformers for SSRL has been predominantly focused on acausal applications.\nHowever, several media processing problems, such as speech processing, require\nreal-time solutions. In this paper, we present an implementation of the\nattention module that enables training of SSRL architectures with low compute\nand memory requirements, while allowing real-time inference with low and fixed\nlatency. The attention module proposed in this paper includes two components,\nstreaming attention (SA) and low-latency streaming attention (LLSA). The SA\nrepresents our proposal for an efficient streaming SSRL implementation, while\nthe LLSA solves the latency build-up problem of other streaming attention\narchitectures, such as the masked acausal attention (MAA), guaranteeing a\nlatency equal to one layer even when multiple layers are stacked. We present a\ncomparative analysis between the vanilla attention, which we will refer here as\nacausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with\nautomatic speech recognition as downstream task. When training on\nlibrispeech-clean-100 and testing on librispeech-test-clean, our low-latency\nattention module has a word error rate (WER) of 5.84%, which represents a\nsignificant improvement over the MAA (WER = 13.82%). Our implementation also\nreduces the inference latency from 1.92 to 0.16 seconds. The proposed\nlow-latency module preserves many of the benefits of conventional acausal\ntransformers, but also enables latency characteristics that make it applicable\nto real-time streaming applications.\n","authors":["Jianbo Ma","Siqi Pan","Deepak Chandran","Andrea Fanelli","Richard Cartwright"],"pdf_url":"https://arxiv.org/pdf/2302.13451v2.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.12258v1","updated":"2024-03-18T21:14:18Z","published":"2024-03-18T21:14:18Z","title":"A Multi-loudspeaker Binaural Room Impulse Response Dataset with\n  High-Resolution Translational and Rotational Head Coordinates in a Listening\n  Room","summary":"  Data report for the 3D3A Lab Binaural Room Impulse Response (BRIR) Dataset\n(https://doi.org/10.34770/6gc9-5787).\n","authors":["Yue Qiao","Ryan Miguel Gonzales","Edgar Choueiri"],"pdf_url":"https://arxiv.org/pdf/2403.12258v1.pdf","comment":"Submitted to Frontiers in Signal Processing"},{"id":"http://arxiv.org/abs/2403.12182v1","updated":"2024-03-18T18:55:37Z","published":"2024-03-18T18:55:37Z","title":"Latent CLAP Loss for Better Foley Sound Synthesis","summary":"  Foley sound generation, the art of creating audio for multimedia, has\nrecently seen notable advancements through text-conditioned latent diffusion\nmodels. These systems use multimodal text-audio representation models, such as\nContrastive Language-Audio Pretraining (CLAP), whose objective is to map\ncorresponding audio and text prompts into a joint embedding space. AudioLDM, a\ntext-to-audio model, was the winner of the DCASE2023 task 7 Foley sound\nsynthesis challenge. The winning system fine-tuned the model for specific audio\nclasses and applied a post-filtering method using CLAP similarity scores\nbetween output audio and input text at inference time, requiring the generation\nof extra samples, thus reducing data generation efficiency. We introduce a new\nloss term to enhance Foley sound generation in AudioLDM without post-filtering.\nThis loss term uses a new module based on the CLAP mode-Latent CLAP encode-to\nalign the latent diffusion output with real audio in a shared CLAP embedding\nspace. Our experiments demonstrate that our method effectively reduces the\nFrechet Audio Distance (FAD) score of the generated audio and eliminates the\nneed for post-filtering, thus enhancing generation efficiency.\n","authors":["Tornike Karchkhadze","Hassan Salami Kavaki","Mohammad Rasool Izadi","Bryce Irvin","Mikolaj Kegler","Ari Hertz","Shuo Zhang","Marko Stamenovic"],"pdf_url":"https://arxiv.org/pdf/2403.12182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12000v1","updated":"2024-03-18T17:35:02Z","published":"2024-03-18T17:35:02Z","title":"Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance","summary":"  Deep learning-based probabilistic models of musical data are producing\nincreasingly realistic results and promise to enter creative workflows of many\nkinds. Yet they have been little-studied in a performance setting, where the\nresults of user actions typically ought to feel instantaneous. To enable such\nstudy, we designed Notochord, a deep probabilistic model for sequences of\nstructured events, and trained an instance of it on the Lakh MIDI dataset. Our\nprobabilistic formulation allows interpretable interventions at a sub-event\nlevel, which enables one model to act as a backbone for diverse interactive\nmusical functions including steerable generation, harmonization, machine\nimprovisation, and likelihood-based interfaces. Notochord can generate\npolyphonic and multi-track MIDI, and respond to inputs with latency below ten\nmilliseconds. Training code, model checkpoints and interactive examples are\nprovided as open source software.\n","authors":["Victor Shepardson","Jack Armitage","Thor Magnusson"],"pdf_url":"https://arxiv.org/pdf/2403.12000v1.pdf","comment":"12 pages, 6 figures. Proceedings of the 3rd Conference on AI Music\n  Creativity (2022, September 17)"},{"id":"http://arxiv.org/abs/2403.11879v1","updated":"2024-03-18T15:32:02Z","published":"2024-03-18T15:32:02Z","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton","summary":"  In this study, we propose a methodology for the Emotional Mimicry Intensity\n(EMI) Estimation task within the context of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0\nframework, pre-trained on a comprehensive podcast dataset, to extract a broad\nrange of audio features encompassing both linguistic and paralinguistic\nelements. We enhance feature representation through a fusion technique that\nintegrates individual features with a global mean vector, introducing global\ncontextual insights into our analysis. Additionally, we incorporate a\npre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.\nOur fusion employs a Long Short-Term Memory (LSTM) architecture for efficient\ntemporal analysis of audio data. Utilizing only the provided audio data, our\napproach demonstrates significant improvements over the established baseline.\n","authors":["Tobias Hallmen","Fabian Deuser","Norbert Oswald","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2403.11879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11827v1","updated":"2024-03-18T14:34:16Z","published":"2024-03-18T14:34:16Z","title":"Sound Event Detection and Localization with Distance Estimation","summary":"  Sound Event Detection and Localization (SELD) is a combined task of\nidentifying sound events and their corresponding direction-of-arrival (DOA).\nWhile this task has numerous applications and has been extensively researched\nin recent years, it fails to provide full information about the sound source\nposition. In this paper, we overcome this problem by extending the task to\nSound Event Detection, Localization with Distance Estimation (3D SELD). We\nstudy two ways of integrating distance estimation within the SELD core - a\nmulti-task approach, in which the problem is tackled by a separate model\noutput, and a single-task approach obtained by extending the multi-ACCDOA\nmethod to include distance information. We investigate both methods for the\nAmbisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial\nSoundscapes 2023. Moreover, our study involves experiments on the loss function\nrelated to the distance estimation part. Our results show that it is possible\nto perform 3D SELD without any degradation of performance in sound event\ndetection and DOA estimation.\n","authors":["Daniel Aleksander Krause","Archontis Politis","Annamaria Mesaros"],"pdf_url":"https://arxiv.org/pdf/2403.11827v1.pdf","comment":"This paper has been submitted for the 32nd European Signal Processing\n  Conference EUSIPCO 2024 in Lyon"},{"id":"http://arxiv.org/abs/2403.11780v1","updated":"2024-03-18T13:39:05Z","published":"2024-03-18T13:39:05Z","title":"Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural\n  Language Prompt","summary":"  Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio\nquality and naturalness, yet they lack the capability to control the style\nattributes of the synthesized singing explicitly. We propose Prompt-Singer, the\nfirst SVS method that enables attribute controlling on singer gender, vocal\nrange and volume with natural language. We adopt a model architecture based on\na decoder-only transformer with a multi-scale hierarchy, and design a\nrange-melody decoupled pitch representation that enables text-conditioned vocal\nrange control while keeping melodic accuracy. Furthermore, we explore various\nexperiment settings, including different types of text representations, text\nencoder fine-tuning, and introducing speech data to alleviate data scarcity,\naiming to facilitate further research. Experiments show that our model achieves\nfavorable controlling ability and audio quality. Audio samples are available at\nhttp://prompt-singer.github.io .\n","authors":["Yongqi Wang","Ruofan Hu","Rongjie Huang","Zhiqing Hong","Ruiqi Li","Wenrui Liu","Fuming You","Tao Jin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.11780v1.pdf","comment":"Accepted by NAACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2403.11778v1","updated":"2024-03-18T13:35:10Z","published":"2024-03-18T13:35:10Z","title":"Towards the Development of a Real-Time Deepfake Audio Detection System\n  in Communication Platforms","summary":"  Deepfake audio poses a rising threat in communication platforms,\nnecessitating real-time detection for audio stream integrity. Unlike\ntraditional non-real-time approaches, this study assesses the viability of\nemploying static deepfake audio detection models in real-time communication\nplatforms. An executable software is developed for cross-platform\ncompatibility, enabling real-time execution. Two deepfake audio detection\nmodels based on Resnet and LCNN architectures are implemented using the\nASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof\n2019 challenge baselines. The study proposes strategies and frameworks for\nenhancing these models, paving the way for real-time deepfake audio detection\nin communication platforms. This work contributes to the advancement of audio\nstream security, ensuring robust detection capabilities in dynamic, real-time\ncommunication scenarios.\n","authors":["Jonat John Mathew","Rakin Ahsan","Sae Furukawa","Jagdish Gautham Krishna Kumar","Huzaifa Pallan","Agamjeet Singh Padda","Sara Adamski","Madhu Reddiboina","Arjun Pankajakshan"],"pdf_url":"https://arxiv.org/pdf/2403.11778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11732v1","updated":"2024-03-18T12:42:11Z","published":"2024-03-18T12:42:11Z","title":"Hallucination in Perceptual Metric-Driven Speech Enhancement Networks","summary":"  Within the area of speech enhancement, there is an ongoing interest in the\ncreation of neural systems which explicitly aim to improve the perceptual\nquality of the processed audio. In concert with this is the topic of\nnon-intrusive (i.e. without clean reference) speech quality prediction, for\nwhich neural networks are trained to predict human-assigned quality labels\ndirectly from distorted audio. When combined, these areas allow for the\ncreation of powerful new speech enhancement systems which can leverage large\nreal-world datasets of distorted audio, by taking inference of a pre-trained\nspeech quality predictor as the sole loss function of the speech enhancement\nsystem. This paper aims to identify a potential pitfall with this approach,\nnamely hallucinations which are introduced by the enhancement system `tricking'\nthe speech quality predictor.\n","authors":["George Close","Thomas Hain","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2403.11732v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2302.02257v4","updated":"2024-03-18T11:39:29Z","published":"2023-02-04T23:18:36Z","title":"Multi-Source Diffusion Models for Simultaneous Music Generation and\n  Separation","summary":"  In this work, we define a diffusion-based generative model capable of both\nmusic synthesis and source separation by learning the score of the joint\nprobability density of sources sharing a context. Alongside the classic total\ninference tasks (i.e., generating a mixture, separating the sources), we also\nintroduce and experiment on the partial generation task of source imputation,\nwhere we generate a subset of the sources given the others (e.g., play a piano\ntrack that goes well with the drums). Additionally, we introduce a novel\ninference method for the separation task based on Dirac likelihood functions.\nWe train our model on Slakh2100, a standard dataset for musical source\nseparation, provide qualitative results in the generation settings, and\nshowcase competitive quantitative results in the source separation setting. Our\nmethod is the first example of a single model that can handle both generation\nand separation tasks, thus representing a step toward general audio models.\n","authors":["Giorgio Mariani","Irene Tallini","Emilian Postolache","Michele Mancusi","Luca Cosmo","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2302.02257v4.pdf","comment":"ICLR 2024 oral presentation. Demo page:\n  https://gladia-research-group.github.io/multi-source-diffusion-models/"},{"id":"http://arxiv.org/abs/2309.05472v2","updated":"2024-03-18T10:54:15Z","published":"2023-09-11T14:13:09Z","title":"LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for\n  Self-supervised Representations of French Speech","summary":"  Self-supervised learning (SSL) is at the origin of unprecedented improvements\nin many different domains including computer vision and natural language\nprocessing. Speech processing drastically benefitted from SSL as most of the\ncurrent domain-related tasks are now being approached with pre-trained models.\nThis work introduces LeBenchmark 2.0 an open-source framework for assessing and\nbuilding SSL-equipped French speech technologies. It includes documented,\nlarge-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous\nspeech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to\none billion learnable parameters shared with the community, and an evaluation\nprotocol made of six downstream tasks to complement existing benchmarks.\nLeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for\nspeech with the investigation of frozen versus fine-tuned downstream models,\ntask-agnostic versus task-specific pre-trained models as well as a discussion\non the carbon footprint of large-scale model training. Overall, the newly\nintroduced models trained on 14,000 hours of French speech outperform\nmultilingual and previous LeBenchmark SSL models across the benchmark but also\nrequired up to four times more energy for pre-training.\n","authors":["Titouan Parcollet","Ha Nguyen","Solene Evain","Marcely Zanon Boito","Adrien Pupier","Salima Mdhaffar","Hang Le","Sina Alisamir","Natalia Tomashenko","Marco Dinarelli","Shucong Zhang","Alexandre Allauzen","Maximin Coavoux","Yannick Esteve","Mickael Rouvier","Jerome Goulian","Benjamin Lecouteux","Francois Portet","Solange Rossato","Fabien Ringeval","Didier Schwab","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2309.05472v2.pdf","comment":"Published in Computer Science and Language. Preprint allowed"}]},"2024-03-17T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2312.08459v2","updated":"2024-03-17T23:45:01Z","published":"2023-12-13T19:01:07Z","title":"FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head\n  Models","summary":"  We introduce FaceTalk, a novel generative approach designed for synthesizing\nhigh-fidelity 3D motion sequences of talking human heads from input audio\nsignal. To capture the expressive, detailed nature of human heads, including\nhair, ears, and finer-scale eye movements, we propose to couple speech signal\nwith the latent space of neural parametric head models to create high-fidelity,\ntemporally coherent motion sequences. We propose a new latent diffusion model\nfor this task, operating in the expression space of neural parametric head\nmodels, to synthesize audio-driven realistic head sequences. In the absence of\na dataset with corresponding NPHM expressions to audio, we optimize for these\ncorrespondences to produce a dataset of temporally-optimized NPHM expressions\nfit to audio-video recordings of people talking. To the best of our knowledge,\nthis is the first work to propose a generative approach for realistic and\nhigh-quality motion synthesis of volumetric human heads, representing a\nsignificant advancement in the field of audio-driven 3D animation. Notably, our\napproach stands out in its ability to generate plausible motion sequences that\ncan produce high-fidelity head animation coupled with the NPHM shape space. Our\nexperimental results substantiate the effectiveness of FaceTalk, consistently\nachieving superior and visually natural motion, encompassing diverse facial\nexpressions and styles, outperforming existing methods by 75% in perceptual\nuser study evaluation.\n","authors":["Shivangi Aneja","Justus Thies","Angela Dai","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.08459v2.pdf","comment":"Paper Video: https://youtu.be/7Jf0kawrA3Q Project Page:\n  https://shivangi-aneja.github.io/projects/facetalk/"},{"id":"http://arxiv.org/abs/2403.08164v2","updated":"2024-03-17T10:06:12Z","published":"2024-03-13T01:27:57Z","title":"EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight\n  Text-to-Speech","summary":"  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved\nhigh-quality speech synthesis results. Recurrent neural networks have become a\nstandard modeling technique for sequential data in TTS systems and are widely\nused. However, training a TTS model which includes RNN components requires\npowerful GPU performance and takes a long time. In contrast, CNN-based sequence\nsynthesis techniques can significantly reduce the parameters and training time\nof a TTS model while guaranteeing a certain performance due to their high\nparallelism, which alleviate these economic costs of training. In this paper,\nwe propose a lightweight TTS system based on deep convolutional neural\nnetworks, which is a two-stage training end-to-end TTS model and does not\nemploy any recurrent units. Our model consists of two stages: Text2Spectrum and\nSSRN. The former is used to encode phonemes into a coarse mel spectrogram and\nthe latter is used to synthesize the complete spectrum from the coarse mel\nspectrogram. Meanwhile, we improve the robustness of our model by a series of\ndata augmentations, such as noise suppression, time warping, frequency masking\nand time masking, for solving the low resource mongolian problem. Experiments\nshow that our model can reduce the training time and parameters while ensuring\nthe quality and naturalness of the synthesized speech compared to using\nmainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for\nvalidation, which significantly reduces training time while maintaining a\ncertain accuracy.\n","authors":["Ziqi Liang","Haoxiang Shi","Jiawei Wang","Keda Lu"],"pdf_url":"https://arxiv.org/pdf/2403.08164v2.pdf","comment":"Accepted by the 27th IEEE International Conference on Computer\n  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:\n  substantial text overlap with arXiv:2211.01948"},{"id":"http://arxiv.org/abs/2403.11091v1","updated":"2024-03-17T05:00:40Z","published":"2024-03-17T05:00:40Z","title":"Multitask frame-level learning for few-shot sound event detection","summary":"  This paper focuses on few-shot Sound Event Detection (SED), which aims to\nautomatically recognize and classify sound events with limited samples.\nHowever, prevailing methods methods in few-shot SED predominantly rely on\nsegment-level predictions, which often providing detailed, fine-grained\npredictions, particularly for events of brief duration. Although frame-level\nprediction strategies have been proposed to overcome these limitations, these\nstrategies commonly face difficulties with prediction truncation caused by\nbackground noise. To alleviate this issue, we introduces an innovative\nmultitask frame-level SED framework. In addition, we introduce TimeFilterAug, a\nlinear timing mask for data augmentation, to increase the model's robustness\nand adaptability to diverse acoustic environments. The proposed method achieves\na F-score of 63.8%, securing the 1st rank in the few-shot bioacoustic event\ndetection category of the Detection and Classification of Acoustic Scenes and\nEvents Challenge 2023.\n","authors":["Liang Zou","Genwei Yan","Ruoyu Wang","Jun Du","Meng Lei","Tian Gao","Xin Fang"],"pdf_url":"https://arxiv.org/pdf/2403.11091v1.pdf","comment":"6 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2403.11074v1","updated":"2024-03-17T03:45:14Z","published":"2024-03-17T03:45:14Z","title":"Audio-Visual Segmentation via Unlabeled Frame Exploitation","summary":"  Audio-visual segmentation (AVS) aims to segment the sounding objects in video\nframes. Although great progress has been witnessed, we experimentally reveal\nthat current methods reach marginal performance gain within the use of the\nunlabeled frames, leading to the underutilization issue. To fully explore the\npotential of the unlabeled frames for AVS, we explicitly divide them into two\ncategories based on their temporal characteristics, i.e., neighboring frame\n(NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame,\noften contain rich motion information that assists in the accurate localization\nof sounding objects. Contrary to NFs, DFs have long temporal distances from the\nlabeled frame, which share semantic-similar objects with appearance variations.\nConsidering their unique characteristics, we propose a versatile framework that\neffectively leverages them to tackle AVS. Specifically, for NFs, we exploit the\nmotion cues as the dynamic guidance to improve the objectness localization.\nBesides, we exploit the semantic cues in DFs by treating them as valid\naugmentations to the labeled frames, which are then used to enrich data\ndiversity in a self-training manner. Extensive experimental results demonstrate\nthe versatility and superiority of our method, unleashing the power of the\nabundant unlabeled frames.\n","authors":["Jinxiang Liu","Yikun Liu","Fei Zhang","Chen Ju","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11074v1.pdf","comment":"Accepted by CVPR 2024"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2312.08459v2","updated":"2024-03-17T23:45:01Z","published":"2023-12-13T19:01:07Z","title":"FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head\n  Models","summary":"  We introduce FaceTalk, a novel generative approach designed for synthesizing\nhigh-fidelity 3D motion sequences of talking human heads from input audio\nsignal. To capture the expressive, detailed nature of human heads, including\nhair, ears, and finer-scale eye movements, we propose to couple speech signal\nwith the latent space of neural parametric head models to create high-fidelity,\ntemporally coherent motion sequences. We propose a new latent diffusion model\nfor this task, operating in the expression space of neural parametric head\nmodels, to synthesize audio-driven realistic head sequences. In the absence of\na dataset with corresponding NPHM expressions to audio, we optimize for these\ncorrespondences to produce a dataset of temporally-optimized NPHM expressions\nfit to audio-video recordings of people talking. To the best of our knowledge,\nthis is the first work to propose a generative approach for realistic and\nhigh-quality motion synthesis of volumetric human heads, representing a\nsignificant advancement in the field of audio-driven 3D animation. Notably, our\napproach stands out in its ability to generate plausible motion sequences that\ncan produce high-fidelity head animation coupled with the NPHM shape space. Our\nexperimental results substantiate the effectiveness of FaceTalk, consistently\nachieving superior and visually natural motion, encompassing diverse facial\nexpressions and styles, outperforming existing methods by 75% in perceptual\nuser study evaluation.\n","authors":["Shivangi Aneja","Justus Thies","Angela Dai","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.08459v2.pdf","comment":"Paper Video: https://youtu.be/7Jf0kawrA3Q Project Page:\n  https://shivangi-aneja.github.io/projects/facetalk/"},{"id":"http://arxiv.org/abs/2403.08164v2","updated":"2024-03-17T10:06:12Z","published":"2024-03-13T01:27:57Z","title":"EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight\n  Text-to-Speech","summary":"  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved\nhigh-quality speech synthesis results. Recurrent neural networks have become a\nstandard modeling technique for sequential data in TTS systems and are widely\nused. However, training a TTS model which includes RNN components requires\npowerful GPU performance and takes a long time. In contrast, CNN-based sequence\nsynthesis techniques can significantly reduce the parameters and training time\nof a TTS model while guaranteeing a certain performance due to their high\nparallelism, which alleviate these economic costs of training. In this paper,\nwe propose a lightweight TTS system based on deep convolutional neural\nnetworks, which is a two-stage training end-to-end TTS model and does not\nemploy any recurrent units. Our model consists of two stages: Text2Spectrum and\nSSRN. The former is used to encode phonemes into a coarse mel spectrogram and\nthe latter is used to synthesize the complete spectrum from the coarse mel\nspectrogram. Meanwhile, we improve the robustness of our model by a series of\ndata augmentations, such as noise suppression, time warping, frequency masking\nand time masking, for solving the low resource mongolian problem. Experiments\nshow that our model can reduce the training time and parameters while ensuring\nthe quality and naturalness of the synthesized speech compared to using\nmainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for\nvalidation, which significantly reduces training time while maintaining a\ncertain accuracy.\n","authors":["Ziqi Liang","Haoxiang Shi","Jiawei Wang","Keda Lu"],"pdf_url":"https://arxiv.org/pdf/2403.08164v2.pdf","comment":"Accepted by the 27th IEEE International Conference on Computer\n  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:\n  substantial text overlap with arXiv:2211.01948"},{"id":"http://arxiv.org/abs/2403.11091v1","updated":"2024-03-17T05:00:40Z","published":"2024-03-17T05:00:40Z","title":"Multitask frame-level learning for few-shot sound event detection","summary":"  This paper focuses on few-shot Sound Event Detection (SED), which aims to\nautomatically recognize and classify sound events with limited samples.\nHowever, prevailing methods methods in few-shot SED predominantly rely on\nsegment-level predictions, which often providing detailed, fine-grained\npredictions, particularly for events of brief duration. Although frame-level\nprediction strategies have been proposed to overcome these limitations, these\nstrategies commonly face difficulties with prediction truncation caused by\nbackground noise. To alleviate this issue, we introduces an innovative\nmultitask frame-level SED framework. In addition, we introduce TimeFilterAug, a\nlinear timing mask for data augmentation, to increase the model's robustness\nand adaptability to diverse acoustic environments. The proposed method achieves\na F-score of 63.8%, securing the 1st rank in the few-shot bioacoustic event\ndetection category of the Detection and Classification of Acoustic Scenes and\nEvents Challenge 2023.\n","authors":["Liang Zou","Genwei Yan","Ruoyu Wang","Jun Du","Meng Lei","Tian Gao","Xin Fang"],"pdf_url":"https://arxiv.org/pdf/2403.11091v1.pdf","comment":"6 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2403.11074v1","updated":"2024-03-17T03:45:14Z","published":"2024-03-17T03:45:14Z","title":"Audio-Visual Segmentation via Unlabeled Frame Exploitation","summary":"  Audio-visual segmentation (AVS) aims to segment the sounding objects in video\nframes. Although great progress has been witnessed, we experimentally reveal\nthat current methods reach marginal performance gain within the use of the\nunlabeled frames, leading to the underutilization issue. To fully explore the\npotential of the unlabeled frames for AVS, we explicitly divide them into two\ncategories based on their temporal characteristics, i.e., neighboring frame\n(NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame,\noften contain rich motion information that assists in the accurate localization\nof sounding objects. Contrary to NFs, DFs have long temporal distances from the\nlabeled frame, which share semantic-similar objects with appearance variations.\nConsidering their unique characteristics, we propose a versatile framework that\neffectively leverages them to tackle AVS. Specifically, for NFs, we exploit the\nmotion cues as the dynamic guidance to improve the objectness localization.\nBesides, we exploit the semantic cues in DFs by treating them as valid\naugmentations to the labeled frames, which are then used to enrich data\ndiversity in a self-training manner. Extensive experimental results demonstrate\nthe versatility and superiority of our method, unleashing the power of the\nabundant unlabeled frames.\n","authors":["Jinxiang Liu","Yikun Liu","Fei Zhang","Chen Ju","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11074v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15442v1","updated":"2024-03-17T11:28:23Z","published":"2024-03-17T11:28:23Z","title":"Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review\n  of Healthcare Strategies, Challenges, and Perspectives","summary":"  Automatic speech recognition (ASR) plays a pivotal role in our daily lives,\noffering utility not only for interacting with machines but also for\nfacilitating communication for individuals with either partial or profound\nhearing impairments. The process involves receiving the speech signal in\nanalogue form, followed by various signal processing algorithms to make it\ncompatible with devices of limited capacity, such as cochlear implants (CIs).\nUnfortunately, these implants, equipped with a finite number of electrodes,\noften result in speech distortion during synthesis. Despite efforts by\nresearchers to enhance received speech quality using various state-of-the-art\nsignal processing techniques, challenges persist, especially in scenarios\ninvolving multiple sources of speech, environmental noise, and other\ncircumstances. The advent of new artificial intelligence (AI) methods has\nushered in cutting-edge strategies to address the limitations and difficulties\nassociated with traditional signal processing techniques dedicated to CIs. This\nreview aims to comprehensively review advancements in CI-based ASR and speech\nenhancement, among other related aspects. The primary objective is to provide a\nthorough overview of metrics and datasets, exploring the capabilities of AI\nalgorithms in this biomedical field, summarizing and commenting on the best\nresults obtained. Additionally, the review will delve into potential\napplications and suggest future directions to bridge existing research gaps in\nthis domain.\n","authors":["Billel Essaid","Hamza Kheddar","Noureddine Batel","Abderrahmane Lakas","Muhammad E. H. Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2403.15442v1.pdf","comment":null}]},"2024-03-16T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.11037v1","updated":"2024-03-16T22:51:02Z","published":"2024-03-16T22:51:02Z","title":"Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals","summary":"  Sound event detection (SED) is an active area of audio research that aims to\ndetect the temporal occurrence of sounds. In this paper, we apply SED to engine\nfault detection by introducing a multimodal SED framework that detects\nfine-grained engine faults of automobile engines using audio and\naccelerometer-recorded vibration. We first introduce the problem of engine\nfault SED on a dataset collected from a large variety of vehicles with\nexpertly-labeled engine fault sound events. Next, we propose a SED model to\ntemporally detect ten fine-grained engine faults that occur within vehicle\nengines and further explore a pretraining strategy using a large-scale\nweakly-labeled engine fault dataset. Through multiple evaluations, we show our\nproposed framework is able to effectively detect engine fault sound events.\nFinally, we investigate the interaction and characteristics of each modality\nand show that fusing features from audio and vibration improves overall engine\nfault SED capabilities.\n","authors":["Dennis Fedorishin","Livio Forte III","Philip Schneider","Srirangaraj Setlur","Venu Govindaraju"],"pdf_url":"https://arxiv.org/pdf/2403.11037v1.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.07183v2","updated":"2024-03-16T19:00:10Z","published":"2023-09-12T23:54:00Z","title":"Respiratory Disease Classification and Biometric Analysis Using\n  Biosignals from Digital Stethoscopes","summary":"  Respiratory diseases remain a leading cause of mortality worldwide,\nhighlighting the need for faster and more accurate diagnostic tools. This work\npresents a novel approach leveraging digital stethoscope technology for\nautomatic respiratory disease classification and biometric analysis. Our\napproach has the potential to significantly enhance traditional auscultation\npractices. By leveraging one of the largest publicly available medical database\nof respiratory sounds, we train machine learning models to classify various\nrespiratory health conditions. Our method differs from conventional methods by\nusing Empirical Mode Decomposition (EMD) and spectral analysis techniques to\nisolate clinically relevant biosignals embedded within acoustic data captured\nby digital stethoscopes. This approach focuses on information closely tied to\ncardiovascular and respiratory patterns within the acoustic data. Spectral\nanalysis and filtering techniques isolate Intrinsic Mode Functions (IMFs)\nstrongly correlated with these physiological phenomena. These biosignals\nundergo a comprehensive feature extraction process for predictive modeling.\nThese features then serve as input to train several machine learning models for\nboth classification and regression tasks. Our approach achieves high accuracy\nin both binary classification (89% balanced accuracy for healthy vs. diseased)\nand multi-class classification (72% balanced accuracy for specific diseases\nlike pneumonia and COPD). For the first time, this work introduces regression\nmodels capable of estimating age and body mass index (BMI) based solely on\nacoustic data, as well as a model for sex classification. Our findings\nunderscore the potential of intelligent digital stethoscopes to significantly\nenhance assistive and remote diagnostic capabilities, contributing to\nadvancements in digital health, telehealth, and remote patient monitoring.\n","authors":["Constantino Álvarez Casado","Manuel Lage Cañellas","Matteo Pedone","Xiaoting Wu","Le Nguyen","Miguel Bordallo López"],"pdf_url":"https://arxiv.org/pdf/2309.07183v2.pdf","comment":"5 pages, 2 figures, 3 tables, Conference paper"},{"id":"http://arxiv.org/abs/2403.10961v1","updated":"2024-03-16T16:16:31Z","published":"2024-03-16T16:16:31Z","title":"Energy-Based Models with Applications to Speech and Language Processing","summary":"  Energy-Based Models (EBMs) are an important class of probabilistic models,\nalso known as random fields and undirected graphical models. EBMs are\nun-normalized and thus radically different from other popular self-normalized\nprobabilistic models such as hidden Markov models (HMMs), autoregressive\nmodels, generative adversarial nets (GANs) and variational auto-encoders\n(VAEs). Over the past years, EBMs have attracted increasing interest not only\nfrom the core machine learning community, but also from application domains\nsuch as speech, vision, natural language processing (NLP) and so on, due to\nsignificant theoretical and algorithmic progress. The sequential nature of\nspeech and language also presents special challenges and needs a different\ntreatment from processing fix-dimensional data (e.g., images). Therefore, the\npurpose of this monograph is to present a systematic introduction to\nenergy-based models, including both algorithmic progress and applications in\nspeech and language processing. First, the basics of EBMs are introduced,\nincluding classic models, recent models parameterized by neural networks,\nsampling methods, and various learning methods from the classic learning\nalgorithms to the most advanced ones. Then, the application of EBMs in three\ndifferent scenarios is presented, i.e., for modeling marginal, conditional and\njoint distributions, respectively. 1) EBMs for sequential data with\napplications in language modeling, where the main focus is on the marginal\ndistribution of a sequence itself; 2) EBMs for modeling conditional\ndistributions of target sequences given observation sequences, with\napplications in speech recognition, sequence labeling and text generation; 3)\nEBMs for modeling joint distributions of both sequences of observations and\ntargets, and their applications in semi-supervised learning and calibrated\nnatural language understanding.\n","authors":["Zhijian Ou"],"pdf_url":"https://arxiv.org/pdf/2403.10961v1.pdf","comment":"The version before publisher editing"},{"id":"http://arxiv.org/abs/2403.10805v1","updated":"2024-03-16T04:40:10Z","published":"2024-03-16T04:40:10Z","title":"Speech-driven Personalized Gesture Synthetics: Harnessing Automatic\n  Fuzzy Feature Inference","summary":"  Speech-driven gesture generation is an emerging field within virtual human\ncreation. However, a significant challenge lies in accurately determining and\nprocessing the multitude of input features (such as acoustic, semantic,\nemotional, personality, and even subtle unknown features). Traditional\napproaches, reliant on various explicit feature inputs and complex multimodal\nprocessing, constrain the expressiveness of resulting gestures and limit their\napplicability. To address these challenges, we present Persona-Gestor, a novel\nend-to-end generative model designed to generate highly personalized 3D\nfull-body gestures solely relying on raw speech audio. The model combines a\nfuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization\n(AdaLN) transformer diffusion architecture. The fuzzy feature extractor\nharnesses a fuzzy inference strategy that automatically infers implicit,\ncontinuous fuzzy features. These fuzzy features, represented as a unified\nlatent feature, are fed into the AdaLN transformer. The AdaLN transformer\nintroduces a conditional mechanism that applies a uniform function across all\ntokens, thereby effectively modeling the correlation between the fuzzy features\nand the gesture sequence. This module ensures a high level of gesture-speech\nsynchronization while preserving naturalness. Finally, we employ the diffusion\nmodel to train and infer various gestures. Extensive subjective and objective\nevaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's\nsuperior performance to the current state-of-the-art approaches. Persona-Gestor\nimproves the system's usability and generalization capabilities, setting a new\nbenchmark in speech-driven gesture synthesis and broadening the horizon for\nvirtual human technology. Supplementary videos and code can be accessed at\nhttps://zf223669.github.io/Diffmotion-v2-website/\n","authors":["Fan Zhang","Zhaohan Wang","Xin Lyu","Siyuan Zhao","Mengjian Li","Weidong Geng","Naye Ji","Hui Du","Fuxing Gao","Hao Wu","Shunman Li"],"pdf_url":"https://arxiv.org/pdf/2403.10805v1.pdf","comment":"12 pages,"},{"id":"http://arxiv.org/abs/2403.10796v1","updated":"2024-03-16T03:59:33Z","published":"2024-03-16T03:59:33Z","title":"CoPlay: Audio-agnostic Cognitive Scaling for Acoustic Sensing","summary":"  Acoustic sensing manifests great potential in various applications that\nencompass health monitoring, gesture interface and imaging by leveraging the\nspeakers and microphones on smart devices. However, in ongoing research and\ndevelopment in acoustic sensing, one problem is often overlooked: the same\nspeaker, when used concurrently for sensing and other traditional applications\n(like playing music), could cause interference in both making it impractical to\nuse in the real world. The strong ultrasonic sensing signals mixed with music\nwould overload the speaker's mixer. To confront this issue of overloaded\nsignals, current solutions are clipping or down-scaling, both of which affect\nthe music playback quality and also sensing range and accuracy. To address this\nchallenge, we propose CoPlay, a deep learning based optimization algorithm to\ncognitively adapt the sensing signal. It can 1) maximize the sensing signal\nmagnitude within the available bandwidth left by the concurrent music to\noptimize sensing range and accuracy and 2) minimize any consequential frequency\ndistortion that can affect music playback. In this work, we design a deep\nlearning model and test it on common types of sensing signals (sine wave or\nFrequency Modulated Continuous Wave FMCW) as inputs with various agnostic\nconcurrent music and speech. First, we evaluated the model performance to show\nthe quality of the generated signals. Then we conducted field studies of\ndownstream acoustic sensing tasks in the real world. A study with 12 users\nproved that respiration monitoring and gesture recognition using our adapted\nsignal achieve similar accuracy as no-concurrent-music scenarios, while\nclipping or down-scaling manifests worse accuracy. A qualitative study also\nmanifests that the music play quality is not degraded, unlike traditional\nclipping or down-scaling methods.\n","authors":["Yin Li","Rajalakshmi Nanadakumar"],"pdf_url":"https://arxiv.org/pdf/2403.10796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10756v1","updated":"2024-03-16T01:38:36Z","published":"2024-03-16T01:38:36Z","title":"Refining Knowledge Transfer on Audio-Image Temporal Agreement for\n  Audio-Text Cross Retrieval","summary":"  The aim of this research is to refine knowledge transfer on audio-image\ntemporal agreement for audio-text cross retrieval. To address the limited\navailability of paired non-speech audio-text data, learning methods for\ntransferring the knowledge acquired from a large amount of paired audio-image\ndata to shared audio-text representation have been investigated, suggesting the\nimportance of how audio-image co-occurrence is learned. Conventional approaches\nin audio-image learning assign a single image randomly selected from the\ncorresponding video stream to the entire audio clip, assuming their\nco-occurrence. However, this method may not accurately capture the temporal\nagreement between the target audio and image because a single image can only\nrepresent a snapshot of a scene, though the target audio changes from moment to\nmoment. To address this problem, we propose two methods for audio and image\nmatching that effectively capture the temporal information: (i) Nearest Match\nwherein an image is selected from multiple time frames based on similarity with\naudio, and (ii) Multiframe Match wherein audio and image pairs of multiple time\nframes are used. Experimental results show that method (i) improves the\naudio-text retrieval performance by selecting the nearest image that aligns\nwith the audio information and transferring the learned knowledge. Conversely,\nmethod (ii) improves the performance of audio-image retrieval while not showing\nsignificant improvements in audio-text retrieval performance. These results\nindicate that refining audio-image temporal agreement may contribute to better\nknowledge transfer to audio-text retrieval.\n","authors":["Shunsuke Tsubaki","Daisuke Niizumi","Daiki Takeuchi","Yasunori Ohishi","Noboru Harada","Keisuke Imoto"],"pdf_url":"https://arxiv.org/pdf/2403.10756v1.pdf","comment":"Submitted to EUSIPCO2024"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.11037v1","updated":"2024-03-16T22:51:02Z","published":"2024-03-16T22:51:02Z","title":"Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals","summary":"  Sound event detection (SED) is an active area of audio research that aims to\ndetect the temporal occurrence of sounds. In this paper, we apply SED to engine\nfault detection by introducing a multimodal SED framework that detects\nfine-grained engine faults of automobile engines using audio and\naccelerometer-recorded vibration. We first introduce the problem of engine\nfault SED on a dataset collected from a large variety of vehicles with\nexpertly-labeled engine fault sound events. Next, we propose a SED model to\ntemporally detect ten fine-grained engine faults that occur within vehicle\nengines and further explore a pretraining strategy using a large-scale\nweakly-labeled engine fault dataset. Through multiple evaluations, we show our\nproposed framework is able to effectively detect engine fault sound events.\nFinally, we investigate the interaction and characteristics of each modality\nand show that fusing features from audio and vibration improves overall engine\nfault SED capabilities.\n","authors":["Dennis Fedorishin","Livio Forte III","Philip Schneider","Srirangaraj Setlur","Venu Govindaraju"],"pdf_url":"https://arxiv.org/pdf/2403.11037v1.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.10961v1","updated":"2024-03-16T16:16:31Z","published":"2024-03-16T16:16:31Z","title":"Energy-Based Models with Applications to Speech and Language Processing","summary":"  Energy-Based Models (EBMs) are an important class of probabilistic models,\nalso known as random fields and undirected graphical models. EBMs are\nun-normalized and thus radically different from other popular self-normalized\nprobabilistic models such as hidden Markov models (HMMs), autoregressive\nmodels, generative adversarial nets (GANs) and variational auto-encoders\n(VAEs). Over the past years, EBMs have attracted increasing interest not only\nfrom the core machine learning community, but also from application domains\nsuch as speech, vision, natural language processing (NLP) and so on, due to\nsignificant theoretical and algorithmic progress. The sequential nature of\nspeech and language also presents special challenges and needs a different\ntreatment from processing fix-dimensional data (e.g., images). Therefore, the\npurpose of this monograph is to present a systematic introduction to\nenergy-based models, including both algorithmic progress and applications in\nspeech and language processing. First, the basics of EBMs are introduced,\nincluding classic models, recent models parameterized by neural networks,\nsampling methods, and various learning methods from the classic learning\nalgorithms to the most advanced ones. Then, the application of EBMs in three\ndifferent scenarios is presented, i.e., for modeling marginal, conditional and\njoint distributions, respectively. 1) EBMs for sequential data with\napplications in language modeling, where the main focus is on the marginal\ndistribution of a sequence itself; 2) EBMs for modeling conditional\ndistributions of target sequences given observation sequences, with\napplications in speech recognition, sequence labeling and text generation; 3)\nEBMs for modeling joint distributions of both sequences of observations and\ntargets, and their applications in semi-supervised learning and calibrated\nnatural language understanding.\n","authors":["Zhijian Ou"],"pdf_url":"https://arxiv.org/pdf/2403.10961v1.pdf","comment":"The version before publisher editing"},{"id":"http://arxiv.org/abs/2403.10937v1","updated":"2024-03-16T14:34:31Z","published":"2024-03-16T14:34:31Z","title":"Initial Decoding with Minimally Augmented Language Model for Improved\n  Lattice Rescoring in Low Resource ASR","summary":"  This paper addresses the problem of improving speech recognition accuracy\nwith lattice rescoring in low-resource languages where the baseline language\nmodel is insufficient for generating inclusive lattices. We minimally augment\nthe baseline language model with word unigram counts that are present in a\nlarger text corpus of the target language but absent in the baseline. The\nlattices generated after decoding with such an augmented baseline language\nmodel are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada)\nrelative word error reduction with our proposed method. This reduction in word\nerror rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word\nerror reduction obtained by decoding with full Wikipedia text augmented\nlanguage mode while our approach consumes only 1/8th the memory. We demonstrate\nthat our method is comparable with various text selection-based language model\naugmentation and also consistent for data sets of different sizes. Our approach\nis applicable for training speech recognition systems under low resource\nconditions where speech data and compute resources are insufficient, while\nthere is a large text corpus that is available in the target language. Our\nresearch involves addressing the issue of out-of-vocabulary words of the\nbaseline in general and does not focus on resolving the absence of named\nentities. Our proposed method is simple and yet computationally less expensive.\n","authors":["Savitha Murthy","Dinkar Sitaram"],"pdf_url":"https://arxiv.org/pdf/2403.10937v1.pdf","comment":"14 pages, 7 figures, Accepted in Sadhana Journal"},{"id":"http://arxiv.org/abs/2403.10805v1","updated":"2024-03-16T04:40:10Z","published":"2024-03-16T04:40:10Z","title":"Speech-driven Personalized Gesture Synthetics: Harnessing Automatic\n  Fuzzy Feature Inference","summary":"  Speech-driven gesture generation is an emerging field within virtual human\ncreation. However, a significant challenge lies in accurately determining and\nprocessing the multitude of input features (such as acoustic, semantic,\nemotional, personality, and even subtle unknown features). Traditional\napproaches, reliant on various explicit feature inputs and complex multimodal\nprocessing, constrain the expressiveness of resulting gestures and limit their\napplicability. To address these challenges, we present Persona-Gestor, a novel\nend-to-end generative model designed to generate highly personalized 3D\nfull-body gestures solely relying on raw speech audio. The model combines a\nfuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization\n(AdaLN) transformer diffusion architecture. The fuzzy feature extractor\nharnesses a fuzzy inference strategy that automatically infers implicit,\ncontinuous fuzzy features. These fuzzy features, represented as a unified\nlatent feature, are fed into the AdaLN transformer. The AdaLN transformer\nintroduces a conditional mechanism that applies a uniform function across all\ntokens, thereby effectively modeling the correlation between the fuzzy features\nand the gesture sequence. This module ensures a high level of gesture-speech\nsynchronization while preserving naturalness. Finally, we employ the diffusion\nmodel to train and infer various gestures. Extensive subjective and objective\nevaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's\nsuperior performance to the current state-of-the-art approaches. Persona-Gestor\nimproves the system's usability and generalization capabilities, setting a new\nbenchmark in speech-driven gesture synthesis and broadening the horizon for\nvirtual human technology. Supplementary videos and code can be accessed at\nhttps://zf223669.github.io/Diffmotion-v2-website/\n","authors":["Fan Zhang","Zhaohan Wang","Xin Lyu","Siyuan Zhao","Mengjian Li","Weidong Geng","Naye Ji","Hui Du","Fuxing Gao","Hao Wu","Shunman Li"],"pdf_url":"https://arxiv.org/pdf/2403.10805v1.pdf","comment":"12 pages,"},{"id":"http://arxiv.org/abs/2403.10796v1","updated":"2024-03-16T03:59:33Z","published":"2024-03-16T03:59:33Z","title":"CoPlay: Audio-agnostic Cognitive Scaling for Acoustic Sensing","summary":"  Acoustic sensing manifests great potential in various applications that\nencompass health monitoring, gesture interface and imaging by leveraging the\nspeakers and microphones on smart devices. However, in ongoing research and\ndevelopment in acoustic sensing, one problem is often overlooked: the same\nspeaker, when used concurrently for sensing and other traditional applications\n(like playing music), could cause interference in both making it impractical to\nuse in the real world. The strong ultrasonic sensing signals mixed with music\nwould overload the speaker's mixer. To confront this issue of overloaded\nsignals, current solutions are clipping or down-scaling, both of which affect\nthe music playback quality and also sensing range and accuracy. To address this\nchallenge, we propose CoPlay, a deep learning based optimization algorithm to\ncognitively adapt the sensing signal. It can 1) maximize the sensing signal\nmagnitude within the available bandwidth left by the concurrent music to\noptimize sensing range and accuracy and 2) minimize any consequential frequency\ndistortion that can affect music playback. In this work, we design a deep\nlearning model and test it on common types of sensing signals (sine wave or\nFrequency Modulated Continuous Wave FMCW) as inputs with various agnostic\nconcurrent music and speech. First, we evaluated the model performance to show\nthe quality of the generated signals. Then we conducted field studies of\ndownstream acoustic sensing tasks in the real world. A study with 12 users\nproved that respiration monitoring and gesture recognition using our adapted\nsignal achieve similar accuracy as no-concurrent-music scenarios, while\nclipping or down-scaling manifests worse accuracy. A qualitative study also\nmanifests that the music play quality is not degraded, unlike traditional\nclipping or down-scaling methods.\n","authors":["Yin Li","Rajalakshmi Nanadakumar"],"pdf_url":"https://arxiv.org/pdf/2403.10796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08355v2","updated":"2024-03-16T01:58:36Z","published":"2023-11-14T17:54:38Z","title":"Mustango: Toward Controllable Text-to-Music Generation","summary":"  The quality of the text-to-music models has reached new heights due to recent\nadvancements in diffusion models. The controllability of various musical\naspects, however, has barely been explored. In this paper, we propose Mustango:\na music-domain-knowledge-inspired text-to-music system based on diffusion.\nMustango aims to control the generated music, not only with general text\ncaptions, but with more rich captions that can include specific instructions\nrelated to chords, beats, tempo, and key. At the core of Mustango is MuNet, a\nMusic-Domain-Knowledge-Informed UNet guidance module that steers the generated\nmusic to include the music-specific conditions, which we predict from the text\nprompt, as well as the general text embedding, during the reverse diffusion\nprocess. To overcome the limited availability of open datasets of music with\ntext captions, we propose a novel data augmentation method that includes\naltering the harmonic, rhythmic, and dynamic aspects of music audio and using\nstate-of-the-art Music Information Retrieval methods to extract the music\nfeatures which will then be appended to the existing descriptions in text\nformat. We release the resulting MusicBench dataset which contains over 52K\ninstances and includes music-theory-based descriptions in the caption text.\nThrough extensive experiments, we show that the quality of the music generated\nby Mustango is state-of-the-art, and the controllability through music-specific\ntext prompts greatly outperforms other models such as MusicGen and AudioLDM2.\n","authors":["Jan Melechovsky","Zixun Guo","Deepanway Ghosal","Navonil Majumder","Dorien Herremans","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2311.08355v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.10756v1","updated":"2024-03-16T01:38:36Z","published":"2024-03-16T01:38:36Z","title":"Refining Knowledge Transfer on Audio-Image Temporal Agreement for\n  Audio-Text Cross Retrieval","summary":"  The aim of this research is to refine knowledge transfer on audio-image\ntemporal agreement for audio-text cross retrieval. To address the limited\navailability of paired non-speech audio-text data, learning methods for\ntransferring the knowledge acquired from a large amount of paired audio-image\ndata to shared audio-text representation have been investigated, suggesting the\nimportance of how audio-image co-occurrence is learned. Conventional approaches\nin audio-image learning assign a single image randomly selected from the\ncorresponding video stream to the entire audio clip, assuming their\nco-occurrence. However, this method may not accurately capture the temporal\nagreement between the target audio and image because a single image can only\nrepresent a snapshot of a scene, though the target audio changes from moment to\nmoment. To address this problem, we propose two methods for audio and image\nmatching that effectively capture the temporal information: (i) Nearest Match\nwherein an image is selected from multiple time frames based on similarity with\naudio, and (ii) Multiframe Match wherein audio and image pairs of multiple time\nframes are used. Experimental results show that method (i) improves the\naudio-text retrieval performance by selecting the nearest image that aligns\nwith the audio information and transferring the learned knowledge. Conversely,\nmethod (ii) improves the performance of audio-image retrieval while not showing\nsignificant improvements in audio-text retrieval performance. These results\nindicate that refining audio-image temporal agreement may contribute to better\nknowledge transfer to audio-text retrieval.\n","authors":["Shunsuke Tsubaki","Daisuke Niizumi","Daiki Takeuchi","Yasunori Ohishi","Noboru Harada","Keisuke Imoto"],"pdf_url":"https://arxiv.org/pdf/2403.10756v1.pdf","comment":"Submitted to EUSIPCO2024"}]},"2024-03-19T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.10904v2","updated":"2024-03-19T11:37:28Z","published":"2024-03-16T11:38:58Z","title":"Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of\n  Complex Physical Systems","summary":"  Data-driven modeling of complex physical systems is receiving a growing\namount of attention in the simulation and machine learning communities. Since\nmost physical simulations are based on compute-intensive, iterative\nimplementations of differential equation systems, a (partial) replacement with\nlearned, 1-step inference models has the potential for significant speedups in\na wide range of application areas. In this context, we present a novel\nbenchmark for the evaluation of 1-step generative learning models in terms of\nspeed and physical correctness. Our Urban Sound Propagation benchmark is based\non the physically complex and practically relevant, yet intuitively easy to\ngrasp task of modeling the 2d propagation of waves from a sound source in an\nurban environment. We provide a dataset with 100k samples, where each sample\nconsists of pairs of real 2d building maps drawn from OpenStreetmap, a\nparameterized sound source, and a simulated ground truth sound propagation for\nthe given scene. The dataset provides four different simulation tasks with\nincreasing complexity regarding reflection, diffraction and source variance. A\nfirst baseline evaluation of common generative U-Net, GAN and Diffusion models\nshows, that while these models are very well capable of modeling sound\npropagations in simple cases, the approximation of sub-systems represented by\nhigher order equations systematically fails. Information about the dataset,\ndownload instructions and source codes are provided on our website:\nhttps://www.urban-sound-data.org.\n","authors":["Martin Spitznagel","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.10904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05791v3","updated":"2024-03-19T12:22:04Z","published":"2024-03-09T04:37:34Z","title":"Asynchronous Microphone Array Calibration using Hybrid TDOA Information","summary":"  Asynchronous microphone array calibration is a prerequisite for most audition\nrobot applications. A popular solution to the above calibration problem is the\nbatch form of Simultaneous Localisation and Mapping (SLAM), using the time\ndifference of arrival measurements between two microphones (TDOA-M), and the\nrobot (which serves as a moving sound source during calibration) odometry\ninformation. In this paper, we introduce a new form of measurement for\nmicrophone array calibration, i.e. the time difference of arrival between\nadjacent sound events (TDOA-S) with respect to the microphone channels. We\npropose to combine TDOA-S and TDOA-M, called hybrid TDOA, together with\nodometry measurements for bath SLAM-based calibration of asynchronous\nmicrophone arrays. Simulation and real-world experiment results consistently\nshow that our method is more independent of microphone number, less sensitive\nto initial values (when using off-the-shelf algorithms such as Gauss-Newton\niterations), and has better calibration accuracy and robustness under various\nTDOA noises. In addition, the simulation result demonstrates that our method\nhas a lower Cram\\'er-Rao lower bound (CRLB) for microphone parameters. To\nbenefit the community, we open-source our code and data at\nhttps://github.com/zcj808/Hybrid-TDOA-Calib.\n","authors":["Chengjie Zhang","Jiang Wang","He Kong"],"pdf_url":"https://arxiv.org/pdf/2403.05791v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08684v2","updated":"2024-03-19T15:26:02Z","published":"2023-09-15T18:20:55Z","title":"Music Source Separation Based on a Lightweight Deep Learning Framework\n  (DTTNET: DUAL-PATH TFC-TDF UNET)","summary":"  Music source separation (MSS) aims to extract 'vocals', 'drums', 'bass' and\n'other' tracks from a piece of mixed music. While deep learning methods have\nshown impressive results, there is a trend toward larger models. In our paper,\nwe introduce a novel and lightweight architecture called DTTNet, which is based\non Dual-Path Module and Time-Frequency Convolutions Time-Distributed\nFully-connected UNet (TFC-TDF UNet). DTTNet achieves 10.12 dB cSDR on 'vocals'\ncompared to 10.01 dB reported for Bandsplit RNN (BSRNN) but with 86.7% fewer\nparameters. We also assess pattern-specific performance and model\ngeneralization for intricate audio patterns.\n","authors":["Junyu Chen","Susmitha Vekkot","Pancham Shukla"],"pdf_url":"https://arxiv.org/pdf/2309.08684v2.pdf","comment":"Accepted for ICASSP 2024. Additional experiments can be found in the\n  published version on IEEE Xplore"},{"id":"http://arxiv.org/abs/2403.12630v1","updated":"2024-03-19T10:57:17Z","published":"2024-03-19T10:57:17Z","title":"Reproducing the Acoustic Velocity Vectors in a Circular Listening Area","summary":"  Acoustic velocity vectors are important for human's localization of sound at\nlow frequencies. This paper proposes a sound field reproduction algorithm,\nwhich matches the acoustic velocity vectors in a circular listening area. In\nprevious work, acoustic velocity vectors are matched either at sweet spots or\non the boundary of the listening area. Sweet spots restrict listener's\nmovement, whereas measuring the acoustic velocity vectors on the boundary\nrequires complicated measurement setup. This paper proposes the cylindrical\nharmonic coefficients of the acoustic velocity vectors in a circular area (CHV\ncoefficients), which are calculated from the cylindrical harmonic coefficients\nof the global pressure (global CHP coefficients) by using the sound field\ntranslation formula. The global CHP coefficients can be measured by a circular\nmicrophone array, which can be bought off-the-shelf. By matching the CHV\ncoefficients, the acoustic velocity vectors are reproduced throughout the\nlistening area. Hence, listener's movements are allowed. Simulations show that\nat low frequency, where the acoustic velocity vectors are the dominant factor\nfor localization, the proposed reproduction method based on the CHV\ncoefficients results in higher accuracy in reproduced acoustic velocity vectors\nwhen compared with traditional method based on the global CHP coefficients.\n","authors":["Jiarui Wang","Thushara Abhayapala","Jihui Aimee Zhang","Prasanga Samarasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.12630v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2309.10787v2","updated":"2024-03-19T08:51:51Z","published":"2023-09-19T17:35:16Z","title":"AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual\n  Representation Models","summary":"  Audio-visual representation learning aims to develop systems with human-like\nperception by utilizing correlation between auditory and visual information.\nHowever, current models often focus on a limited set of tasks, and\ngeneralization abilities of learned representations are unclear. To this end,\nwe propose the AV-SUPERB benchmark that enables general-purpose evaluation of\nunimodal audio/visual and bimodal fusion representations on 7 datasets covering\n5 audio-visual tasks in speech and audio processing. We evaluate 5 recent\nself-supervised models and show that none of these models generalize to all\ntasks, emphasizing the need for future study on improving universal model\nperformance. In addition, we show that representations may be improved with\nintermediate-task fine-tuning and audio event classification with AudioSet\nserves as a strong intermediate task. We release our benchmark with evaluation\ncode and a model submission platform to encourage further research in\naudio-visual learning.\n","authors":["Yuan Tseng","Layne Berry","Yi-Ting Chen","I-Hsiang Chiu","Hsuan-Hao Lin","Max Liu","Puyuan Peng","Yi-Jen Shih","Hung-Yu Wang","Haibin Wu","Po-Yao Huang","Chun-Mao Lai","Shang-Wen Li","David Harwath","Yu Tsao","Shinji Watanabe","Abdelrahman Mohamed","Chi-Luen Feng","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2309.10787v2.pdf","comment":"Accepted to ICASSP 2024; Evaluation Code:\n  https://github.com/roger-tseng/av-superb Submission Platform:\n  https://av.superbbenchmark.org"},{"id":"http://arxiv.org/abs/2403.12477v1","updated":"2024-03-19T06:27:47Z","published":"2024-03-19T06:27:47Z","title":"Real-time Speech Extraction Using Spatially Regularized Independent\n  Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix\n  Estimation","summary":"  Real-time speech extraction is an important challenge with various\napplications such as speech recognition in a human-like avatar/robot. In this\npaper, we propose the real-time extension of a speech extraction method based\non independent low-rank matrix analysis (ILRMA) and rank-constrained spatial\ncovariance matrix estimation (RCSCME). The RCSCME-based method is a\nmultichannel blind speech extraction method that demonstrates superior speech\nextraction performance in diffuse noise environments. To improve the\nperformance, we introduce spatial regularization into the ILRMA part of the\nRCSCME-based speech extraction and design two regularizers. Speech extraction\nexperiments demonstrated that the proposed methods can function in real time\nand the designed regularizers improve the speech extraction performance.\n","authors":["Yuto Ishikawa","Kohei Konaka","Tomohiko Nakamura","Norihiro Takamune","Hiroshi Saruwatari"],"pdf_url":"https://arxiv.org/pdf/2403.12477v1.pdf","comment":"5 pages, 3 figures, accepted at HSCMA 2024"},{"id":"http://arxiv.org/abs/2403.12408v1","updated":"2024-03-19T03:35:20Z","published":"2024-03-19T03:35:20Z","title":"MSLM-S2ST: A Multitask Speech Language Model for Textless\n  Speech-to-Speech Translation with Speaker Style Preservation","summary":"  There have been emerging research interest and advances in speech-to-speech\ntranslation (S2ST), translating utterances from one language to another. This\nwork proposes Multitask Speech Language Model (MSLM), which is a decoder-only\nspeech language model trained in a multitask setting. Without reliance on text\ntraining data, our model is able to support multilingual S2ST with speaker\nstyle preserved.\n","authors":["Yifan Peng","Ilia Kulikov","Yilin Yang","Sravya Popuri","Hui Lu","Changhan Wang","Hongyu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.12408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12402v1","updated":"2024-03-19T03:22:28Z","published":"2024-03-19T03:22:28Z","title":"An Empirical Study of Speech Language Models for Prompt-Conditioned\n  Speech Synthesis","summary":"  Speech language models (LMs) are promising for high-quality speech synthesis\nthrough in-context learning. A typical speech LM takes discrete semantic units\nas content and a short utterance as prompt, and synthesizes speech which\npreserves the content's semantics but mimics the prompt's style. However, there\nis no systematic understanding on how the synthesized audio is controlled by\nthe prompt and content. In this work, we conduct an empirical study of the\nwidely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and\nprovide insights into the prompt design and content semantic units. Our\nanalysis reveals that heterogeneous and nonstationary prompts hurt the audio\nquality in contrast to the previous finding that longer prompts always lead to\nbetter synthesis. Moreover, we find that the speaker style of the synthesized\naudio is also affected by the content in addition to the prompt. We further\nshow that semantic units carry rich acoustic information such as pitch, tempo,\nvolume and speech emphasis, which might be leaked from the content to the\nsynthesized audio.\n","authors":["Yifan Peng","Ilia Kulikov","Yilin Yang","Sravya Popuri","Hui Lu","Changhan Wang","Hongyu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.12402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11757v2","updated":"2024-03-19T18:14:35Z","published":"2024-03-18T13:11:10Z","title":"Efficient Feature Extraction and Late Fusion Strategy for Audiovisual\n  Emotional Mimicry Intensity Estimation","summary":"  In this paper, we present the solution to the Emotional Mimicry Intensity\n(EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis\nin-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to\nevaluate the emotional intensity of seed videos by assessing them from a set of\npredefined emotion categories (i.e., \"Admiration\", \"Amusement\",\n\"Determination\", \"Empathic Pain\", \"Excitement\" and \"Joy\"). To tackle this\nchallenge, we extracted rich dual-channel visual features based on ResNet18 and\nAUs for the video modality and effective single-channel features based on\nWav2Vec2.0 for the audio modality. This allowed us to obtain comprehensive\nemotional features for the audiovisual modality. Additionally, leveraging a\nlate fusion strategy, we averaged the predictions of the visual and acoustic\nmodels, resulting in a more accurate estimation of audiovisual emotional\nmimicry intensity. Experimental results validate the effectiveness of our\napproach, with the average Pearson's correlation Coefficient($\\rho$) across the\n6 emotion dimensionson the validation set achieving 0.3288.\n","authors":["Jun Yu","Wangyuan Zhu","Jichao Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.11757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13086v1","updated":"2024-03-19T18:32:48Z","published":"2024-03-19T18:32:48Z","title":"Listenable Maps for Audio Classifiers","summary":"  Despite the impressive performance of deep learning models across diverse\ntasks, their complexity poses challenges for interpretation. This challenge is\nparticularly evident for audio signals, where conveying interpretations becomes\ninherently difficult. To address this issue, we introduce Listenable Maps for\nAudio Classifiers (L-MAC), a posthoc interpretation method that generates\nfaithful and listenable interpretations. L-MAC utilizes a decoder on top of a\npretrained classifier to generate binary masks that highlight relevant portions\nof the input audio. We train the decoder with a special loss that maximizes the\nconfidence of the classifier decision on the masked-in portion of the audio\nwhile minimizing the probability of model output for the masked-out portion.\nQuantitative evaluations on both in-domain and out-of-domain data demonstrate\nthat L-MAC consistently produces more faithful interpretations than several\ngradient and masking-based methodologies. Furthermore, a user study confirms\nthat, on average, users prefer the interpretations generated by the proposed\ntechnique.\n","authors":["Francesco Paissan","Mirco Ravanelli","Cem Subakan"],"pdf_url":"https://arxiv.org/pdf/2403.13086v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.10904v2","updated":"2024-03-19T11:37:28Z","published":"2024-03-16T11:38:58Z","title":"Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of\n  Complex Physical Systems","summary":"  Data-driven modeling of complex physical systems is receiving a growing\namount of attention in the simulation and machine learning communities. Since\nmost physical simulations are based on compute-intensive, iterative\nimplementations of differential equation systems, a (partial) replacement with\nlearned, 1-step inference models has the potential for significant speedups in\na wide range of application areas. In this context, we present a novel\nbenchmark for the evaluation of 1-step generative learning models in terms of\nspeed and physical correctness. Our Urban Sound Propagation benchmark is based\non the physically complex and practically relevant, yet intuitively easy to\ngrasp task of modeling the 2d propagation of waves from a sound source in an\nurban environment. We provide a dataset with 100k samples, where each sample\nconsists of pairs of real 2d building maps drawn from OpenStreetmap, a\nparameterized sound source, and a simulated ground truth sound propagation for\nthe given scene. The dataset provides four different simulation tasks with\nincreasing complexity regarding reflection, diffraction and source variance. A\nfirst baseline evaluation of common generative U-Net, GAN and Diffusion models\nshows, that while these models are very well capable of modeling sound\npropagations in simple cases, the approximation of sub-systems represented by\nhigher order equations systematically fails. Information about the dataset,\ndownload instructions and source codes are provided on our website:\nhttps://www.urban-sound-data.org.\n","authors":["Martin Spitznagel","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.10904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05791v3","updated":"2024-03-19T12:22:04Z","published":"2024-03-09T04:37:34Z","title":"Asynchronous Microphone Array Calibration using Hybrid TDOA Information","summary":"  Asynchronous microphone array calibration is a prerequisite for most audition\nrobot applications. A popular solution to the above calibration problem is the\nbatch form of Simultaneous Localisation and Mapping (SLAM), using the time\ndifference of arrival measurements between two microphones (TDOA-M), and the\nrobot (which serves as a moving sound source during calibration) odometry\ninformation. In this paper, we introduce a new form of measurement for\nmicrophone array calibration, i.e. the time difference of arrival between\nadjacent sound events (TDOA-S) with respect to the microphone channels. We\npropose to combine TDOA-S and TDOA-M, called hybrid TDOA, together with\nodometry measurements for bath SLAM-based calibration of asynchronous\nmicrophone arrays. Simulation and real-world experiment results consistently\nshow that our method is more independent of microphone number, less sensitive\nto initial values (when using off-the-shelf algorithms such as Gauss-Newton\niterations), and has better calibration accuracy and robustness under various\nTDOA noises. In addition, the simulation result demonstrates that our method\nhas a lower Cram\\'er-Rao lower bound (CRLB) for microphone parameters. To\nbenefit the community, we open-source our code and data at\nhttps://github.com/zcj808/Hybrid-TDOA-Calib.\n","authors":["Chengjie Zhang","Jiang Wang","He Kong"],"pdf_url":"https://arxiv.org/pdf/2403.05791v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08684v2","updated":"2024-03-19T15:26:02Z","published":"2023-09-15T18:20:55Z","title":"Music Source Separation Based on a Lightweight Deep Learning Framework\n  (DTTNET: DUAL-PATH TFC-TDF UNET)","summary":"  Music source separation (MSS) aims to extract 'vocals', 'drums', 'bass' and\n'other' tracks from a piece of mixed music. While deep learning methods have\nshown impressive results, there is a trend toward larger models. In our paper,\nwe introduce a novel and lightweight architecture called DTTNet, which is based\non Dual-Path Module and Time-Frequency Convolutions Time-Distributed\nFully-connected UNet (TFC-TDF UNet). DTTNet achieves 10.12 dB cSDR on 'vocals'\ncompared to 10.01 dB reported for Bandsplit RNN (BSRNN) but with 86.7% fewer\nparameters. We also assess pattern-specific performance and model\ngeneralization for intricate audio patterns.\n","authors":["Junyu Chen","Susmitha Vekkot","Pancham Shukla"],"pdf_url":"https://arxiv.org/pdf/2309.08684v2.pdf","comment":"Accepted for ICASSP 2024. Additional experiments can be found in the\n  published version on IEEE Xplore"},{"id":"http://arxiv.org/abs/2403.12630v1","updated":"2024-03-19T10:57:17Z","published":"2024-03-19T10:57:17Z","title":"Reproducing the Acoustic Velocity Vectors in a Circular Listening Area","summary":"  Acoustic velocity vectors are important for human's localization of sound at\nlow frequencies. This paper proposes a sound field reproduction algorithm,\nwhich matches the acoustic velocity vectors in a circular listening area. In\nprevious work, acoustic velocity vectors are matched either at sweet spots or\non the boundary of the listening area. Sweet spots restrict listener's\nmovement, whereas measuring the acoustic velocity vectors on the boundary\nrequires complicated measurement setup. This paper proposes the cylindrical\nharmonic coefficients of the acoustic velocity vectors in a circular area (CHV\ncoefficients), which are calculated from the cylindrical harmonic coefficients\nof the global pressure (global CHP coefficients) by using the sound field\ntranslation formula. The global CHP coefficients can be measured by a circular\nmicrophone array, which can be bought off-the-shelf. By matching the CHV\ncoefficients, the acoustic velocity vectors are reproduced throughout the\nlistening area. Hence, listener's movements are allowed. Simulations show that\nat low frequency, where the acoustic velocity vectors are the dominant factor\nfor localization, the proposed reproduction method based on the CHV\ncoefficients results in higher accuracy in reproduced acoustic velocity vectors\nwhen compared with traditional method based on the global CHP coefficients.\n","authors":["Jiarui Wang","Thushara Abhayapala","Jihui Aimee Zhang","Prasanga Samarasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.12630v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2309.10787v2","updated":"2024-03-19T08:51:51Z","published":"2023-09-19T17:35:16Z","title":"AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual\n  Representation Models","summary":"  Audio-visual representation learning aims to develop systems with human-like\nperception by utilizing correlation between auditory and visual information.\nHowever, current models often focus on a limited set of tasks, and\ngeneralization abilities of learned representations are unclear. To this end,\nwe propose the AV-SUPERB benchmark that enables general-purpose evaluation of\nunimodal audio/visual and bimodal fusion representations on 7 datasets covering\n5 audio-visual tasks in speech and audio processing. We evaluate 5 recent\nself-supervised models and show that none of these models generalize to all\ntasks, emphasizing the need for future study on improving universal model\nperformance. In addition, we show that representations may be improved with\nintermediate-task fine-tuning and audio event classification with AudioSet\nserves as a strong intermediate task. We release our benchmark with evaluation\ncode and a model submission platform to encourage further research in\naudio-visual learning.\n","authors":["Yuan Tseng","Layne Berry","Yi-Ting Chen","I-Hsiang Chiu","Hsuan-Hao Lin","Max Liu","Puyuan Peng","Yi-Jen Shih","Hung-Yu Wang","Haibin Wu","Po-Yao Huang","Chun-Mao Lai","Shang-Wen Li","David Harwath","Yu Tsao","Shinji Watanabe","Abdelrahman Mohamed","Chi-Luen Feng","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2309.10787v2.pdf","comment":"Accepted to ICASSP 2024; Evaluation Code:\n  https://github.com/roger-tseng/av-superb Submission Platform:\n  https://av.superbbenchmark.org"},{"id":"http://arxiv.org/abs/2403.12477v1","updated":"2024-03-19T06:27:47Z","published":"2024-03-19T06:27:47Z","title":"Real-time Speech Extraction Using Spatially Regularized Independent\n  Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix\n  Estimation","summary":"  Real-time speech extraction is an important challenge with various\napplications such as speech recognition in a human-like avatar/robot. In this\npaper, we propose the real-time extension of a speech extraction method based\non independent low-rank matrix analysis (ILRMA) and rank-constrained spatial\ncovariance matrix estimation (RCSCME). The RCSCME-based method is a\nmultichannel blind speech extraction method that demonstrates superior speech\nextraction performance in diffuse noise environments. To improve the\nperformance, we introduce spatial regularization into the ILRMA part of the\nRCSCME-based speech extraction and design two regularizers. Speech extraction\nexperiments demonstrated that the proposed methods can function in real time\nand the designed regularizers improve the speech extraction performance.\n","authors":["Yuto Ishikawa","Kohei Konaka","Tomohiko Nakamura","Norihiro Takamune","Hiroshi Saruwatari"],"pdf_url":"https://arxiv.org/pdf/2403.12477v1.pdf","comment":"5 pages, 3 figures, accepted at HSCMA 2024"},{"id":"http://arxiv.org/abs/2403.12408v1","updated":"2024-03-19T03:35:20Z","published":"2024-03-19T03:35:20Z","title":"MSLM-S2ST: A Multitask Speech Language Model for Textless\n  Speech-to-Speech Translation with Speaker Style Preservation","summary":"  There have been emerging research interest and advances in speech-to-speech\ntranslation (S2ST), translating utterances from one language to another. This\nwork proposes Multitask Speech Language Model (MSLM), which is a decoder-only\nspeech language model trained in a multitask setting. Without reliance on text\ntraining data, our model is able to support multilingual S2ST with speaker\nstyle preserved.\n","authors":["Yifan Peng","Ilia Kulikov","Yilin Yang","Sravya Popuri","Hui Lu","Changhan Wang","Hongyu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.12408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12402v1","updated":"2024-03-19T03:22:28Z","published":"2024-03-19T03:22:28Z","title":"An Empirical Study of Speech Language Models for Prompt-Conditioned\n  Speech Synthesis","summary":"  Speech language models (LMs) are promising for high-quality speech synthesis\nthrough in-context learning. A typical speech LM takes discrete semantic units\nas content and a short utterance as prompt, and synthesizes speech which\npreserves the content's semantics but mimics the prompt's style. However, there\nis no systematic understanding on how the synthesized audio is controlled by\nthe prompt and content. In this work, we conduct an empirical study of the\nwidely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and\nprovide insights into the prompt design and content semantic units. Our\nanalysis reveals that heterogeneous and nonstationary prompts hurt the audio\nquality in contrast to the previous finding that longer prompts always lead to\nbetter synthesis. Moreover, we find that the speaker style of the synthesized\naudio is also affected by the content in addition to the prompt. We further\nshow that semantic units carry rich acoustic information such as pitch, tempo,\nvolume and speech emphasis, which might be leaked from the content to the\nsynthesized audio.\n","authors":["Yifan Peng","Ilia Kulikov","Yilin Yang","Sravya Popuri","Hui Lu","Changhan Wang","Hongyu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.12402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11757v2","updated":"2024-03-19T18:14:35Z","published":"2024-03-18T13:11:10Z","title":"Efficient Feature Extraction and Late Fusion Strategy for Audiovisual\n  Emotional Mimicry Intensity Estimation","summary":"  In this paper, we present the solution to the Emotional Mimicry Intensity\n(EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis\nin-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to\nevaluate the emotional intensity of seed videos by assessing them from a set of\npredefined emotion categories (i.e., \"Admiration\", \"Amusement\",\n\"Determination\", \"Empathic Pain\", \"Excitement\" and \"Joy\"). To tackle this\nchallenge, we extracted rich dual-channel visual features based on ResNet18 and\nAUs for the video modality and effective single-channel features based on\nWav2Vec2.0 for the audio modality. This allowed us to obtain comprehensive\nemotional features for the audiovisual modality. Additionally, leveraging a\nlate fusion strategy, we averaged the predictions of the visual and acoustic\nmodels, resulting in a more accurate estimation of audiovisual emotional\nmimicry intensity. Experimental results validate the effectiveness of our\napproach, with the average Pearson's correlation Coefficient($\\rho$) across the\n6 emotion dimensionson the validation set achieving 0.3288.\n","authors":["Jun Yu","Wangyuan Zhu","Jichao Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.11757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13086v1","updated":"2024-03-19T18:32:48Z","published":"2024-03-19T18:32:48Z","title":"Listenable Maps for Audio Classifiers","summary":"  Despite the impressive performance of deep learning models across diverse\ntasks, their complexity poses challenges for interpretation. This challenge is\nparticularly evident for audio signals, where conveying interpretations becomes\ninherently difficult. To address this issue, we introduce Listenable Maps for\nAudio Classifiers (L-MAC), a posthoc interpretation method that generates\nfaithful and listenable interpretations. L-MAC utilizes a decoder on top of a\npretrained classifier to generate binary masks that highlight relevant portions\nof the input audio. We train the decoder with a special loss that maximizes the\nconfidence of the classifier decision on the masked-in portion of the audio\nwhile minimizing the probability of model output for the masked-out portion.\nQuantitative evaluations on both in-domain and out-of-domain data demonstrate\nthat L-MAC consistently produces more faithful interpretations than several\ngradient and masking-based methodologies. Furthermore, a user study confirms\nthat, on average, users prefer the interpretations generated by the proposed\ntechnique.\n","authors":["Francesco Paissan","Mirco Ravanelli","Cem Subakan"],"pdf_url":"https://arxiv.org/pdf/2403.13086v1.pdf","comment":null}]},"2024-03-20T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.12425v2","updated":"2024-03-20T13:56:56Z","published":"2024-03-19T04:25:54Z","title":"Multimodal Fusion Method with Spatiotemporal Sequences and Relationship\n  Learning for Valence-Arousal Estimation","summary":"  This paper presents our approach for the VA (Valence-Arousal) estimation task\nin the ABAW6 competition. We devised a comprehensive model by preprocessing\nvideo frames and audio segments to extract visual and audio features. Through\nthe utilization of Temporal Convolutional Network (TCN) modules, we effectively\ncaptured the temporal and spatial correlations between these features.\nSubsequently, we employed a Transformer encoder structure to learn long-range\ndependencies, thereby enhancing the model's performance and generalization\nability. Our method leverages a multimodal data fusion approach, integrating\npre-trained audio and video backbones for feature extraction, followed by\nTCN-based spatiotemporal encoding and Transformer-based temporal information\ncapture. Experimental results demonstrate the effectiveness of our approach,\nachieving competitive performance in VA estimation on the AffWild2 dataset.\n","authors":["Jun Yu","Gongpeng Zhao","Yongqi Wang","Zhihong Wei","Yang Zheng","Zerui Zhang","Zhongpeng Cai","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12425v2.pdf","comment":"8 pages,3 figures"},{"id":"http://arxiv.org/abs/2403.10493v2","updated":"2024-03-20T06:05:00Z","published":"2024-03-15T17:27:42Z","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","summary":"  Diffusion-based audio and music generation models commonly generate music by\nconstructing an image representation of audio (e.g., a mel-spectrogram) and\nthen converting it to audio using a phase reconstruction model or vocoder.\nTypical vocoders, however, produce monophonic audio at lower resolutions (e.g.,\n16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an\nefficient high-fidelity stereophonic vocoder. Our method employs a cascade of\nthree generative adversarial networks (GANs) that convert low-resolution\nmel-spectrograms to audio, upsamples to high-resolution audio via bandwidth\nexpansion, and upmixes to stereophonic audio. Compared to previous work, we\npropose 1) a unified GAN-based generator and discriminator architecture and\ntraining procedure for each stage of our cascade, 2) a new fast, near\ndownsampling-compatible bandwidth extension module, and 3) a new fast\ndownmix-compatible mono-to-stereo upmixer that ensures the preservation of\nmonophonic content in the output. We evaluate our approach using both objective\nand subjective listening tests and find our approach yields comparable or\nbetter audio quality, better spatialization control, and significantly faster\ninference speed compared to past work. Sound examples are at\nhttps://MusicHiFi.github.io/web/.\n","authors":["Ge Zhu","Juan-Pablo Caceres","Zhiyao Duan","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2403.10493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13720v1","updated":"2024-03-20T16:27:13Z","published":"2024-03-20T16:27:13Z","title":"UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing\n  Using Discrete Speech Unit Challenge","summary":"  We present UTDUSS, the UTokyo-SaruLab system submitted to Interspeech2024\nSpeech Processing Using Discrete Speech Unit Challenge. The challenge focuses\non using discrete speech unit learned from large speech corpora for some tasks.\nWe submitted our UTDUSS system to two text-to-speech tracks: Vocoder and\nAcoustic+Vocoder. Our system incorporates neural audio codec (NAC) pre-trained\non only speech corpora, which makes the learned codec represent rich acoustic\nfeatures that are necessary for high-fidelity speech reconstruction. For the\nacoustic+vocoder track, we trained an acoustic model based on Transformer\nencoder-decoder that predicted the pre-trained NAC tokens from text input. We\ndescribe our strategies to build these models, such as data selection,\ndownsampling, and hyper-parameter tuning. Our system ranked in second and first\nfor the Vocoder and Acoustic+Vocoder tracks, respectively.\n","authors":["Wataru Nakata","Kazuki Yamauchi","Dong Yang","Hiroaki Hyodo","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2403.13720v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.13659v1","updated":"2024-03-20T15:08:43Z","published":"2024-03-20T15:08:43Z","title":"Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional\n  Emotion Recognition","summary":"  Multi-modal emotion recognition has recently gained a lot of attention since\nit can leverage diverse and complementary relationships over multiple\nmodalities, such as audio, visual, and text. Most state-of-the-art methods for\nmultimodal fusion rely on recurrent networks or conventional attention\nmechanisms that do not effectively leverage the complementary nature of the\nmodalities. In this paper, we focus on dimensional emotion recognition based on\nthe fusion of facial, vocal, and text modalities extracted from videos.\nSpecifically, we propose a recursive cross-modal attention (RCMA) to\neffectively capture the complementary relationships across the modalities in a\nrecursive fashion. The proposed model is able to effectively capture the\ninter-modal relationships by computing the cross-attention weights across the\nindividual modalities and the joint representation of the other two modalities.\nTo further improve the inter-modal relationships, the obtained attended\nfeatures of the individual modalities are again fed as input to the cross-modal\nattention to refine the feature representations of the individual modalities.\nIn addition to that, we have used Temporal convolution networks (TCNs) to\ncapture the temporal modeling (intra-modal relationships) of the individual\nmodalities. By deploying the TCNs as well cross-modal attention in a recursive\nfashion, we are able to effectively capture both intra- and inter-modal\nrelationships across the audio, visual, and text modalities. Experimental\nresults on validation-set videos from the AffWild2 dataset indicate that our\nproposed fusion model is able to achieve significant improvement over the\nbaseline for the sixth challenge of Affective Behavior Analysis in-the-Wild\n2024 (ABAW6) competition.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.13659v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2209.09068;\n  text overlap with arXiv:2203.14779 by other authors"},{"id":"http://arxiv.org/abs/2403.13643v1","updated":"2024-03-20T14:50:00Z","published":"2024-03-20T14:50:00Z","title":"Vibration Sensitivity of one-port and two-port MEMS microphones","summary":"  Micro-electro-mechanical system (MEMS) microphones (mics) with two acoustic\nports are currently receiving considerable interest, with the promise of\nachieving higher directional sensitivity compared to traditional one-port\narchitectures. However, measuring pressure differences in two-port microphones\ntypically commands sensing elements that are softer than in one-port mics, and\nare therefore presumably more prone to interference from external vibration.\nHere we derive a universal expression for microphone sensitivity to vibration\nand we experimentally demonstrate its validity for several emerging two-port\nmicrophone technologies. We also perform vibration measurements on a one-port\nmic, thus providing a one-stop direct comparison between one-port and two-port\nsensing approaches. We find that the acoustically-referred vibration\nsensitivity of two-port MEMS mics, in units of measured acoustic pressure per\nexternal acceleration (i.e., Pascals per g), does not depend on the sensing\nelement stiffness nor on its natural frequency. We also show that this\nvibration sensitivity in two-port mics is inversely proportional to frequency\nas opposed to the frequency independent behavior observed in one-port mics.\nThis is confirmed experimentally for several types of microphone packages.\n","authors":["Francis Doyon-D'Amour","Carly Stalder","Timothy Hodges","Michel Stephan","Lixiue Wu","Triantafillos Koukoulas","Stephane Leahy","Raphael St-Gelais"],"pdf_url":"https://arxiv.org/pdf/2403.13643v1.pdf","comment":"8 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.13423v1","updated":"2024-03-20T09:09:49Z","published":"2024-03-20T09:09:49Z","title":"Advanced Long-Content Speech Recognition With Factorized Neural\n  Transducer","summary":"  In this paper, we propose two novel approaches, which integrate long-content\ninformation into the factorized neural transducer (FNT) based architecture in\nboth non-streaming (referred to as LongFNT ) and streaming (referred to as\nSLongFNT ) scenarios. We first investigate whether long-content transcriptions\ncan improve the vanilla conformer transducer (C-T) models. Our experiments\nindicate that the vanilla C-T models do not exhibit improved performance when\nutilizing long-content transcriptions, possibly due to the predictor network of\nC-T models not functioning as a pure language model. Instead, FNT shows its\npotential in utilizing long-content information, where we propose the LongFNT\nmodel and explore the impact of long-content information in both text\n(LongFNT-Text) and speech (LongFNT-Speech). The proposed LongFNT-Text and\nLongFNT-Speech models further complement each other to achieve better\nperformance, with transcription history proving more valuable to the model. The\neffectiveness of our LongFNT approach is evaluated on LibriSpeech and\nGigaSpeech corpora, and obtains relative 19% and 12% word error rate reduction,\nrespectively. Furthermore, we extend the LongFNT model to the streaming\nscenario, which is named SLongFNT , consisting of SLongFNT-Text and\nSLongFNT-Speech approaches to utilize long-content text and speech information.\nExperiments show that the proposed SLongFNT model achieves relative 26% and 17%\nWER reduction on LibriSpeech and GigaSpeech respectively while keeping a good\nlatency, compared to the FNT baseline. Overall, our proposed LongFNT and\nSLongFNT highlight the significance of considering long-content speech and\ntranscription knowledge for improving both non-streaming and streaming speech\nrecognition systems.\n","authors":["Xun Gong","Yu Wu","Jinyu Li","Shujie Liu","Rui Zhao","Xie Chen","Yanmin Qian"],"pdf_url":"https://arxiv.org/pdf/2403.13423v1.pdf","comment":"Accepted by TASLP 2024"},{"id":"http://arxiv.org/abs/2403.13356v1","updated":"2024-03-20T07:34:21Z","published":"2024-03-20T07:34:21Z","title":"KunquDB: An Attempt for Speaker Verification in the Chinese Opera\n  Scenario","summary":"  This work aims to promote Chinese opera research in both musical and speech\ndomains, with a primary focus on overcoming the data limitations. We introduce\nKunquDB, a relatively large-scale, well-annotated audio-visual dataset\ncomprising 339 speakers and 128 hours of content. Originating from the Kunqu\nOpera Art Canon (Kunqu yishu dadian), KunquDB is meticulously structured by\ndialogue lines, providing explicit annotations including character names,\nspeaker names, gender information, vocal manner classifications, and\naccompanied by preliminary text transcriptions. KunquDB provides a versatile\nfoundation for role-centric acoustic studies and advancements in speech-related\nresearch, including Automatic Speaker Verification (ASV). Beyond enriching\nopera research, this dataset bridges the gap between artistic expression and\ntechnological innovation. Pioneering the exploration of ASV in Chinese opera,\nwe construct four test trials considering two distinct vocal manners in opera\nvoices: stage speech (ST) and singing (S). Implementing domain adaptation\nmethods effectively mitigates domain mismatches induced by these vocal manner\nvariations while there is still room for further improvement as a benchmark.\n","authors":["Huali Zhou","Yuke Lin","Dong Liu","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2403.13356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13353v1","updated":"2024-03-20T07:31:27Z","published":"2024-03-20T07:31:27Z","title":"Building speech corpus with diverse voice characteristics for its\n  prompt-based representation","summary":"  In text-to-speech synthesis, the ability to control voice characteristics is\nvital for various applications. By leveraging thriving text prompt-based\ngeneration techniques, it should be possible to enhance the nuanced control of\nvoice characteristics. While previous research has explored the prompt-based\nmanipulation of voice characteristics, most studies have used pre-recorded\nspeech, which limits the diversity of voice characteristics available. Thus, we\naim to address this gap by creating a novel corpus and developing a model for\nprompt-based manipulation of voice characteristics in text-to-speech synthesis,\nfacilitating a broader range of voice characteristics. Specifically, we propose\na method to build a sizable corpus pairing voice characteristics descriptions\nwith corresponding speech samples. This involves automatically gathering\nvoice-related speech data from the Internet, ensuring its quality, and manually\nannotating it using crowdsourcing. We implement this method with Japanese\nlanguage data and analyze the results to validate its effectiveness.\nSubsequently, we propose a construction method of the model to retrieve speech\nfrom voice characteristics descriptions based on a contrastive learning method.\nWe train the model using not only conservative contrastive learning but also\nfeature prediction learning to predict quantitative speech features\ncorresponding to voice characteristics. We evaluate the model performance via\nexperiments with the corpus we constructed above.\n","authors":["Aya Watanabe","Shinnosuke Takamichi","Yuki Saito","Wataru Nakata","Detai Xin","Hiroshi Saruwatari"],"pdf_url":"https://arxiv.org/pdf/2403.13353v1.pdf","comment":"Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. arXiv admin note: text overlap with arXiv:2309.13509"},{"id":"http://arxiv.org/abs/2403.13332v1","updated":"2024-03-20T06:24:25Z","published":"2024-03-20T06:24:25Z","title":"TDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration\n  Transducer","summary":"  Designing an efficient keyword spotting (KWS) system that delivers\nexceptional performance on resource-constrained edge devices has long been a\nsubject of significant attention. Existing KWS search algorithms typically\nfollow a frame-synchronous approach, where search decisions are made repeatedly\nat each frame despite the fact that most frames are keyword-irrelevant. In this\npaper, we propose TDT-KWS, which leverages token-and-duration Transducers (TDT)\nfor KWS tasks. We also propose a novel KWS task-specific decoding algorithm for\nTransducer-based models, which supports highly effective frame-asynchronous\nkeyword search in streaming speech scenarios. With evaluations conducted on\nboth the public Hey Snips and self-constructed LibriKWS-20 datasets, our\nproposed KWS-decoding algorithm produces more accurate results than\nconventional ASR decoding algorithms. Additionally, TDT-KWS achieves on-par or\nbetter wake word detection performance than both RNN-T and traditional TDT-ASR\nsystems while achieving significant inference speed-up. Furthermore,\nexperiments show that TDT-KWS is more robust to noisy environments compared to\nRNN-T KWS.\n","authors":["Yu Xi","Hao Li","Baochen Yang","Haoyu Li","Hainan Xu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.13332v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2309.07081v2","updated":"2024-03-20T03:04:47Z","published":"2023-09-13T16:46:27Z","title":"Can Whisper perform speech-based in-context learning?","summary":"  This paper investigates the in-context learning abilities of the Whisper\nautomatic speech recognition (ASR) models released by OpenAI. A novel\nspeech-based in-context learning (SICL) approach is proposed for test-time\nadaptation, which can reduce the word error rates (WERs) with only a small\nnumber of labelled speech samples without gradient descent. Language-level\nadaptation experiments using Chinese dialects showed that when applying SICL to\nisolated word ASR, consistent and considerable relative WER reductions can be\nachieved using Whisper models of any size on two dialects, which is on average\n32.3%. A k-nearest-neighbours-based in-context example selection technique can\nbe applied to further improve the efficiency of SICL, which can increase the\naverage relative WER reduction to 36.4%. The findings are verified using\nspeaker adaptation or continuous speech recognition tasks, and both achieved\nconsiderable relative WER reductions. Detailed quantitative analyses are also\nprovided to shed light on SICL's adaptability to phonological variances and\ndialect-specific lexical nuances.\n","authors":["Siyin Wang","Chao-Han Huck Yang","Ji Wu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.07081v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.13254v1","updated":"2024-03-20T02:32:41Z","published":"2024-03-20T02:32:41Z","title":"Onset and offset weighted loss function for sound event detection","summary":"  In a typical sound event detection (SED) system, the existence of a sound\nevent is detected at a frame level, and consecutive frames with the same event\ndetected are combined as one sound event. The median filter is applied as a\npost-processing step to remove detection errors as much as possible. However,\ndetection errors occurring around the onset and offset of a sound event are\nbeyond the capacity of the median filter. To address this issue, an onset and\noffset weighted binary cross-entropy (OWBCE) loss function is proposed in this\npaper, which trains the DNN model to be more robust on frames around (a) onsets\nand offsets. Experiments are carried out in the context of DCASE 2022 task 4.\nResults show that OWBCE outperforms BCE when different models are considered.\nFor a basic CRNN, relative improvements of 6.43% in event-F1, 1.96% in PSDS1,\nand 2.43% in PSDS2 can be achieved by OWBCE.\n","authors":["Tao Song"],"pdf_url":"https://arxiv.org/pdf/2403.13254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13252v1","updated":"2024-03-20T02:32:13Z","published":"2024-03-20T02:32:13Z","title":"Frequency-aware convolution for sound event detection","summary":"  In sound event detection (SED), convolution neural networks (CNNs) are widely\nused to extract time-frequency patterns from the input spectrogram. However,\nfeatures extracted by CNN can be insensitive to the shift of time-frequency\npatterns along the frequency axis. To address this issue, frequency dynamic\nconvolution (FDY) has been proposed, which applies different kernels to\ndifferent frequency components. Compared to the vannila CNN, FDY requires\nseveral times more parameters. In this paper, a more efficient solution named\nfrequency-aware convolution (FAC) is proposed. In FAC, frequency-positional\ninformation is encoded in a vector and added to the input spectrogram. To match\nthe amplitude of input, the encoding vector is scaled adaptively and\nchannel-independently. Experiments are carried out in the context of DCASE 2022\ntask 4, and the results demonstrate that FAC can achieve comparable performance\nto that of FDY with only 515 additional parameters, while FDY requires 8.02\nmillion additional parameters. The ablation study shows that scaling the\nencoding vector adaptively and channel-independently is critical to the\nperformance of FAC.\n","authors":["Tao Song"],"pdf_url":"https://arxiv.org/pdf/2403.13252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08150v2","updated":"2024-03-20T02:17:16Z","published":"2023-09-15T04:34:40Z","title":"Unimodal Aggregation for CTC-based Speech Recognition","summary":"  This paper works on non-autoregressive automatic speech recognition. A\nunimodal aggregation (UMA) is proposed to segment and integrate the feature\nframes that belong to the same text token, and thus to learn better feature\nrepresentations for text tokens. The frame-wise features and weights are both\nderived from an encoder. Then, the feature frames with unimodal weights are\nintegrated and further processed by a decoder. Connectionist temporal\nclassification (CTC) loss is applied for training. Compared to the regular CTC,\nthe proposed method learns better feature representations and shortens the\nsequence length, resulting in lower recognition error and computational\ncomplexity. Experiments on three Mandarin datasets show that UMA demonstrates\nsuperior or comparable performance to other advanced non-autoregressive\nmethods, such as self-conditioned CTC. Moreover, by integrating\nself-conditioned CTC into the proposed framework, the performance can be\nfurther noticeably improved.\n","authors":["Ying Fang","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2309.08150v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.03378v3","updated":"2024-03-20T20:57:51Z","published":"2023-09-06T21:56:24Z","title":"RoDia: A New Dataset for Romanian Dialect Identification from Speech","summary":"  We introduce RoDia, the first dataset for Romanian dialect identification\nfrom speech. The RoDia dataset includes a varied compilation of speech samples\nfrom five distinct regions of Romania, covering both urban and rural\nenvironments, totaling 2 hours of manually annotated speech data. Along with\nour dataset, we introduce a set of competitive models to be used as baselines\nfor future research. The top scoring model achieves a macro F1 score of 59.83%\nand a micro F1 score of 62.08%, indicating that the task is challenging. We\nthus believe that RoDia is a valuable resource that will stimulate research\naiming to address the challenges of Romanian dialect identification. We release\nour dataset at https://github.com/codrut2/RoDia.\n","authors":["Codrut Rotaru","Nicolae-Catalin Ristea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2309.03378v3.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2307.03296v2","updated":"2024-03-20T19:08:06Z","published":"2023-07-06T21:10:50Z","title":"Gammatonegram Representation for End-to-End Dysarthric Speech Processing\n  Tasks: Speech Recognition, Speaker Identification, and Intelligibility\n  Assessment","summary":"  Dysarthria is a disability that causes a disturbance in the human speech\nsystem and reduces the quality and intelligibility of a person's speech.\nBecause of this effect, the normal speech processing systems can not work\nproperly on impaired speech. This disability is usually associated with\nphysical disabilities. Therefore, designing a system that can perform some\ntasks by receiving voice commands in the smart home can be a significant\nachievement. In this work, we introduce gammatonegram as an effective method to\nrepresent audio files with discriminative details, which is used as input for\nthe convolutional neural network. On the other word, we convert each speech\nfile into an image and propose image recognition system to classify speech in\ndifferent scenarios. Proposed CNN is based on the transfer learning method on\nthe pre-trained Alexnet. In this research, the efficiency of the proposed\nsystem for speech recognition, speaker identification, and intelligibility\nassessment is evaluated. According to the results on the UA dataset, the\nproposed speech recognition system achieved 91.29% accuracy in\nspeaker-dependent mode, the speaker identification system acquired 87.74%\naccuracy in text-dependent mode, and the intelligibility assessment system\nachieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network\nspeech recognition system that works fully automatically. This system is\nlocated in a cascade arrangement with the two-class intelligibility assessment\nsystem, and the output of this system activates each one of the speech\nrecognition networks. This architecture achieves an accuracy of 92.3% WRR. The\nsource code of this paper is available.\n","authors":["Aref Farhadipour","Hadi Veisi"],"pdf_url":"https://arxiv.org/pdf/2307.03296v2.pdf","comment":"12 pages, 8 figures. Iran J Comput Sci (2024)"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.12425v2","updated":"2024-03-20T13:56:56Z","published":"2024-03-19T04:25:54Z","title":"Multimodal Fusion Method with Spatiotemporal Sequences and Relationship\n  Learning for Valence-Arousal Estimation","summary":"  This paper presents our approach for the VA (Valence-Arousal) estimation task\nin the ABAW6 competition. We devised a comprehensive model by preprocessing\nvideo frames and audio segments to extract visual and audio features. Through\nthe utilization of Temporal Convolutional Network (TCN) modules, we effectively\ncaptured the temporal and spatial correlations between these features.\nSubsequently, we employed a Transformer encoder structure to learn long-range\ndependencies, thereby enhancing the model's performance and generalization\nability. Our method leverages a multimodal data fusion approach, integrating\npre-trained audio and video backbones for feature extraction, followed by\nTCN-based spatiotemporal encoding and Transformer-based temporal information\ncapture. Experimental results demonstrate the effectiveness of our approach,\nachieving competitive performance in VA estimation on the AffWild2 dataset.\n","authors":["Jun Yu","Gongpeng Zhao","Yongqi Wang","Zhihong Wei","Yang Zheng","Zerui Zhang","Zhongpeng Cai","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12425v2.pdf","comment":"8 pages,3 figures"},{"id":"http://arxiv.org/abs/2310.14016v3","updated":"2024-03-20T07:53:04Z","published":"2023-10-21T13:56:23Z","title":"SwG-former: A Sliding-Window Graph Convolutional Network for\n  Simultaneous Spatial-Temporal Information Extraction in Sound Event\n  Localization and Detection","summary":"  Sound event localization and detection (SELD) involves sound event detection\n(SED) and direction of arrival (DoA) estimation tasks. SED mainly relies on\ntemporal dependencies to distinguish different sound classes, while DoA\nestimation depends on spatial correlations to estimate source directions. This\npaper addresses the need to simultaneously extract spatial-temporal information\nin audio signals to improve SELD performance. A novel block, the sliding-window\ngraph-former (SwG-former), is designed to learn temporal context information of\nsound events based on their spatial correlations. The SwG-former block\ntransforms audio signals into a graph representation and constructs graph\nvertices to capture higher abstraction levels for spatial correlations. It uses\ndifferent-sized sliding windows to adapt various sound event durations and\naggregates temporal features with similar spatial information while\nincorporating multi-head self-attention (MHSA) to model global information.\nFurthermore, as the cornerstone of message passing, a robust Conv2dAgg function\nis proposed and embedded into the block to aggregate the features of neighbor\nvertices. As a result, a SwG-former model, which stacks the SwG-former blocks,\ndemonstrates superior performance compared to recent advanced SELD models. The\nSwG-former block is also integrated into the event-independent network version\n2 (EINV2), called SwG-EINV2, which surpasses the state-of-the-art (SOTA)\nmethods under the same acoustic environment.\n","authors":["Weiming Huang","Qinghua Huang","Liyan Ma","Chuan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10493v2","updated":"2024-03-20T06:05:00Z","published":"2024-03-15T17:27:42Z","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","summary":"  Diffusion-based audio and music generation models commonly generate music by\nconstructing an image representation of audio (e.g., a mel-spectrogram) and\nthen converting it to audio using a phase reconstruction model or vocoder.\nTypical vocoders, however, produce monophonic audio at lower resolutions (e.g.,\n16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an\nefficient high-fidelity stereophonic vocoder. Our method employs a cascade of\nthree generative adversarial networks (GANs) that convert low-resolution\nmel-spectrograms to audio, upsamples to high-resolution audio via bandwidth\nexpansion, and upmixes to stereophonic audio. Compared to previous work, we\npropose 1) a unified GAN-based generator and discriminator architecture and\ntraining procedure for each stage of our cascade, 2) a new fast, near\ndownsampling-compatible bandwidth extension module, and 3) a new fast\ndownmix-compatible mono-to-stereo upmixer that ensures the preservation of\nmonophonic content in the output. We evaluate our approach using both objective\nand subjective listening tests and find our approach yields comparable or\nbetter audio quality, better spatialization control, and significantly faster\ninference speed compared to past work. Sound examples are at\nhttps://MusicHiFi.github.io/web/.\n","authors":["Ge Zhu","Juan-Pablo Caceres","Zhiyao Duan","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2403.10493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13720v1","updated":"2024-03-20T16:27:13Z","published":"2024-03-20T16:27:13Z","title":"UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing\n  Using Discrete Speech Unit Challenge","summary":"  We present UTDUSS, the UTokyo-SaruLab system submitted to Interspeech2024\nSpeech Processing Using Discrete Speech Unit Challenge. The challenge focuses\non using discrete speech unit learned from large speech corpora for some tasks.\nWe submitted our UTDUSS system to two text-to-speech tracks: Vocoder and\nAcoustic+Vocoder. Our system incorporates neural audio codec (NAC) pre-trained\non only speech corpora, which makes the learned codec represent rich acoustic\nfeatures that are necessary for high-fidelity speech reconstruction. For the\nacoustic+vocoder track, we trained an acoustic model based on Transformer\nencoder-decoder that predicted the pre-trained NAC tokens from text input. We\ndescribe our strategies to build these models, such as data selection,\ndownsampling, and hyper-parameter tuning. Our system ranked in second and first\nfor the Vocoder and Acoustic+Vocoder tracks, respectively.\n","authors":["Wataru Nakata","Kazuki Yamauchi","Dong Yang","Hiroaki Hyodo","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2403.13720v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.13659v1","updated":"2024-03-20T15:08:43Z","published":"2024-03-20T15:08:43Z","title":"Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional\n  Emotion Recognition","summary":"  Multi-modal emotion recognition has recently gained a lot of attention since\nit can leverage diverse and complementary relationships over multiple\nmodalities, such as audio, visual, and text. Most state-of-the-art methods for\nmultimodal fusion rely on recurrent networks or conventional attention\nmechanisms that do not effectively leverage the complementary nature of the\nmodalities. In this paper, we focus on dimensional emotion recognition based on\nthe fusion of facial, vocal, and text modalities extracted from videos.\nSpecifically, we propose a recursive cross-modal attention (RCMA) to\neffectively capture the complementary relationships across the modalities in a\nrecursive fashion. The proposed model is able to effectively capture the\ninter-modal relationships by computing the cross-attention weights across the\nindividual modalities and the joint representation of the other two modalities.\nTo further improve the inter-modal relationships, the obtained attended\nfeatures of the individual modalities are again fed as input to the cross-modal\nattention to refine the feature representations of the individual modalities.\nIn addition to that, we have used Temporal convolution networks (TCNs) to\ncapture the temporal modeling (intra-modal relationships) of the individual\nmodalities. By deploying the TCNs as well cross-modal attention in a recursive\nfashion, we are able to effectively capture both intra- and inter-modal\nrelationships across the audio, visual, and text modalities. Experimental\nresults on validation-set videos from the AffWild2 dataset indicate that our\nproposed fusion model is able to achieve significant improvement over the\nbaseline for the sixth challenge of Affective Behavior Analysis in-the-Wild\n2024 (ABAW6) competition.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.13659v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2209.09068;\n  text overlap with arXiv:2203.14779 by other authors"},{"id":"http://arxiv.org/abs/2403.13643v1","updated":"2024-03-20T14:50:00Z","published":"2024-03-20T14:50:00Z","title":"Vibration Sensitivity of one-port and two-port MEMS microphones","summary":"  Micro-electro-mechanical system (MEMS) microphones (mics) with two acoustic\nports are currently receiving considerable interest, with the promise of\nachieving higher directional sensitivity compared to traditional one-port\narchitectures. However, measuring pressure differences in two-port microphones\ntypically commands sensing elements that are softer than in one-port mics, and\nare therefore presumably more prone to interference from external vibration.\nHere we derive a universal expression for microphone sensitivity to vibration\nand we experimentally demonstrate its validity for several emerging two-port\nmicrophone technologies. We also perform vibration measurements on a one-port\nmic, thus providing a one-stop direct comparison between one-port and two-port\nsensing approaches. We find that the acoustically-referred vibration\nsensitivity of two-port MEMS mics, in units of measured acoustic pressure per\nexternal acceleration (i.e., Pascals per g), does not depend on the sensing\nelement stiffness nor on its natural frequency. We also show that this\nvibration sensitivity in two-port mics is inversely proportional to frequency\nas opposed to the frequency independent behavior observed in one-port mics.\nThis is confirmed experimentally for several types of microphone packages.\n","authors":["Francis Doyon-D'Amour","Carly Stalder","Timothy Hodges","Michel Stephan","Lixiue Wu","Triantafillos Koukoulas","Stephane Leahy","Raphael St-Gelais"],"pdf_url":"https://arxiv.org/pdf/2403.13643v1.pdf","comment":"8 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.13465v1","updated":"2024-03-20T10:16:33Z","published":"2024-03-20T10:16:33Z","title":"BanglaNum -- A Public Dataset for Bengali Digit Recognition from Speech","summary":"  Automatic speech recognition (ASR) converts the human voice into readily\nunderstandable and categorized text or words. Although Bengali is one of the\nmost widely spoken languages in the world, there have been very few studies on\nBengali ASR, particularly on Bangladeshi-accented Bengali. In this study, audio\nrecordings of spoken digits (0-9) from university students were used to create\na Bengali speech digits dataset that may be employed to train artificial neural\nnetworks for voice-based digital input systems. This paper also compares the\nBengali digit recognition accuracy of several Convolutional Neural Networks\n(CNNs) using spectrograms and shows that a test accuracy of 98.23% is\nachievable using parameter-efficient models such as SqueezeNet on our dataset.\n","authors":["Mir Sayeed Mohammad","Azizul Zahid","Md Asif Iqbal"],"pdf_url":"https://arxiv.org/pdf/2403.13465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13423v1","updated":"2024-03-20T09:09:49Z","published":"2024-03-20T09:09:49Z","title":"Advanced Long-Content Speech Recognition With Factorized Neural\n  Transducer","summary":"  In this paper, we propose two novel approaches, which integrate long-content\ninformation into the factorized neural transducer (FNT) based architecture in\nboth non-streaming (referred to as LongFNT ) and streaming (referred to as\nSLongFNT ) scenarios. We first investigate whether long-content transcriptions\ncan improve the vanilla conformer transducer (C-T) models. Our experiments\nindicate that the vanilla C-T models do not exhibit improved performance when\nutilizing long-content transcriptions, possibly due to the predictor network of\nC-T models not functioning as a pure language model. Instead, FNT shows its\npotential in utilizing long-content information, where we propose the LongFNT\nmodel and explore the impact of long-content information in both text\n(LongFNT-Text) and speech (LongFNT-Speech). The proposed LongFNT-Text and\nLongFNT-Speech models further complement each other to achieve better\nperformance, with transcription history proving more valuable to the model. The\neffectiveness of our LongFNT approach is evaluated on LibriSpeech and\nGigaSpeech corpora, and obtains relative 19% and 12% word error rate reduction,\nrespectively. Furthermore, we extend the LongFNT model to the streaming\nscenario, which is named SLongFNT , consisting of SLongFNT-Text and\nSLongFNT-Speech approaches to utilize long-content text and speech information.\nExperiments show that the proposed SLongFNT model achieves relative 26% and 17%\nWER reduction on LibriSpeech and GigaSpeech respectively while keeping a good\nlatency, compared to the FNT baseline. Overall, our proposed LongFNT and\nSLongFNT highlight the significance of considering long-content speech and\ntranscription knowledge for improving both non-streaming and streaming speech\nrecognition systems.\n","authors":["Xun Gong","Yu Wu","Jinyu Li","Shujie Liu","Rui Zhao","Xie Chen","Yanmin Qian"],"pdf_url":"https://arxiv.org/pdf/2403.13423v1.pdf","comment":"Accepted by TASLP 2024"},{"id":"http://arxiv.org/abs/2403.13356v1","updated":"2024-03-20T07:34:21Z","published":"2024-03-20T07:34:21Z","title":"KunquDB: An Attempt for Speaker Verification in the Chinese Opera\n  Scenario","summary":"  This work aims to promote Chinese opera research in both musical and speech\ndomains, with a primary focus on overcoming the data limitations. We introduce\nKunquDB, a relatively large-scale, well-annotated audio-visual dataset\ncomprising 339 speakers and 128 hours of content. Originating from the Kunqu\nOpera Art Canon (Kunqu yishu dadian), KunquDB is meticulously structured by\ndialogue lines, providing explicit annotations including character names,\nspeaker names, gender information, vocal manner classifications, and\naccompanied by preliminary text transcriptions. KunquDB provides a versatile\nfoundation for role-centric acoustic studies and advancements in speech-related\nresearch, including Automatic Speaker Verification (ASV). Beyond enriching\nopera research, this dataset bridges the gap between artistic expression and\ntechnological innovation. Pioneering the exploration of ASV in Chinese opera,\nwe construct four test trials considering two distinct vocal manners in opera\nvoices: stage speech (ST) and singing (S). Implementing domain adaptation\nmethods effectively mitigates domain mismatches induced by these vocal manner\nvariations while there is still room for further improvement as a benchmark.\n","authors":["Huali Zhou","Yuke Lin","Dong Liu","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2403.13356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13353v1","updated":"2024-03-20T07:31:27Z","published":"2024-03-20T07:31:27Z","title":"Building speech corpus with diverse voice characteristics for its\n  prompt-based representation","summary":"  In text-to-speech synthesis, the ability to control voice characteristics is\nvital for various applications. By leveraging thriving text prompt-based\ngeneration techniques, it should be possible to enhance the nuanced control of\nvoice characteristics. While previous research has explored the prompt-based\nmanipulation of voice characteristics, most studies have used pre-recorded\nspeech, which limits the diversity of voice characteristics available. Thus, we\naim to address this gap by creating a novel corpus and developing a model for\nprompt-based manipulation of voice characteristics in text-to-speech synthesis,\nfacilitating a broader range of voice characteristics. Specifically, we propose\na method to build a sizable corpus pairing voice characteristics descriptions\nwith corresponding speech samples. This involves automatically gathering\nvoice-related speech data from the Internet, ensuring its quality, and manually\nannotating it using crowdsourcing. We implement this method with Japanese\nlanguage data and analyze the results to validate its effectiveness.\nSubsequently, we propose a construction method of the model to retrieve speech\nfrom voice characteristics descriptions based on a contrastive learning method.\nWe train the model using not only conservative contrastive learning but also\nfeature prediction learning to predict quantitative speech features\ncorresponding to voice characteristics. We evaluate the model performance via\nexperiments with the corpus we constructed above.\n","authors":["Aya Watanabe","Shinnosuke Takamichi","Yuki Saito","Wataru Nakata","Detai Xin","Hiroshi Saruwatari"],"pdf_url":"https://arxiv.org/pdf/2403.13353v1.pdf","comment":"Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. arXiv admin note: text overlap with arXiv:2309.13509"},{"id":"http://arxiv.org/abs/2403.13332v1","updated":"2024-03-20T06:24:25Z","published":"2024-03-20T06:24:25Z","title":"TDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration\n  Transducer","summary":"  Designing an efficient keyword spotting (KWS) system that delivers\nexceptional performance on resource-constrained edge devices has long been a\nsubject of significant attention. Existing KWS search algorithms typically\nfollow a frame-synchronous approach, where search decisions are made repeatedly\nat each frame despite the fact that most frames are keyword-irrelevant. In this\npaper, we propose TDT-KWS, which leverages token-and-duration Transducers (TDT)\nfor KWS tasks. We also propose a novel KWS task-specific decoding algorithm for\nTransducer-based models, which supports highly effective frame-asynchronous\nkeyword search in streaming speech scenarios. With evaluations conducted on\nboth the public Hey Snips and self-constructed LibriKWS-20 datasets, our\nproposed KWS-decoding algorithm produces more accurate results than\nconventional ASR decoding algorithms. Additionally, TDT-KWS achieves on-par or\nbetter wake word detection performance than both RNN-T and traditional TDT-ASR\nsystems while achieving significant inference speed-up. Furthermore,\nexperiments show that TDT-KWS is more robust to noisy environments compared to\nRNN-T KWS.\n","authors":["Yu Xi","Hao Li","Baochen Yang","Haoyu Li","Hainan Xu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.13332v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2309.07081v2","updated":"2024-03-20T03:04:47Z","published":"2023-09-13T16:46:27Z","title":"Can Whisper perform speech-based in-context learning?","summary":"  This paper investigates the in-context learning abilities of the Whisper\nautomatic speech recognition (ASR) models released by OpenAI. A novel\nspeech-based in-context learning (SICL) approach is proposed for test-time\nadaptation, which can reduce the word error rates (WERs) with only a small\nnumber of labelled speech samples without gradient descent. Language-level\nadaptation experiments using Chinese dialects showed that when applying SICL to\nisolated word ASR, consistent and considerable relative WER reductions can be\nachieved using Whisper models of any size on two dialects, which is on average\n32.3%. A k-nearest-neighbours-based in-context example selection technique can\nbe applied to further improve the efficiency of SICL, which can increase the\naverage relative WER reduction to 36.4%. The findings are verified using\nspeaker adaptation or continuous speech recognition tasks, and both achieved\nconsiderable relative WER reductions. Detailed quantitative analyses are also\nprovided to shed light on SICL's adaptability to phonological variances and\ndialect-specific lexical nuances.\n","authors":["Siyin Wang","Chao-Han Huck Yang","Ji Wu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.07081v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.13254v1","updated":"2024-03-20T02:32:41Z","published":"2024-03-20T02:32:41Z","title":"Onset and offset weighted loss function for sound event detection","summary":"  In a typical sound event detection (SED) system, the existence of a sound\nevent is detected at a frame level, and consecutive frames with the same event\ndetected are combined as one sound event. The median filter is applied as a\npost-processing step to remove detection errors as much as possible. However,\ndetection errors occurring around the onset and offset of a sound event are\nbeyond the capacity of the median filter. To address this issue, an onset and\noffset weighted binary cross-entropy (OWBCE) loss function is proposed in this\npaper, which trains the DNN model to be more robust on frames around (a) onsets\nand offsets. Experiments are carried out in the context of DCASE 2022 task 4.\nResults show that OWBCE outperforms BCE when different models are considered.\nFor a basic CRNN, relative improvements of 6.43% in event-F1, 1.96% in PSDS1,\nand 2.43% in PSDS2 can be achieved by OWBCE.\n","authors":["Tao Song"],"pdf_url":"https://arxiv.org/pdf/2403.13254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13253v1","updated":"2024-03-20T02:32:24Z","published":"2024-03-20T02:32:24Z","title":"Document Author Classification Using Parsed Language Structure","summary":"  Over the years there has been ongoing interest in detecting authorship of a\ntext based on statistical properties of the text, such as by using occurrence\nrates of noncontextual words. In previous work, these techniques have been\nused, for example, to determine authorship of all of \\emph{The Federalist\nPapers}. Such methods may be useful in more modern times to detect fake or AI\nauthorship. Progress in statistical natural language parsers introduces the\npossibility of using grammatical structure to detect authorship. In this paper\nwe explore a new possibility for detecting authorship using grammatical\nstructural information extracted using a statistical natural language parser.\nThis paper provides a proof of concept, testing author classification based on\ngrammatical structure on a set of \"proof texts,\" The Federalist Papers and\nSanditon which have been as test cases in previous authorship detection\nstudies. Several features extracted from the statistical natural language\nparser were explored: all subtrees of some depth from any level; rooted\nsubtrees of some depth, part of speech, and part of speech by level in the\nparse tree. It was found to be helpful to project the features into a lower\ndimensional space. Statistical experiments on these documents demonstrate that\ninformation from a statistical parser can, in fact, assist in distinguishing\nauthors.\n","authors":["Todd K Moon","Jacob H. Gunther"],"pdf_url":"https://arxiv.org/pdf/2403.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13252v1","updated":"2024-03-20T02:32:13Z","published":"2024-03-20T02:32:13Z","title":"Frequency-aware convolution for sound event detection","summary":"  In sound event detection (SED), convolution neural networks (CNNs) are widely\nused to extract time-frequency patterns from the input spectrogram. However,\nfeatures extracted by CNN can be insensitive to the shift of time-frequency\npatterns along the frequency axis. To address this issue, frequency dynamic\nconvolution (FDY) has been proposed, which applies different kernels to\ndifferent frequency components. Compared to the vannila CNN, FDY requires\nseveral times more parameters. In this paper, a more efficient solution named\nfrequency-aware convolution (FAC) is proposed. In FAC, frequency-positional\ninformation is encoded in a vector and added to the input spectrogram. To match\nthe amplitude of input, the encoding vector is scaled adaptively and\nchannel-independently. Experiments are carried out in the context of DCASE 2022\ntask 4, and the results demonstrate that FAC can achieve comparable performance\nto that of FDY with only 515 additional parameters, while FDY requires 8.02\nmillion additional parameters. The ablation study shows that scaling the\nencoding vector adaptively and channel-independently is critical to the\nperformance of FAC.\n","authors":["Tao Song"],"pdf_url":"https://arxiv.org/pdf/2403.13252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08150v2","updated":"2024-03-20T02:17:16Z","published":"2023-09-15T04:34:40Z","title":"Unimodal Aggregation for CTC-based Speech Recognition","summary":"  This paper works on non-autoregressive automatic speech recognition. A\nunimodal aggregation (UMA) is proposed to segment and integrate the feature\nframes that belong to the same text token, and thus to learn better feature\nrepresentations for text tokens. The frame-wise features and weights are both\nderived from an encoder. Then, the feature frames with unimodal weights are\nintegrated and further processed by a decoder. Connectionist temporal\nclassification (CTC) loss is applied for training. Compared to the regular CTC,\nthe proposed method learns better feature representations and shortens the\nsequence length, resulting in lower recognition error and computational\ncomplexity. Experiments on three Mandarin datasets show that UMA demonstrates\nsuperior or comparable performance to other advanced non-autoregressive\nmethods, such as self-conditioned CTC. Moreover, by integrating\nself-conditioned CTC into the proposed framework, the performance can be\nfurther noticeably improved.\n","authors":["Ying Fang","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2309.08150v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.03378v3","updated":"2024-03-20T20:57:51Z","published":"2023-09-06T21:56:24Z","title":"RoDia: A New Dataset for Romanian Dialect Identification from Speech","summary":"  We introduce RoDia, the first dataset for Romanian dialect identification\nfrom speech. The RoDia dataset includes a varied compilation of speech samples\nfrom five distinct regions of Romania, covering both urban and rural\nenvironments, totaling 2 hours of manually annotated speech data. Along with\nour dataset, we introduce a set of competitive models to be used as baselines\nfor future research. The top scoring model achieves a macro F1 score of 59.83%\nand a micro F1 score of 62.08%, indicating that the task is challenging. We\nthus believe that RoDia is a valuable resource that will stimulate research\naiming to address the challenges of Romanian dialect identification. We release\nour dataset at https://github.com/codrut2/RoDia.\n","authors":["Codrut Rotaru","Nicolae-Catalin Ristea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2309.03378v3.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2307.03296v2","updated":"2024-03-20T19:08:06Z","published":"2023-07-06T21:10:50Z","title":"Gammatonegram Representation for End-to-End Dysarthric Speech Processing\n  Tasks: Speech Recognition, Speaker Identification, and Intelligibility\n  Assessment","summary":"  Dysarthria is a disability that causes a disturbance in the human speech\nsystem and reduces the quality and intelligibility of a person's speech.\nBecause of this effect, the normal speech processing systems can not work\nproperly on impaired speech. This disability is usually associated with\nphysical disabilities. Therefore, designing a system that can perform some\ntasks by receiving voice commands in the smart home can be a significant\nachievement. In this work, we introduce gammatonegram as an effective method to\nrepresent audio files with discriminative details, which is used as input for\nthe convolutional neural network. On the other word, we convert each speech\nfile into an image and propose image recognition system to classify speech in\ndifferent scenarios. Proposed CNN is based on the transfer learning method on\nthe pre-trained Alexnet. In this research, the efficiency of the proposed\nsystem for speech recognition, speaker identification, and intelligibility\nassessment is evaluated. According to the results on the UA dataset, the\nproposed speech recognition system achieved 91.29% accuracy in\nspeaker-dependent mode, the speaker identification system acquired 87.74%\naccuracy in text-dependent mode, and the intelligibility assessment system\nachieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network\nspeech recognition system that works fully automatically. This system is\nlocated in a cascade arrangement with the two-class intelligibility assessment\nsystem, and the output of this system activates each one of the speech\nrecognition networks. This architecture achieves an accuracy of 92.3% WRR. The\nsource code of this paper is available.\n","authors":["Aref Farhadipour","Hadi Veisi"],"pdf_url":"https://arxiv.org/pdf/2307.03296v2.pdf","comment":"12 pages, 8 figures. Iran J Comput Sci (2024)"},{"id":"http://arxiv.org/abs/2403.13922v1","updated":"2024-03-20T18:49:59Z","published":"2024-03-20T18:49:59Z","title":"Visually Grounded Speech Models have a Mutual Exclusivity Bias","summary":"  When children learn new words, they employ constraints such as the mutual\nexclusivity (ME) bias: a novel word is mapped to a novel object rather than a\nfamiliar one. This bias has been studied computationally, but only in models\nthat use discrete word representations as input, ignoring the high variability\nof spoken words. We investigate the ME bias in the context of visually grounded\nspeech models that learn from natural images and continuous speech audio.\nConcretely, we train a model on familiar words and test its ME bias by asking\nit to select between a novel and a familiar object when queried with a novel\nword. To simulate prior acoustic and visual knowledge, we experiment with\nseveral initialisation strategies using pretrained speech and vision networks.\nOur findings reveal the ME bias across the different initialisation approaches,\nwith a stronger bias in models with more prior (in particular, visual)\nknowledge. Additional tests confirm the robustness of our results, even when\ndifferent loss functions are considered.\n","authors":["Leanne Nortje","Dan Oneaţă","Yevgen Matusevych","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2403.13922v1.pdf","comment":"Accepted to TACL, pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2403.15469v1","updated":"2024-03-20T08:52:40Z","published":"2024-03-20T08:52:40Z","title":"Isometric Neural Machine Translation using Phoneme Count Ratio\n  Reward-based Reinforcement Learning","summary":"  Traditional Automatic Video Dubbing (AVD) pipeline consists of three key\nmodules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation\n(NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms\nare employed to regulate the length of the synthesized output text. This is\ndone to guarantee synchronization with respect to the alignment of video and\naudio subsequent to the dubbing process. Previous approaches have focused on\naligning the number of characters and words in the source and target language\ntexts of Machine Translation models. However, our approach aims to align the\nnumber of phonemes instead, as they are closely associated with speech\nduration. In this paper, we present the development of an isometric NMT system\nusing Reinforcement Learning (RL), with a focus on optimizing the alignment of\nphoneme counts in the source and target language sentence pairs. To evaluate\nour models, we propose the Phoneme Count Compliance (PCC) score, which is a\nmeasure of length compliance. Our approach demonstrates a substantial\nimprovement of approximately 36% in the PCC score compared to the\nstate-of-the-art models when applied to English-Hindi language pairs. Moreover,\nwe propose a student-teacher architecture within the framework of our RL\napproach to maintain a trade-off between the phoneme count and translation\nquality.\n","authors":["Shivam Ratnakant Mhaskar","Nirmesh J. Shah","Mohammadi Zaki","Ashishkumar P. Gudmalwar","Pankaj Wasnik","Rajiv Ratn Shah"],"pdf_url":"https://arxiv.org/pdf/2403.15469v1.pdf","comment":"Accepted in NAACL2024 Findings"}]},"2024-03-21T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.11879v2","updated":"2024-03-21T16:15:52Z","published":"2024-03-18T15:32:02Z","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton","summary":"  In this study, we propose a methodology for the Emotional Mimicry Intensity\n(EMI) Estimation task within the context of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0\nframework, pre-trained on a comprehensive podcast dataset, to extract a broad\nrange of audio features encompassing both linguistic and paralinguistic\nelements. We enhance feature representation through a fusion technique that\nintegrates individual features with a global mean vector, introducing global\ncontextual insights into our analysis. Additionally, we incorporate a\npre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.\nOur fusion employs a Long Short-Term Memory (LSTM) architecture for efficient\ntemporal analysis of audio data. Utilizing only the provided audio data, our\napproach demonstrates significant improvements over the established baseline.\n","authors":["Tobias Hallmen","Fabian Deuser","Norbert Oswald","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2403.11879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14402v1","updated":"2024-03-21T13:52:17Z","published":"2024-03-21T13:52:17Z","title":"XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for\n  Noise-Robust Speech Perception","summary":"  Speech recognition and translation systems perform poorly on noisy inputs,\nwhich are frequent in realistic environments. Augmenting these systems with\nvisual signals has the potential to improve robustness to noise. However,\naudio-visual (AV) data is only available in limited amounts and for fewer\nlanguages than audio-only resources. To address this gap, we present XLAVS-R, a\ncross-lingual audio-visual speech representation model for noise-robust speech\nrecognition and translation in over 100 languages. It is designed to maximize\nthe benefits of limited multilingual AV pre-training data, by building on top\nof audio-only multilingual pre-training and simplifying existing pre-training\nschemes. Extensive evaluation on the MuAViC benchmark shows the strength of\nXLAVS-R on downstream audio-visual speech recognition and translation tasks,\nwhere it outperforms the previous state of the art by up to 18.5% WER and 4.7\nBLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability\nwith audio-only fine-tuning.\n","authors":["HyoJung Han","Mohamed Anwar","Juan Pino","Wei-Ning Hsu","Marine Carpuat","Bowen Shi","Changhan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14290v1","updated":"2024-03-21T10:54:21Z","published":"2024-03-21T10:54:21Z","title":"Exploring Green AI for Audio Deepfake Detection","summary":"  The state-of-the-art audio deepfake detectors leveraging deep neural networks\nexhibit impressive recognition performance. Nonetheless, this advantage is\naccompanied by a significant carbon footprint. This is mainly due to the use of\nhigh-performance computing with accelerators and high training time. Studies\nshow that average deep NLP model produces around 626k lbs of\nCO\\textsubscript{2} which is equivalent to five times of average US car\nemission at its lifetime. This is certainly a massive threat to the\nenvironment. To tackle this challenge, this study presents a novel framework\nfor audio deepfake detection that can be seamlessly trained using standard CPU\nresources. Our proposed framework utilizes off-the-shelve self-supervised\nlearning (SSL) based models which are pre-trained and available in public\nrepositories. In contrast to existing methods that fine-tune SSL models and\nemploy additional deep neural networks for downstream tasks, we exploit\nclassical machine learning algorithms such as logistic regression and shallow\nneural networks using the SSL embeddings extracted using the pre-trained model.\nOur approach shows competitive results compared to the commonly used\nhigh-carbon footprint approaches. In experiments with the ASVspoof 2019 LA\ndataset, we achieve a 0.90\\% equal error rate (EER) with less than 1k trainable\nmodel parameters. To encourage further research in this direction and support\nreproducible results, the Python code will be made publicly accessible\nfollowing acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-\n","authors":["Subhajit Saha","Md Sahidullah","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2403.14290v1.pdf","comment":"This manuscript is under review in a conference"},{"id":"http://arxiv.org/abs/2403.14286v1","updated":"2024-03-21T10:49:54Z","published":"2024-03-21T10:49:54Z","title":"Assessing the Robustness of Spectral Clustering for Deep Speaker\n  Diarization","summary":"  Clustering speaker embeddings is crucial in speaker diarization but hasn't\nreceived as much focus as other components. Moreover, the robustness of speaker\ndiarization across various datasets hasn't been explored when the development\nand evaluation data are from different domains. To bridge this gap, this study\nthoroughly examines spectral clustering for both same-domain and cross-domain\nspeaker diarization. Our extensive experiments on two widely used corpora, AMI\nand DIHARD, reveal the performance trend of speaker diarization in the presence\nof domain mismatch. We observe that the performance difference between two\ndifferent domain conditions can be attributed to the role of spectral\nclustering. In particular, keeping other modules unchanged, we show that\ndifferences in optimal tuning parameters as well as speaker count estimation\noriginates due to the mismatch. This study opens several future directions for\nspeaker diarization research.\n","authors":["Nikhil Raghav","Md Sahidullah"],"pdf_url":"https://arxiv.org/pdf/2403.14286v1.pdf","comment":"Manuscript Under Review"},{"id":"http://arxiv.org/abs/2403.14268v1","updated":"2024-03-21T10:09:46Z","published":"2024-03-21T10:09:46Z","title":"Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by\n  Attention Constraints","summary":"  End-to-End Neural Diarization with Encoder-Decoder based Attractor (EEND-EDA)\nis an end-to-end neural model for automatic speaker segmentation and labeling.\nIt achieves the capability to handle flexible number of speakers by estimating\nthe number of attractors. EEND-EDA, however, struggles to accurately capture\nlocal speaker dynamics. This work proposes an auxiliary loss that aims to guide\nthe Transformer encoders at the lower layer of EEND-EDA model to enhance the\neffect of self-attention modules using speaker activity information. The\nresults evaluated on public dataset Mini LibriSpeech, demonstrates the\neffectiveness of the work, reducing Diarization Error Rate from 30.95% to\n28.17%. We will release the source code on GitHub to allow further research and\nreproducibility.\n","authors":["PeiYing Lee","HauYun Guo","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14268v1.pdf","comment":"Accepted to The 28th International Conference on Technologies and\n  Applications of Artificial Intelligence (TAAI), in Chinese language"},{"id":"http://arxiv.org/abs/2309.17189v4","updated":"2024-03-21T09:19:18Z","published":"2023-09-29T12:38:00Z","title":"RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual\n  Speech Separation","summary":"  Audio-visual speech separation methods aim to integrate different modalities\nto generate high-quality separated speech, thereby enhancing the performance of\ndownstream tasks such as speech recognition. Most existing state-of-the-art\n(SOTA) models operate in the time domain. However, their overly simplistic\napproach to modeling acoustic features often necessitates larger and more\ncomputationally intensive models in order to achieve SOTA performance. In this\npaper, we present a novel time-frequency domain audio-visual speech separation\nmethod: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies\nits algorithms on the complex time-frequency bins yielded by the Short-Time\nFourier Transform. We model and capture the time and frequency dimensions of\nthe audio independently using a multi-layered RNN along each dimension.\nFurthermore, we introduce a unique attention-based fusion technique for the\nefficient integration of audio and visual information, and a new mask\nseparation approach that takes advantage of the intrinsic spectral nature of\nthe acoustic features for a clearer separation. RTFS-Net outperforms the prior\nSOTA method in both inference speed and separation quality while reducing the\nnumber of parameters by 90% and MACs by 83%. This is the first time-frequency\ndomain audio-visual speech separation method to outperform all contemporary\ntime-domain counterparts.\n","authors":["Samuel Pegg","Kai Li","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2309.17189v4.pdf","comment":"Accepted by The Twelfth International Conference on Learning\n  Representations (ICLR) 2024, see https://openreview.net/forum?id=PEuDO2EiDr"},{"id":"http://arxiv.org/abs/2403.14179v1","updated":"2024-03-21T07:06:30Z","published":"2024-03-21T07:06:30Z","title":"AdaProj: Adaptively Scaled Angular Margin Subspace Projections for\n  Anomalous Sound Detection with Auxiliary Classification Tasks","summary":"  The state-of-the-art approach for semi-supervised anomalous sound detection\nis to first learn an embedding space by using auxiliary classification tasks\nbased on meta information or self-supervised learning and then estimate the\ndistribution of normal data. In this work, AdaProj a novel loss function is\npresented. In contrast to commonly used angular margin losses, which project\ndata of each class as close as possible to their corresponding class centers,\nAdaProj learns to project data onto class-specific subspaces. By doing so, the\nresulting distributions of embeddings belonging to normal data are not required\nto be as restrictive as other loss functions allowing a more detailed view on\nthe data. In experiments conducted on the DCASE2022 and DCASE2023 datasets, it\nis shown that using AdaProj to learn an embedding space significantly\noutperforms other commonly used loss functions and results in a\nstate-of-the-art performance on the DCASE2023 dataset.\n","authors":["Kevin Wilkinghoff"],"pdf_url":"https://arxiv.org/pdf/2403.14179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14083v1","updated":"2024-03-21T02:26:30Z","published":"2024-03-21T02:26:30Z","title":"emoDARTS: Joint Optimisation of CNN & Sequential Neural Network\n  Architectures for Superior Speech Emotion Recognition","summary":"  Speech Emotion Recognition (SER) is crucial for enabling computers to\nunderstand the emotions conveyed in human communication. With recent\nadvancements in Deep Learning (DL), the performance of SER models has\nsignificantly improved. However, designing an optimal DL architecture requires\nspecialised knowledge and experimental assessments. Fortunately, Neural\nArchitecture Search (NAS) provides a potential solution for automatically\ndetermining the best DL model. The Differentiable Architecture Search (DARTS)\nis a particularly efficient method for discovering optimal models. This study\npresents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network\n(SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature\nsupports the selection of CNN and LSTM coupling to improve performance.\n  While DARTS has previously been used to choose CNN and LSTM operations\nindependently, our technique adds a novel mechanism for selecting CNN and SeqNN\noperations in conjunction using DARTS. Unlike earlier work, we do not impose\nlimits on the layer order of the CNN. Instead, we let DARTS choose the best\nlayer order inside the DARTS cell. We demonstrate that emoDARTS outperforms\nconventionally designed CNN-LSTM models and surpasses the best-reported SER\nresults achieved through DARTS on CNN-LSTM by evaluating our approach on the\nIEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.\n","authors":["Thejan Rajapakshe","Rajib Rana","Sara Khalifa","Berrak Sisman","Bjorn W. Schuller","Carlos Busso"],"pdf_url":"https://arxiv.org/pdf/2403.14083v1.pdf","comment":"Submitted to IEEE Transactions on Affective Computing on February 19,\n  2024. arXiv admin note: text overlap with arXiv:2305.14402"},{"id":"http://arxiv.org/abs/2403.14048v1","updated":"2024-03-21T00:13:59Z","published":"2024-03-21T00:13:59Z","title":"The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio\n  Benchmarks and Novel Data","summary":"  The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine\nlearning (ML) experts from various audio domains. There are several valuable\naudio-driven ML tasks, from speech emotion recognition to audio event\ndetection, but the community is sparse compared to other ML areas, e.g.,\ncomputer vision or natural language processing. A major limitation with audio\nis the available data; with audio being a time-dependent modality, high-quality\ndata collection is time-consuming and costly, making it challenging for\nacademic groups to apply their often state-of-the-art strategies to a larger,\nmore generalizable dataset. In this short white paper, to encourage researchers\nwith limited access to large-datasets, the organizers first outline several\nopen-source datasets that are available to the community, and for the duration\nof the workshop are making several propriety datasets available. Namely, three\nvocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech\ndataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We\noutline the current baselines on these datasets but encourage researchers from\nacross audio to utilize them outside of the initial baseline tasks.\n","authors":["Alice Baird","Rachel Manzelli","Panagiotis Tzirakis","Chris Gagne","Haoqi Li","Sadie Allen","Sander Dieleman","Brian Kulis","Shrikanth S. Narayanan","Alan Cowen"],"pdf_url":"https://arxiv.org/pdf/2403.14048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00969v7","updated":"2024-03-21T17:52:22Z","published":"2023-05-01T17:56:32Z","title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds","summary":"  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries - and the accompanying CryCeleb 2023 task, which is a public\nspeaker verification challenge based on cry sounds. We released more than 6\nhours of manually segmented cry sounds from 786 newborns for academic use,\naiming to encourage research in infant cry analysis. The inaugural public\ncompetition attracted 59 participants, 11 of whom improved the baseline\nperformance. The top-performing system achieved a significant improvement\nscoring 25.8% equal error rate, which is still far from the performance of\nstate-of-the-art adult speaker verification systems. Therefore, we believe\nthere is room for further research on this dataset, potentially extending\nbeyond the verification task.\n","authors":["David Budaghyan","Charles C. Onu","Arsenii Gorin","Cem Subakan","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2305.00969v7.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2211.06687v4","updated":"2024-03-21T21:35:04Z","published":"2022-11-12T15:25:20Z","title":"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion\n  and Keyword-to-Caption Augmentation","summary":"  Contrastive learning has shown remarkable success in the field of multimodal\nrepresentation learning. In this paper, we propose a pipeline of contrastive\nlanguage-audio pretraining to develop an audio representation by combining\naudio data with natural language descriptions. To accomplish this target, we\nfirst release LAION-Audio-630K, a large collection of 633,526 audio-text pairs\nfrom different data sources. Second, we construct a contrastive language-audio\npretraining model by considering different audio encoders and text encoders. We\nincorporate the feature fusion mechanism and keyword-to-caption augmentation\ninto the model design to further enable the model to process audio inputs of\nvariable lengths and enhance the performance. Third, we perform comprehensive\nexperiments to evaluate our model across three tasks: text-to-audio retrieval,\nzero-shot audio classification, and supervised audio classification. The\nresults demonstrate that our model achieves superior performance in\ntext-to-audio retrieval task. In audio classification tasks, the model achieves\nstate-of-the-art performance in the zero-shot setting and is able to obtain\nperformance comparable to models' results in the non-zero-shot setting.\nLAION-Audio-630K and the proposed model are both available to the public.\n","authors":["Yusong Wu","Ke Chen","Tianyu Zhang","Yuchen Hui","Marianna Nezhurina","Taylor Berg-Kirkpatrick","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2211.06687v4.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.11879v2","updated":"2024-03-21T16:15:52Z","published":"2024-03-18T15:32:02Z","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton","summary":"  In this study, we propose a methodology for the Emotional Mimicry Intensity\n(EMI) Estimation task within the context of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0\nframework, pre-trained on a comprehensive podcast dataset, to extract a broad\nrange of audio features encompassing both linguistic and paralinguistic\nelements. We enhance feature representation through a fusion technique that\nintegrates individual features with a global mean vector, introducing global\ncontextual insights into our analysis. Additionally, we incorporate a\npre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.\nOur fusion employs a Long Short-Term Memory (LSTM) architecture for efficient\ntemporal analysis of audio data. Utilizing only the provided audio data, our\napproach demonstrates significant improvements over the established baseline.\n","authors":["Tobias Hallmen","Fabian Deuser","Norbert Oswald","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2403.11879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14438v1","updated":"2024-03-21T14:44:03Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wager","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.14402v1","updated":"2024-03-21T13:52:17Z","published":"2024-03-21T13:52:17Z","title":"XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for\n  Noise-Robust Speech Perception","summary":"  Speech recognition and translation systems perform poorly on noisy inputs,\nwhich are frequent in realistic environments. Augmenting these systems with\nvisual signals has the potential to improve robustness to noise. However,\naudio-visual (AV) data is only available in limited amounts and for fewer\nlanguages than audio-only resources. To address this gap, we present XLAVS-R, a\ncross-lingual audio-visual speech representation model for noise-robust speech\nrecognition and translation in over 100 languages. It is designed to maximize\nthe benefits of limited multilingual AV pre-training data, by building on top\nof audio-only multilingual pre-training and simplifying existing pre-training\nschemes. Extensive evaluation on the MuAViC benchmark shows the strength of\nXLAVS-R on downstream audio-visual speech recognition and translation tasks,\nwhere it outperforms the previous state of the art by up to 18.5% WER and 4.7\nBLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability\nwith audio-only fine-tuning.\n","authors":["HyoJung Han","Mohamed Anwar","Juan Pino","Wei-Ning Hsu","Marine Carpuat","Bowen Shi","Changhan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14268v1","updated":"2024-03-21T10:09:46Z","published":"2024-03-21T10:09:46Z","title":"Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by\n  Attention Constraints","summary":"  End-to-End Neural Diarization with Encoder-Decoder based Attractor (EEND-EDA)\nis an end-to-end neural model for automatic speaker segmentation and labeling.\nIt achieves the capability to handle flexible number of speakers by estimating\nthe number of attractors. EEND-EDA, however, struggles to accurately capture\nlocal speaker dynamics. This work proposes an auxiliary loss that aims to guide\nthe Transformer encoders at the lower layer of EEND-EDA model to enhance the\neffect of self-attention modules using speaker activity information. The\nresults evaluated on public dataset Mini LibriSpeech, demonstrates the\neffectiveness of the work, reducing Diarization Error Rate from 30.95% to\n28.17%. We will release the source code on GitHub to allow further research and\nreproducibility.\n","authors":["PeiYing Lee","HauYun Guo","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14268v1.pdf","comment":"Accepted to The 28th International Conference on Technologies and\n  Applications of Artificial Intelligence (TAAI), in Chinese language"},{"id":"http://arxiv.org/abs/2309.17189v4","updated":"2024-03-21T09:19:18Z","published":"2023-09-29T12:38:00Z","title":"RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual\n  Speech Separation","summary":"  Audio-visual speech separation methods aim to integrate different modalities\nto generate high-quality separated speech, thereby enhancing the performance of\ndownstream tasks such as speech recognition. Most existing state-of-the-art\n(SOTA) models operate in the time domain. However, their overly simplistic\napproach to modeling acoustic features often necessitates larger and more\ncomputationally intensive models in order to achieve SOTA performance. In this\npaper, we present a novel time-frequency domain audio-visual speech separation\nmethod: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies\nits algorithms on the complex time-frequency bins yielded by the Short-Time\nFourier Transform. We model and capture the time and frequency dimensions of\nthe audio independently using a multi-layered RNN along each dimension.\nFurthermore, we introduce a unique attention-based fusion technique for the\nefficient integration of audio and visual information, and a new mask\nseparation approach that takes advantage of the intrinsic spectral nature of\nthe acoustic features for a clearer separation. RTFS-Net outperforms the prior\nSOTA method in both inference speed and separation quality while reducing the\nnumber of parameters by 90% and MACs by 83%. This is the first time-frequency\ndomain audio-visual speech separation method to outperform all contemporary\ntime-domain counterparts.\n","authors":["Samuel Pegg","Kai Li","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2309.17189v4.pdf","comment":"Accepted by The Twelfth International Conference on Learning\n  Representations (ICLR) 2024, see https://openreview.net/forum?id=PEuDO2EiDr"},{"id":"http://arxiv.org/abs/2403.14246v1","updated":"2024-03-21T09:06:28Z","published":"2024-03-21T09:06:28Z","title":"CATSE: A Context-Aware Framework for Causal Target Sound Extraction","summary":"  Target Sound Extraction (TSE) focuses on the problem of separating sources of\ninterest, indicated by a user's cue, from the input mixture. Most existing\nsolutions operate in an offline fashion and are not suited to the low-latency\ncausal processing constraints imposed by applications in live-streamed content\nsuch as augmented hearing. We introduce a family of context-aware low-latency\ncausal TSE models suitable for real-time processing. First, we explore the\nutility of context by providing the TSE model with oracle information about\nwhat sound classes make up the input mixture, where the objective of the model\nis to extract one or more sources of interest indicated by the user. Since the\npractical applications of oracle models are limited due to their assumptions,\nwe introduce a composite multi-task training objective involving separation and\nclassification losses. Our evaluation involving single- and multi-source\nextraction shows the benefit of using context information in the model either\nby means of providing full context or via the proposed multi-task training loss\nwithout the need for full context information. Specifically, we show that our\nproposed model outperforms size- and latency-matched Waveformer, a\nstate-of-the-art model for real-time TSE.\n","authors":["Shrishail Baligar","Mikolaj Kegler","Bryce Irvin","Marko Stamenovic","Shawn Newsam"],"pdf_url":"https://arxiv.org/pdf/2403.14246v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.14179v1","updated":"2024-03-21T07:06:30Z","published":"2024-03-21T07:06:30Z","title":"AdaProj: Adaptively Scaled Angular Margin Subspace Projections for\n  Anomalous Sound Detection with Auxiliary Classification Tasks","summary":"  The state-of-the-art approach for semi-supervised anomalous sound detection\nis to first learn an embedding space by using auxiliary classification tasks\nbased on meta information or self-supervised learning and then estimate the\ndistribution of normal data. In this work, AdaProj a novel loss function is\npresented. In contrast to commonly used angular margin losses, which project\ndata of each class as close as possible to their corresponding class centers,\nAdaProj learns to project data onto class-specific subspaces. By doing so, the\nresulting distributions of embeddings belonging to normal data are not required\nto be as restrictive as other loss functions allowing a more detailed view on\nthe data. In experiments conducted on the DCASE2022 and DCASE2023 datasets, it\nis shown that using AdaProj to learn an embedding space significantly\noutperforms other commonly used loss functions and results in a\nstate-of-the-art performance on the DCASE2023 dataset.\n","authors":["Kevin Wilkinghoff"],"pdf_url":"https://arxiv.org/pdf/2403.14179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14083v1","updated":"2024-03-21T02:26:30Z","published":"2024-03-21T02:26:30Z","title":"emoDARTS: Joint Optimisation of CNN & Sequential Neural Network\n  Architectures for Superior Speech Emotion Recognition","summary":"  Speech Emotion Recognition (SER) is crucial for enabling computers to\nunderstand the emotions conveyed in human communication. With recent\nadvancements in Deep Learning (DL), the performance of SER models has\nsignificantly improved. However, designing an optimal DL architecture requires\nspecialised knowledge and experimental assessments. Fortunately, Neural\nArchitecture Search (NAS) provides a potential solution for automatically\ndetermining the best DL model. The Differentiable Architecture Search (DARTS)\nis a particularly efficient method for discovering optimal models. This study\npresents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network\n(SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature\nsupports the selection of CNN and LSTM coupling to improve performance.\n  While DARTS has previously been used to choose CNN and LSTM operations\nindependently, our technique adds a novel mechanism for selecting CNN and SeqNN\noperations in conjunction using DARTS. Unlike earlier work, we do not impose\nlimits on the layer order of the CNN. Instead, we let DARTS choose the best\nlayer order inside the DARTS cell. We demonstrate that emoDARTS outperforms\nconventionally designed CNN-LSTM models and surpasses the best-reported SER\nresults achieved through DARTS on CNN-LSTM by evaluating our approach on the\nIEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.\n","authors":["Thejan Rajapakshe","Rajib Rana","Sara Khalifa","Berrak Sisman","Bjorn W. Schuller","Carlos Busso"],"pdf_url":"https://arxiv.org/pdf/2403.14083v1.pdf","comment":"Submitted to IEEE Transactions on Affective Computing on February 19,\n  2024. arXiv admin note: text overlap with arXiv:2305.14402"},{"id":"http://arxiv.org/abs/2403.14048v1","updated":"2024-03-21T00:13:59Z","published":"2024-03-21T00:13:59Z","title":"The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio\n  Benchmarks and Novel Data","summary":"  The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine\nlearning (ML) experts from various audio domains. There are several valuable\naudio-driven ML tasks, from speech emotion recognition to audio event\ndetection, but the community is sparse compared to other ML areas, e.g.,\ncomputer vision or natural language processing. A major limitation with audio\nis the available data; with audio being a time-dependent modality, high-quality\ndata collection is time-consuming and costly, making it challenging for\nacademic groups to apply their often state-of-the-art strategies to a larger,\nmore generalizable dataset. In this short white paper, to encourage researchers\nwith limited access to large-datasets, the organizers first outline several\nopen-source datasets that are available to the community, and for the duration\nof the workshop are making several propriety datasets available. Namely, three\nvocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech\ndataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We\noutline the current baselines on these datasets but encourage researchers from\nacross audio to utilize them outside of the initial baseline tasks.\n","authors":["Alice Baird","Rachel Manzelli","Panagiotis Tzirakis","Chris Gagne","Haoqi Li","Sadie Allen","Sander Dieleman","Brian Kulis","Shrikanth S. Narayanan","Alan Cowen"],"pdf_url":"https://arxiv.org/pdf/2403.14048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00969v7","updated":"2024-03-21T17:52:22Z","published":"2023-05-01T17:56:32Z","title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds","summary":"  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries - and the accompanying CryCeleb 2023 task, which is a public\nspeaker verification challenge based on cry sounds. We released more than 6\nhours of manually segmented cry sounds from 786 newborns for academic use,\naiming to encourage research in infant cry analysis. The inaugural public\ncompetition attracted 59 participants, 11 of whom improved the baseline\nperformance. The top-performing system achieved a significant improvement\nscoring 25.8% equal error rate, which is still far from the performance of\nstate-of-the-art adult speaker verification systems. Therefore, we believe\nthere is room for further research on this dataset, potentially extending\nbeyond the verification task.\n","authors":["David Budaghyan","Charles C. Onu","Arsenii Gorin","Cem Subakan","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2305.00969v7.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.14290v1","updated":"2024-03-21T10:54:21Z","published":"2024-03-21T10:54:21Z","title":"Exploring Green AI for Audio Deepfake Detection","summary":"  The state-of-the-art audio deepfake detectors leveraging deep neural networks\nexhibit impressive recognition performance. Nonetheless, this advantage is\naccompanied by a significant carbon footprint. This is mainly due to the use of\nhigh-performance computing with accelerators and high training time. Studies\nshow that average deep NLP model produces around 626k lbs of\nCO\\textsubscript{2} which is equivalent to five times of average US car\nemission at its lifetime. This is certainly a massive threat to the\nenvironment. To tackle this challenge, this study presents a novel framework\nfor audio deepfake detection that can be seamlessly trained using standard CPU\nresources. Our proposed framework utilizes off-the-shelve self-supervised\nlearning (SSL) based models which are pre-trained and available in public\nrepositories. In contrast to existing methods that fine-tune SSL models and\nemploy additional deep neural networks for downstream tasks, we exploit\nclassical machine learning algorithms such as logistic regression and shallow\nneural networks using the SSL embeddings extracted using the pre-trained model.\nOur approach shows competitive results compared to the commonly used\nhigh-carbon footprint approaches. In experiments with the ASVspoof 2019 LA\ndataset, we achieve a 0.90\\% equal error rate (EER) with less than 1k trainable\nmodel parameters. To encourage further research in this direction and support\nreproducible results, the Python code will be made publicly accessible\nfollowing acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-\n","authors":["Subhajit Saha","Md Sahidullah","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2403.14290v1.pdf","comment":"This manuscript is under review in a conference"},{"id":"http://arxiv.org/abs/2403.14286v1","updated":"2024-03-21T10:49:54Z","published":"2024-03-21T10:49:54Z","title":"Assessing the Robustness of Spectral Clustering for Deep Speaker\n  Diarization","summary":"  Clustering speaker embeddings is crucial in speaker diarization but hasn't\nreceived as much focus as other components. Moreover, the robustness of speaker\ndiarization across various datasets hasn't been explored when the development\nand evaluation data are from different domains. To bridge this gap, this study\nthoroughly examines spectral clustering for both same-domain and cross-domain\nspeaker diarization. Our extensive experiments on two widely used corpora, AMI\nand DIHARD, reveal the performance trend of speaker diarization in the presence\nof domain mismatch. We observe that the performance difference between two\ndifferent domain conditions can be attributed to the role of spectral\nclustering. In particular, keeping other modules unchanged, we show that\ndifferences in optimal tuning parameters as well as speaker count estimation\noriginates due to the mismatch. This study opens several future directions for\nspeaker diarization research.\n","authors":["Nikhil Raghav","Md Sahidullah"],"pdf_url":"https://arxiv.org/pdf/2403.14286v1.pdf","comment":"Manuscript Under Review"},{"id":"http://arxiv.org/abs/2211.06687v4","updated":"2024-03-21T21:35:04Z","published":"2022-11-12T15:25:20Z","title":"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion\n  and Keyword-to-Caption Augmentation","summary":"  Contrastive learning has shown remarkable success in the field of multimodal\nrepresentation learning. In this paper, we propose a pipeline of contrastive\nlanguage-audio pretraining to develop an audio representation by combining\naudio data with natural language descriptions. To accomplish this target, we\nfirst release LAION-Audio-630K, a large collection of 633,526 audio-text pairs\nfrom different data sources. Second, we construct a contrastive language-audio\npretraining model by considering different audio encoders and text encoders. We\nincorporate the feature fusion mechanism and keyword-to-caption augmentation\ninto the model design to further enable the model to process audio inputs of\nvariable lengths and enhance the performance. Third, we perform comprehensive\nexperiments to evaluate our model across three tasks: text-to-audio retrieval,\nzero-shot audio classification, and supervised audio classification. The\nresults demonstrate that our model achieves superior performance in\ntext-to-audio retrieval task. In audio classification tasks, the model achieves\nstate-of-the-art performance in the zero-shot setting and is able to obtain\nperformance comparable to models' results in the non-zero-shot setting.\nLAION-Audio-630K and the proposed model are both available to the public.\n","authors":["Yusong Wu","Ke Chen","Tianyu Zhang","Yuchen Hui","Marianna Nezhurina","Taylor Berg-Kirkpatrick","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2211.06687v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14817v1","updated":"2024-03-21T20:14:53Z","published":"2024-03-21T20:14:53Z","title":"Crowdsourced Multilingual Speech Intelligibility Testing","summary":"  With the advent of generative audio features, there is an increasing need for\nrapid evaluation of their impact on speech intelligibility. Beyond the existing\nlaboratory measures, which are expensive and do not scale well, there has been\ncomparatively little work on crowdsourced assessment of intelligibility.\nStandards and recommendations are yet to be defined, and publicly available\nmultilingual test materials are lacking. In response to this challenge, we\npropose an approach for a crowdsourced intelligibility assessment. We detail\nthe test design, the collection and public release of the multilingual speech\ndata, and the results of our early experiments.\n","authors":["Laura Lechler","Kamil Wojcicki"],"pdf_url":"https://arxiv.org/pdf/2403.14817v1.pdf","comment":null}]},"2024-03-22T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.11879v3","updated":"2024-03-22T10:08:51Z","published":"2024-03-18T15:32:02Z","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediction","summary":"  In this study, we propose a methodology for the Emotional Mimicry Intensity\n(EMI) Estimation task within the context of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0\nframework, pre-trained on a comprehensive podcast dataset, to extract a broad\nrange of audio features encompassing both linguistic and paralinguistic\nelements. We enhance feature representation through a fusion technique that\nintegrates individual features with a global mean vector, introducing global\ncontextual insights into our analysis. Additionally, we incorporate a\npre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.\nOur fusion employs a Long Short-Term Memory (LSTM) architecture for efficient\ntemporal analysis of audio data. Utilizing only the provided audio data, our\napproach demonstrates significant improvements over the established baseline.\n","authors":["Tobias Hallmen","Fabian Deuser","Norbert Oswald","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2403.11879v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15569v1","updated":"2024-03-22T18:47:54Z","published":"2024-03-22T18:47:54Z","title":"Music to Dance as Language Translation using Sequence Models","summary":"  Synthesising appropriate choreographies from music remains an open problem.\nWe introduce MDLT, a novel approach that frames the choreography generation\nproblem as a translation task. Our method leverages an existing data set to\nlearn to translate sequences of audio into corresponding dance poses. We\npresent two variants of MDLT: one utilising the Transformer architecture and\nthe other employing the Mamba architecture. We train our method on AIST++ and\nPhantomDance data sets to teach a robotic arm to dance, but our method can be\napplied to a full humanoid robot. Evaluation metrics, including Average Joint\nError and Frechet Inception Distance, consistently demonstrate that, when given\na piece of music, MDLT excels at producing realistic and high-quality\nchoreography. The code can be found at github.com/meowatthemoon/MDLT.\n","authors":["André Correia","Luís A. Alexandre"],"pdf_url":"https://arxiv.org/pdf/2403.15569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11151v2","updated":"2024-03-22T17:21:20Z","published":"2023-05-18T17:43:03Z","title":"Unsupervised Multi-channel Separation and Adaptation","summary":"  A key challenge in machine learning is to generalize from training data to an\napplication domain of interest. This work generalizes the recently-proposed\nmixture invariant training (MixIT) algorithm to perform unsupervised learning\nin the multi-channel setting. We use MixIT to train a model on far-field\nmicrophone array recordings of overlapping reverberant and noisy speech from\nthe AMI Corpus. The models are trained on both supervised and unsupervised\ntraining data, and are tested on real AMI recordings containing overlapping\nspeech. To objectively evaluate our models, we also use a synthetic\nmulti-channel AMI test set. Holding network architectures constant, we find\nthat a fine-tuned semi-supervised model yields the largest improvement to\nSI-SNR and to human listening ratings across synthetic and real datasets,\noutperforming supervised models trained on well-matched synthetic data. Our\nresults demonstrate that unsupervised learning through MixIT enables model\nadaptation on both single- and multi-channel real-world speech recordings.\n","authors":["Cong Han","Kevin Wilson","Scott Wisdom","John R. Hershey"],"pdf_url":"https://arxiv.org/pdf/2305.11151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09510v2","updated":"2024-03-22T15:25:04Z","published":"2023-09-18T06:43:30Z","title":"Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\n  Instruction-Tuning Benchmark for Speech","summary":"  Text language models have shown remarkable zero-shot capability in\ngeneralizing to unseen tasks when provided with well-formulated instructions.\nHowever, existing studies in speech processing primarily focus on limited or\nspecific tasks. Moreover, the lack of standardized benchmarks hinders a fair\ncomparison across different approaches. Thus, we present Dynamic-SUPERB, a\nbenchmark designed for building universal speech models capable of leveraging\ninstruction tuning to perform multiple tasks in a zero-shot fashion. To achieve\ncomprehensive coverage of diverse speech tasks and harness instruction tuning,\nwe invite the community to collaborate and contribute, facilitating the dynamic\ngrowth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation\ninstances by combining 33 tasks and 22 datasets. This spans a broad spectrum of\ndimensions, providing a comprehensive platform for evaluation. Additionally, we\npropose several approaches to establish benchmark baselines. These include the\nutilization of speech models, text language models, and the multimodal encoder.\nEvaluation results indicate that while these baselines perform reasonably on\nseen tasks, they struggle with unseen ones. We release all materials to the\npublic and welcome researchers to collaborate on the project, advancing\ntechnologies in the field together.\n","authors":["Chien-yu Huang","Ke-Han Lu","Shih-Heng Wang","Chi-Yuan Hsiao","Chun-Yi Kuan","Haibin Wu","Siddhant Arora","Kai-Wei Chang","Jiatong Shi","Yifan Peng","Roshan Sharma","Shinji Watanabe","Bhiksha Ramakrishnan","Shady Shehata","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2309.09510v2.pdf","comment":"To appear in the proceedings of ICASSP 2024"},{"id":"http://arxiv.org/abs/2308.04025v3","updated":"2024-03-22T14:49:31Z","published":"2023-08-08T03:43:24Z","title":"MSAC: Multiple Speech Attribute Control Method for Reliable Speech\n  Emotion Recognition","summary":"  Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.\n","authors":["Yu Pan","Yuguang Yang","Yuheng Huang","Jixun Yao","Jingjing Yin","Yanni Hu","Heng Lu","Lei Ma","Jianjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.04025v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2212.10744v2","updated":"2024-03-22T14:33:35Z","published":"2022-12-21T03:28:30Z","title":"An Audio-Visual Speech Separation Model Inspired by\n  Cortico-Thalamo-Cortical Circuits","summary":"  Audio-visual approaches involving visual inputs have laid the foundation for\nrecent progress in speech separation. However, the optimization of the\nconcurrent usage of auditory and visual inputs is still an active research\narea. Inspired by the cortico-thalamo-cortical circuit, in which the sensory\nprocessing mechanisms of different modalities modulate one another via the\nnon-lemniscal sensory thalamus, we propose a novel cortico-thalamo-cortical\nneural network (CTCNet) for audio-visual speech separation (AVSS). First, the\nCTCNet learns hierarchical auditory and visual representations in a bottom-up\nmanner in separate auditory and visual subnetworks, mimicking the functions of\nthe auditory and visual cortical areas. Then, inspired by the large number of\nconnections between cortical regions and the thalamus, the model fuses the\nauditory and visual information in a thalamic subnetwork through top-down\nconnections. Finally, the model transmits this fused information back to the\nauditory and visual subnetworks, and the above process is repeated several\ntimes. The results of experiments on three speech separation benchmark datasets\nshow that CTCNet remarkably outperforms existing AVSS methods with considerably\nfewer parameters. These results suggest that mimicking the anatomical\nconnectome of the mammalian brain has great potential for advancing the\ndevelopment of deep neural networks. Project repo is\nhttps://github.com/JusperLee/CTCNet.\n","authors":["Kai Li","Fenghua Xie","Hang Chen","Kexin Yuan","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2212.10744v2.pdf","comment":"Accepted by TPAMI 2024"},{"id":"http://arxiv.org/abs/2310.06554v2","updated":"2024-03-22T14:27:04Z","published":"2023-10-10T12:09:56Z","title":"Modeling of Speech-dependent Own Voice Transfer Characteristics for\n  Hearables with In-ear Microphones","summary":"  Many hearables contain an in-ear microphone, which may be used to capture the\nown voice of its user. However, due to the hearable occluding the ear canal,\nthe in-ear microphone mostly records body-conducted speech, typically suffering\nfrom band-limitation effects and amplification at low frequencies. Since the\nocclusion effect is determined by the ratio between the air-conducted and\nbody-conducted components of own voice, the own voice transfer characteristics\nbetween the outer face of the hearable and the in-ear microphone depend on the\nspeech content and the individual talker. In this paper, we propose a\nspeech-dependent model of the own voice transfer characteristics based on\nphoneme recognition, assuming a linear time-invariant relative transfer\nfunction for each phoneme. We consider both individual models as well as models\naveraged over several talkers. Experimental results based on recordings with a\nprototype hearable show that the proposed speech-dependent model enables to\nsimulate in-ear signals more accurately than a speech-independent model in\nterms of technical measures, especially under utterance mismatch and talker\nmismatch. Additionally, simulation results show that talker-averaged models\ngeneralize better to different talkers than individual models.\n","authors":["Mattes Ohlenbusch","Christian Rollwage","Simon Doclo"],"pdf_url":"https://arxiv.org/pdf/2310.06554v2.pdf","comment":"20 pages, 11 figures; Extended version of arXiv:2309.08294 (more\n  detailed description of the problem, additional models considered, more\n  systematic evaluation conducted on a different, larger dataset) -> Updated\n  version (20th march 2024): major changes after internal review; in submission"},{"id":"http://arxiv.org/abs/2403.15523v1","updated":"2024-03-22T13:35:34Z","published":"2024-03-22T13:35:34Z","title":"Towards auditory attention decoding with noise-tagging: A pilot study","summary":"  Auditory attention decoding (AAD) aims to extract from brain activity the\nattended speaker amidst candidate speakers, offering promising applications for\nneuro-steered hearing devices and brain-computer interfacing. This pilot study\nmakes a first step towards AAD using the noise-tagging stimulus protocol, which\nevokes reliable code-modulated evoked potentials, but is minimally explored in\nthe auditory modality. Participants were sequentially presented with two Dutch\nspeech stimuli that were amplitude modulated with a unique binary pseudo-random\nnoise-code, effectively tagging these with additional decodable information. We\ncompared the decoding of unmodulated audio against audio modulated with various\nmodulation depths, and a conventional AAD method against a standard method to\ndecode noise-codes. Our pilot study revealed higher performances for the\nconventional method with 70 to 100 percent modulation depths compared to\nunmodulated audio. The noise-code decoder did not further improve these\nresults. These fundamental insights highlight the potential of integrating\nnoise-codes in speech to enhance auditory speaker detection when multiple\nspeakers are presented simultaneously.\n","authors":["H. A. Scheppink","S. Ahmadi","P. Desain","M. Tangermann","J. Thielen"],"pdf_url":"https://arxiv.org/pdf/2403.15523v1.pdf","comment":"6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.11879v3","updated":"2024-03-22T10:08:51Z","published":"2024-03-18T15:32:02Z","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediction","summary":"  In this study, we propose a methodology for the Emotional Mimicry Intensity\n(EMI) Estimation task within the context of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0\nframework, pre-trained on a comprehensive podcast dataset, to extract a broad\nrange of audio features encompassing both linguistic and paralinguistic\nelements. We enhance feature representation through a fusion technique that\nintegrates individual features with a global mean vector, introducing global\ncontextual insights into our analysis. Additionally, we incorporate a\npre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.\nOur fusion employs a Long Short-Term Memory (LSTM) architecture for efficient\ntemporal analysis of audio data. Utilizing only the provided audio data, our\napproach demonstrates significant improvements over the established baseline.\n","authors":["Tobias Hallmen","Fabian Deuser","Norbert Oswald","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2403.11879v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15569v1","updated":"2024-03-22T18:47:54Z","published":"2024-03-22T18:47:54Z","title":"Music to Dance as Language Translation using Sequence Models","summary":"  Synthesising appropriate choreographies from music remains an open problem.\nWe introduce MDLT, a novel approach that frames the choreography generation\nproblem as a translation task. Our method leverages an existing data set to\nlearn to translate sequences of audio into corresponding dance poses. We\npresent two variants of MDLT: one utilising the Transformer architecture and\nthe other employing the Mamba architecture. We train our method on AIST++ and\nPhantomDance data sets to teach a robotic arm to dance, but our method can be\napplied to a full humanoid robot. Evaluation metrics, including Average Joint\nError and Frechet Inception Distance, consistently demonstrate that, when given\na piece of music, MDLT excels at producing realistic and high-quality\nchoreography. The code can be found at github.com/meowatthemoon/MDLT.\n","authors":["André Correia","Luís A. Alexandre"],"pdf_url":"https://arxiv.org/pdf/2403.15569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11151v2","updated":"2024-03-22T17:21:20Z","published":"2023-05-18T17:43:03Z","title":"Unsupervised Multi-channel Separation and Adaptation","summary":"  A key challenge in machine learning is to generalize from training data to an\napplication domain of interest. This work generalizes the recently-proposed\nmixture invariant training (MixIT) algorithm to perform unsupervised learning\nin the multi-channel setting. We use MixIT to train a model on far-field\nmicrophone array recordings of overlapping reverberant and noisy speech from\nthe AMI Corpus. The models are trained on both supervised and unsupervised\ntraining data, and are tested on real AMI recordings containing overlapping\nspeech. To objectively evaluate our models, we also use a synthetic\nmulti-channel AMI test set. Holding network architectures constant, we find\nthat a fine-tuned semi-supervised model yields the largest improvement to\nSI-SNR and to human listening ratings across synthetic and real datasets,\noutperforming supervised models trained on well-matched synthetic data. Our\nresults demonstrate that unsupervised learning through MixIT enables model\nadaptation on both single- and multi-channel real-world speech recordings.\n","authors":["Cong Han","Kevin Wilson","Scott Wisdom","John R. Hershey"],"pdf_url":"https://arxiv.org/pdf/2305.11151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15336v1","updated":"2024-03-22T16:41:45Z","published":"2024-03-22T16:41:45Z","title":"Dialogue Understandability: Why are we streaming movies with subtitles?","summary":"  Watching movies and TV shows with subtitles enabled is not simply down to\naudibility or speech intelligibility. A variety of evolving factors related to\ntechnological advances, cinema production and social behaviour challenge our\nperception and understanding. This study seeks to formalise and give context to\nthese influential factors under a wider and novel term referred to as Dialogue\nUnderstandability. We propose a working definition for Dialogue\nUnderstandability being a listener's capacity to follow the story without undue\ncognitive effort or concentration being required that impacts their Quality of\nExperience (QoE). The paper identifies, describes and categorises the factors\nthat influence Dialogue Understandability mapping them over the QoE framework,\na media streaming lifecycle, and the stakeholders involved. We then explore\navailable measurement tools in the literature and link them to the factors they\ncould potentially be used for. The maturity and suitability of these tools is\nevaluated over a set of pilot experiments. Finally, we reflect on the gaps that\nstill need to be filled, what we can measure and what not, future subjective\nexperiments, and new research trends that could help us to fully characterise\nDialogue Understandability.\n","authors":["Helard Becerra","Alessandro Ragano","Diptasree Debnath","Asad Ullah","Crisron Rudolf Lucas","Martin Walsh","Andrew Hines"],"pdf_url":"https://arxiv.org/pdf/2403.15336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09510v2","updated":"2024-03-22T15:25:04Z","published":"2023-09-18T06:43:30Z","title":"Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\n  Instruction-Tuning Benchmark for Speech","summary":"  Text language models have shown remarkable zero-shot capability in\ngeneralizing to unseen tasks when provided with well-formulated instructions.\nHowever, existing studies in speech processing primarily focus on limited or\nspecific tasks. Moreover, the lack of standardized benchmarks hinders a fair\ncomparison across different approaches. Thus, we present Dynamic-SUPERB, a\nbenchmark designed for building universal speech models capable of leveraging\ninstruction tuning to perform multiple tasks in a zero-shot fashion. To achieve\ncomprehensive coverage of diverse speech tasks and harness instruction tuning,\nwe invite the community to collaborate and contribute, facilitating the dynamic\ngrowth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation\ninstances by combining 33 tasks and 22 datasets. This spans a broad spectrum of\ndimensions, providing a comprehensive platform for evaluation. Additionally, we\npropose several approaches to establish benchmark baselines. These include the\nutilization of speech models, text language models, and the multimodal encoder.\nEvaluation results indicate that while these baselines perform reasonably on\nseen tasks, they struggle with unseen ones. We release all materials to the\npublic and welcome researchers to collaborate on the project, advancing\ntechnologies in the field together.\n","authors":["Chien-yu Huang","Ke-Han Lu","Shih-Heng Wang","Chi-Yuan Hsiao","Chun-Yi Kuan","Haibin Wu","Siddhant Arora","Kai-Wei Chang","Jiatong Shi","Yifan Peng","Roshan Sharma","Shinji Watanabe","Bhiksha Ramakrishnan","Shady Shehata","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2309.09510v2.pdf","comment":"To appear in the proceedings of ICASSP 2024"},{"id":"http://arxiv.org/abs/2308.04025v3","updated":"2024-03-22T14:49:31Z","published":"2023-08-08T03:43:24Z","title":"MSAC: Multiple Speech Attribute Control Method for Reliable Speech\n  Emotion Recognition","summary":"  Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.\n","authors":["Yu Pan","Yuguang Yang","Yuheng Huang","Jixun Yao","Jingjing Yin","Yanni Hu","Heng Lu","Lei Ma","Jianjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.04025v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.06554v2","updated":"2024-03-22T14:27:04Z","published":"2023-10-10T12:09:56Z","title":"Modeling of Speech-dependent Own Voice Transfer Characteristics for\n  Hearables with In-ear Microphones","summary":"  Many hearables contain an in-ear microphone, which may be used to capture the\nown voice of its user. However, due to the hearable occluding the ear canal,\nthe in-ear microphone mostly records body-conducted speech, typically suffering\nfrom band-limitation effects and amplification at low frequencies. Since the\nocclusion effect is determined by the ratio between the air-conducted and\nbody-conducted components of own voice, the own voice transfer characteristics\nbetween the outer face of the hearable and the in-ear microphone depend on the\nspeech content and the individual talker. In this paper, we propose a\nspeech-dependent model of the own voice transfer characteristics based on\nphoneme recognition, assuming a linear time-invariant relative transfer\nfunction for each phoneme. We consider both individual models as well as models\naveraged over several talkers. Experimental results based on recordings with a\nprototype hearable show that the proposed speech-dependent model enables to\nsimulate in-ear signals more accurately than a speech-independent model in\nterms of technical measures, especially under utterance mismatch and talker\nmismatch. Additionally, simulation results show that talker-averaged models\ngeneralize better to different talkers than individual models.\n","authors":["Mattes Ohlenbusch","Christian Rollwage","Simon Doclo"],"pdf_url":"https://arxiv.org/pdf/2310.06554v2.pdf","comment":"20 pages, 11 figures; Extended version of arXiv:2309.08294 (more\n  detailed description of the problem, additional models considered, more\n  systematic evaluation conducted on a different, larger dataset) -> Updated\n  version (20th march 2024): major changes after internal review; in submission"},{"id":"http://arxiv.org/abs/2403.15523v1","updated":"2024-03-22T13:35:34Z","published":"2024-03-22T13:35:34Z","title":"Towards auditory attention decoding with noise-tagging: A pilot study","summary":"  Auditory attention decoding (AAD) aims to extract from brain activity the\nattended speaker amidst candidate speakers, offering promising applications for\nneuro-steered hearing devices and brain-computer interfacing. This pilot study\nmakes a first step towards AAD using the noise-tagging stimulus protocol, which\nevokes reliable code-modulated evoked potentials, but is minimally explored in\nthe auditory modality. Participants were sequentially presented with two Dutch\nspeech stimuli that were amplitude modulated with a unique binary pseudo-random\nnoise-code, effectively tagging these with additional decodable information. We\ncompared the decoding of unmodulated audio against audio modulated with various\nmodulation depths, and a conventional AAD method against a standard method to\ndecode noise-codes. Our pilot study revealed higher performances for the\nconventional method with 70 to 100 percent modulation depths compared to\nunmodulated audio. The noise-code decoder did not further improve these\nresults. These fundamental insights highlight the potential of integrating\nnoise-codes in speech to enhance auditory speaker detection when multiple\nspeakers are presented simultaneously.\n","authors":["H. A. Scheppink","S. Ahmadi","P. Desain","M. Tangermann","J. Thielen"],"pdf_url":"https://arxiv.org/pdf/2403.15523v1.pdf","comment":"6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024"},{"id":"http://arxiv.org/abs/2403.15510v1","updated":"2024-03-22T03:41:57Z","published":"2024-03-22T03:41:57Z","title":"Privacy-Preserving End-to-End Spoken Language Understanding","summary":"  Spoken language understanding (SLU), one of the key enabling technologies for\nhuman-computer interaction in IoT devices, provides an easy-to-use user\ninterface. Human speech can contain a lot of user-sensitive information, such\nas gender, identity, and sensitive content. New types of security and privacy\nbreaches have thus emerged. Users do not want to expose their personal\nsensitive information to malicious attacks by untrusted third parties. Thus,\nthe SLU system needs to ensure that a potential malicious attacker cannot\ndeduce the sensitive attributes of the users, while it should avoid greatly\ncompromising the SLU accuracy. To address the above challenge, this paper\nproposes a novel SLU multi-task privacy-preserving model to prevent both the\nspeech recognition (ASR) and identity recognition (IR) attacks. The model uses\nthe hidden layer separation technique so that SLU information is distributed\nonly in a specific portion of the hidden layer, and the other two types of\ninformation are removed to obtain a privacy-secure hidden layer. In order to\nachieve good balance between efficiency and privacy, we introduce a new\nmechanism of model pre-training, namely joint adversarial training, to further\nenhance the user privacy. Experiments over two SLU datasets show that the\nproposed method can reduce the accuracy of both the ASR and IR attacks close to\nthat of a random guess, while leaving the SLU performance largely unaffected.\n","authors":["Yinggui Wang","Wei Huang","Le Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15510v1.pdf","comment":"Accepted by IJCAI"}]},"2024-03-25T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.16610v1","updated":"2024-03-25T10:40:04Z","published":"2024-03-25T10:40:04Z","title":"Distributed collaborative anomalous sound detection by embedding sharing","summary":"  To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.\n","authors":["Kota Dohi","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2403.16610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16464v1","updated":"2024-03-25T06:46:27Z","published":"2024-03-25T06:46:27Z","title":"Training Generative Adversarial Network-Based Vocoder with Limited Data\n  Using Augmentation-Conditional Discriminator","summary":"  A generative adversarial network (GAN)-based vocoder trained with an\nadversarial discriminator is commonly used for speech synthesis because of its\nfast, lightweight, and high-quality characteristics. However, this data-driven\nmodel requires a large amount of training data incurring high data-collection\ncosts. This fact motivates us to train a GAN-based vocoder on limited data. A\npromising solution is to augment the training data to avoid overfitting.\nHowever, a standard discriminator is unconditional and insensitive to\ndistributional changes caused by data augmentation. Thus, augmented speech\n(which can be extraordinary) may be considered real speech. To address this\nissue, we propose an augmentation-conditional discriminator (AugCondD) that\nreceives the augmentation state as input in addition to speech, thereby\nassessing the input speech according to the augmentation state, without\ninhibiting the learning of the original non-augmented distribution.\nExperimental results indicate that AugCondD improves speech quality under\nlimited data conditions while achieving comparable speech quality under\nsufficient data conditions. Audio samples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.\n","authors":["Takuhiro Kaneko","Hirokazu Kameoka","Kou Tanaka"],"pdf_url":"https://arxiv.org/pdf/2403.16464v1.pdf","comment":"Accepted to ICASSP 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/"},{"id":"http://arxiv.org/abs/2309.02836v2","updated":"2024-03-25T03:17:30Z","published":"2023-09-06T08:48:03Z","title":"BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial\n  Network","summary":"  Generative adversarial network (GAN)-based vocoders have been intensively\nstudied because they can synthesize high-fidelity audio waveforms faster than\nreal-time. However, it has been reported that most GANs fail to obtain the\noptimal projection for discriminating between real and fake data in the feature\nspace. In the literature, it has been demonstrated that slicing adversarial\nnetwork (SAN), an improved GAN training framework that can find the optimal\nprojection, is effective in the image generation task. In this paper, we\ninvestigate the effectiveness of SAN in the vocoding task. For this purpose, we\npropose a scheme to modify least-squares GAN, which most GAN-based vocoders\nadopt, so that their loss functions satisfy the requirements of SAN. Through\nour experiments, we demonstrate that SAN can improve the performance of\nGAN-based vocoders, including BigVGAN, with small modifications. Our code is\navailable at https://github.com/sony/bigvsan.\n","authors":["Takashi Shibuya","Yuhta Takida","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2309.02836v2.pdf","comment":"Accepted at ICASSP 2024. Equation (5) in the previous version is\n  wrong. We modified it"},{"id":"http://arxiv.org/abs/2305.10666v3","updated":"2024-03-25T10:59:04Z","published":"2023-05-18T02:57:54Z","title":"A unified front-end framework for English text-to-speech synthesis","summary":"  The front-end is a critical component of English text-to-speech (TTS)\nsystems, responsible for extracting linguistic features that are essential for\na text-to-speech model to synthesize speech, such as prosodies and phonemes.\nThe English TTS front-end typically consists of a text normalization (TN)\nmodule, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme\n(G2P) module. However, current research on the English TTS front-end focuses\nsolely on individual modules, neglecting the interdependence between them and\nresulting in sub-optimal performance for each module. Therefore, this paper\nproposes a unified front-end framework that captures the dependencies among the\nEnglish TTS front-end modules. Extensive experiments have demonstrated that the\nproposed method achieves state-of-the-art (SOTA) performance in all modules.\n","authors":["Zelin Ying","Chen Li","Yu Dong","Qiuqiang Kong","Qiao Tian","Yuanyuan Huo","Yuxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2305.10666v3.pdf","comment":"Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.16338v2","updated":"2024-03-25T18:18:40Z","published":"2023-10-25T03:40:50Z","title":"Generative Pre-training for Speech with Flow Matching","summary":"  Generative models have gained more and more attention in recent years for\ntheir remarkable success in tasks that required estimating and sampling data\ndistribution to generate high-fidelity synthetic data. In speech,\ntext-to-speech synthesis and neural vocoder are good examples where generative\nmodels have shined. While generative models have been applied to different\napplications in speech, there exists no general-purpose generative model that\nmodels speech directly. In this work, we take a step toward this direction by\nshowing a single pre-trained generative model can be adapted to different\ndownstream tasks with strong performance. Specifically, we pre-trained a\ngenerative model, named SpeechFlow, on 60k hours of untranscribed speech with\nFlow Matching and masked conditions. Experiment results show the pre-trained\ngenerative model can be fine-tuned with task-specific data to match or surpass\nexisting expert models on speech enhancement, separation, and synthesis. Our\nwork suggested a foundational model for generation tasks in speech can be built\nwith generative pre-training.\n","authors":["Alexander H. Liu","Matt Le","Apoorv Vyas","Bowen Shi","Andros Tjandra","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.16338v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16973v1","updated":"2024-03-25T17:38:32Z","published":"2024-03-25T17:38:32Z","title":"VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild","summary":"  We introduce VoiceCraft, a token infilling neural codec language model, that\nachieves state-of-the-art performance on both speech editing and zero-shot\ntext-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft\nemploys a Transformer decoder architecture and introduces a token rearrangement\nprocedure that combines causal masking and delayed stacking to enable\ngeneration within an existing sequence. On speech editing tasks, VoiceCraft\nproduces edited speech that is nearly indistinguishable from unedited\nrecordings in terms of naturalness, as evaluated by humans; for zero-shot TTS,\nour model outperforms prior SotA models including VALLE and the popular\ncommercial model XTTS-v2. Crucially, the models are evaluated on challenging\nand realistic datasets, that consist of diverse accents, speaking styles,\nrecording conditions, and background noise and music, and our model performs\nconsistently well compared to other models and real recordings. In particular,\nfor speech editing evaluation, we introduce a high quality, challenging, and\nrealistic dataset named RealEdit. We encourage readers to listen to the demos\nat https://jasonppy.github.io/VoiceCraft_web.\n","authors":["Puyuan Peng","Po-Yao Huang","Daniel Li","Abdelrahman Mohamed","David Harwath"],"pdf_url":"https://arxiv.org/pdf/2403.16973v1.pdf","comment":"Data, code, and model weights are available at\n  https://github.com/jasonppy/VoiceCraft"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.16610v1","updated":"2024-03-25T10:40:04Z","published":"2024-03-25T10:40:04Z","title":"Distributed collaborative anomalous sound detection by embedding sharing","summary":"  To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.\n","authors":["Kota Dohi","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2403.16610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16464v1","updated":"2024-03-25T06:46:27Z","published":"2024-03-25T06:46:27Z","title":"Training Generative Adversarial Network-Based Vocoder with Limited Data\n  Using Augmentation-Conditional Discriminator","summary":"  A generative adversarial network (GAN)-based vocoder trained with an\nadversarial discriminator is commonly used for speech synthesis because of its\nfast, lightweight, and high-quality characteristics. However, this data-driven\nmodel requires a large amount of training data incurring high data-collection\ncosts. This fact motivates us to train a GAN-based vocoder on limited data. A\npromising solution is to augment the training data to avoid overfitting.\nHowever, a standard discriminator is unconditional and insensitive to\ndistributional changes caused by data augmentation. Thus, augmented speech\n(which can be extraordinary) may be considered real speech. To address this\nissue, we propose an augmentation-conditional discriminator (AugCondD) that\nreceives the augmentation state as input in addition to speech, thereby\nassessing the input speech according to the augmentation state, without\ninhibiting the learning of the original non-augmented distribution.\nExperimental results indicate that AugCondD improves speech quality under\nlimited data conditions while achieving comparable speech quality under\nsufficient data conditions. Audio samples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.\n","authors":["Takuhiro Kaneko","Hirokazu Kameoka","Kou Tanaka"],"pdf_url":"https://arxiv.org/pdf/2403.16464v1.pdf","comment":"Accepted to ICASSP 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/"},{"id":"http://arxiv.org/abs/2309.02836v2","updated":"2024-03-25T03:17:30Z","published":"2023-09-06T08:48:03Z","title":"BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial\n  Network","summary":"  Generative adversarial network (GAN)-based vocoders have been intensively\nstudied because they can synthesize high-fidelity audio waveforms faster than\nreal-time. However, it has been reported that most GANs fail to obtain the\noptimal projection for discriminating between real and fake data in the feature\nspace. In the literature, it has been demonstrated that slicing adversarial\nnetwork (SAN), an improved GAN training framework that can find the optimal\nprojection, is effective in the image generation task. In this paper, we\ninvestigate the effectiveness of SAN in the vocoding task. For this purpose, we\npropose a scheme to modify least-squares GAN, which most GAN-based vocoders\nadopt, so that their loss functions satisfy the requirements of SAN. Through\nour experiments, we demonstrate that SAN can improve the performance of\nGAN-based vocoders, including BigVGAN, with small modifications. Our code is\navailable at https://github.com/sony/bigvsan.\n","authors":["Takashi Shibuya","Yuhta Takida","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2309.02836v2.pdf","comment":"Accepted at ICASSP 2024. Equation (5) in the previous version is\n  wrong. We modified it"},{"id":"http://arxiv.org/abs/2305.10666v3","updated":"2024-03-25T10:59:04Z","published":"2023-05-18T02:57:54Z","title":"A unified front-end framework for English text-to-speech synthesis","summary":"  The front-end is a critical component of English text-to-speech (TTS)\nsystems, responsible for extracting linguistic features that are essential for\na text-to-speech model to synthesize speech, such as prosodies and phonemes.\nThe English TTS front-end typically consists of a text normalization (TN)\nmodule, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme\n(G2P) module. However, current research on the English TTS front-end focuses\nsolely on individual modules, neglecting the interdependence between them and\nresulting in sub-optimal performance for each module. Therefore, this paper\nproposes a unified front-end framework that captures the dependencies among the\nEnglish TTS front-end modules. Extensive experiments have demonstrated that the\nproposed method achieves state-of-the-art (SOTA) performance in all modules.\n","authors":["Zelin Ying","Chen Li","Yu Dong","Qiuqiang Kong","Qiao Tian","Yuanyuan Huo","Yuxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2305.10666v3.pdf","comment":"Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.16338v2","updated":"2024-03-25T18:18:40Z","published":"2023-10-25T03:40:50Z","title":"Generative Pre-training for Speech with Flow Matching","summary":"  Generative models have gained more and more attention in recent years for\ntheir remarkable success in tasks that required estimating and sampling data\ndistribution to generate high-fidelity synthetic data. In speech,\ntext-to-speech synthesis and neural vocoder are good examples where generative\nmodels have shined. While generative models have been applied to different\napplications in speech, there exists no general-purpose generative model that\nmodels speech directly. In this work, we take a step toward this direction by\nshowing a single pre-trained generative model can be adapted to different\ndownstream tasks with strong performance. Specifically, we pre-trained a\ngenerative model, named SpeechFlow, on 60k hours of untranscribed speech with\nFlow Matching and masked conditions. Experiment results show the pre-trained\ngenerative model can be fine-tuned with task-specific data to match or surpass\nexisting expert models on speech enhancement, separation, and synthesis. Our\nwork suggested a foundational model for generation tasks in speech can be built\nwith generative pre-training.\n","authors":["Alexander H. Liu","Matt Le","Apoorv Vyas","Bowen Shi","Andros Tjandra","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.16338v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16973v1","updated":"2024-03-25T17:38:32Z","published":"2024-03-25T17:38:32Z","title":"VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild","summary":"  We introduce VoiceCraft, a token infilling neural codec language model, that\nachieves state-of-the-art performance on both speech editing and zero-shot\ntext-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft\nemploys a Transformer decoder architecture and introduces a token rearrangement\nprocedure that combines causal masking and delayed stacking to enable\ngeneration within an existing sequence. On speech editing tasks, VoiceCraft\nproduces edited speech that is nearly indistinguishable from unedited\nrecordings in terms of naturalness, as evaluated by humans; for zero-shot TTS,\nour model outperforms prior SotA models including VALLE and the popular\ncommercial model XTTS-v2. Crucially, the models are evaluated on challenging\nand realistic datasets, that consist of diverse accents, speaking styles,\nrecording conditions, and background noise and music, and our model performs\nconsistently well compared to other models and real recordings. In particular,\nfor speech editing evaluation, we introduce a high quality, challenging, and\nrealistic dataset named RealEdit. We encourage readers to listen to the demos\nat https://jasonppy.github.io/VoiceCraft_web.\n","authors":["Puyuan Peng","Po-Yao Huang","Daniel Li","Abdelrahman Mohamed","David Harwath"],"pdf_url":"https://arxiv.org/pdf/2403.16973v1.pdf","comment":"Data, code, and model weights are available at\n  https://github.com/jasonppy/VoiceCraft"},{"id":"http://arxiv.org/abs/2403.16865v1","updated":"2024-03-25T15:28:38Z","published":"2024-03-25T15:28:38Z","title":"Encoding of lexical tone in self-supervised models of spoken language","summary":"  Interpretability research has shown that self-supervised Spoken Language\nModels (SLMs) encode a wide variety of features in human speech from the\nacoustic, phonetic, phonological, syntactic and semantic levels, to speaker\ncharacteristics. The bulk of prior research on representations of phonology has\nfocused on segmental features such as phonemes; the encoding of suprasegmental\nphonology (such as tone and stress patterns) in SLMs is not yet well\nunderstood. Tone is a suprasegmental feature that is present in more than half\nof the world's languages. This paper aims to analyze the tone encoding\ncapabilities of SLMs, using Mandarin and Vietnamese as case studies. We show\nthat SLMs encode lexical tone to a significant degree even when they are\ntrained on data from non-tonal languages. We further find that SLMs behave\nsimilarly to native and non-native human participants in tone and consonant\nperception studies, but they do not follow the same developmental trajectory.\n","authors":["Gaofei Shen","Michaela Watkins","Afra Alishahi","Arianna Bisazza","Grzegorz Chrupała"],"pdf_url":"https://arxiv.org/pdf/2403.16865v1.pdf","comment":"Accepted to NAACL 2024"}]},"2024-03-24T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.16331v1","updated":"2024-03-24T23:50:15Z","published":"2024-03-24T23:50:15Z","title":"Modeling Analog Dynamic Range Compressors using Deep Learning and\n  State-space Models","summary":"  We describe a novel approach for developing realistic digital models of\ndynamic range compressors for digital audio production by analyzing their\nanalog prototypes. While realistic digital dynamic compressors are potentially\nuseful for many applications, the design process is challenging because the\ncompressors operate nonlinearly over long time scales. Our approach is based on\nthe structured state space sequence model (S4), as implementing the state-space\nmodel (SSM) has proven to be efficient at learning long-range dependencies and\nis promising for modeling dynamic range compressors. We present in this paper a\ndeep learning model with S4 layers to model the Teletronix LA-2A analog dynamic\nrange compressor. The model is causal, executes efficiently in real time, and\nachieves roughly the same quality as previous deep-learning models but with\nfewer parameters.\n","authors":["Hanzhi Yin","Gang Cheng","Christian J. Steinmetz","Ruibin Yuan","Richard M. Stern","Roger B. Dannenberg"],"pdf_url":"https://arxiv.org/pdf/2403.16331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16078v1","updated":"2024-03-24T09:42:05Z","published":"2024-03-24T09:42:05Z","title":"Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover\n  Strategy","summary":"  Audio-visual target speech extraction (AV-TSE) is one of the enabling\ntechnologies in robotics and many audio-visual applications. One of the\nchallenges of AV-TSE is how to effectively utilize audio-visual synchronization\ninformation in the process. AV-HuBERT can be a useful pre-trained model for\nlip-reading, which has not been adopted by AV-TSE. In this paper, we would like\nto explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system.\nWe have good reasons to expect an improved performance. To benefit from the\ninter and intra-modality correlations, we also propose a novel Mask-And-Recover\n(MAR) strategy for self-supervised learning. The experimental results on the\nVoxCeleb2 dataset show that our proposed model outperforms the baselines both\nin terms of subjective and objective metrics, suggesting that the pre-trained\nAV-HuBERT model provides more informative visual cues for target speech\nextraction. Furthermore, through a comparative study, we confirm that the\nproposed Mask-And-Recover strategy is significantly effective.\n","authors":["Wenxuan Wu","Xueyuan Chen","Xixin Wu","Haizhou Li","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2403.16078v1.pdf","comment":"Accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.17376v1","updated":"2024-03-24T10:18:04Z","published":"2024-03-24T10:18:04Z","title":"Theoretical Analysis of Quality of Conventional Beamforming for Phased\n  Microphone Arrays","summary":"  A theoretical study is performed to analyze the directional response of\ndifferent types of microphone array designs. 1-D (linear) and 2-D (planar)\nmicrophone array types are considered, and the delay and sum beamforming and\nconventional beamforming techniques are employed to localize the sound source.\nA non-dimensional parameter, G, is characterized to simplify and standardize\nthe rejection performance of both 1-D and 2-D microphone arrays as a function\nof array geometry and sound source parameters. This parameter G is then used to\ndetermine an improved design of a 2-D microphone array for far-field sound\nlocalization. One such design, termed the Equi-area array is introduced and\nanalyzed in detail. The design is shown to have an advantageous rejection\nperformance compared to other conventionally used 2-D planar microphone arrays.\n","authors":["Dheepak Khatri","Kenneth Granlund"],"pdf_url":"https://arxiv.org/pdf/2403.17376v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.16331v1","updated":"2024-03-24T23:50:15Z","published":"2024-03-24T23:50:15Z","title":"Modeling Analog Dynamic Range Compressors using Deep Learning and\n  State-space Models","summary":"  We describe a novel approach for developing realistic digital models of\ndynamic range compressors for digital audio production by analyzing their\nanalog prototypes. While realistic digital dynamic compressors are potentially\nuseful for many applications, the design process is challenging because the\ncompressors operate nonlinearly over long time scales. Our approach is based on\nthe structured state space sequence model (S4), as implementing the state-space\nmodel (SSM) has proven to be efficient at learning long-range dependencies and\nis promising for modeling dynamic range compressors. We present in this paper a\ndeep learning model with S4 layers to model the Teletronix LA-2A analog dynamic\nrange compressor. The model is causal, executes efficiently in real time, and\nachieves roughly the same quality as previous deep-learning models but with\nfewer parameters.\n","authors":["Hanzhi Yin","Gang Cheng","Christian J. Steinmetz","Ruibin Yuan","Richard M. Stern","Roger B. Dannenberg"],"pdf_url":"https://arxiv.org/pdf/2403.16331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17376v1","updated":"2024-03-24T10:18:04Z","published":"2024-03-24T10:18:04Z","title":"Theoretical Analysis of Quality of Conventional Beamforming for Phased\n  Microphone Arrays","summary":"  A theoretical study is performed to analyze the directional response of\ndifferent types of microphone array designs. 1-D (linear) and 2-D (planar)\nmicrophone array types are considered, and the delay and sum beamforming and\nconventional beamforming techniques are employed to localize the sound source.\nA non-dimensional parameter, G, is characterized to simplify and standardize\nthe rejection performance of both 1-D and 2-D microphone arrays as a function\nof array geometry and sound source parameters. This parameter G is then used to\ndetermine an improved design of a 2-D microphone array for far-field sound\nlocalization. One such design, termed the Equi-area array is introduced and\nanalyzed in detail. The design is shown to have an advantageous rejection\nperformance compared to other conventionally used 2-D planar microphone arrays.\n","authors":["Dheepak Khatri","Kenneth Granlund"],"pdf_url":"https://arxiv.org/pdf/2403.17376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16078v1","updated":"2024-03-24T09:42:05Z","published":"2024-03-24T09:42:05Z","title":"Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover\n  Strategy","summary":"  Audio-visual target speech extraction (AV-TSE) is one of the enabling\ntechnologies in robotics and many audio-visual applications. One of the\nchallenges of AV-TSE is how to effectively utilize audio-visual synchronization\ninformation in the process. AV-HuBERT can be a useful pre-trained model for\nlip-reading, which has not been adopted by AV-TSE. In this paper, we would like\nto explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system.\nWe have good reasons to expect an improved performance. To benefit from the\ninter and intra-modality correlations, we also propose a novel Mask-And-Recover\n(MAR) strategy for self-supervised learning. The experimental results on the\nVoxCeleb2 dataset show that our proposed model outperforms the baselines both\nin terms of subjective and objective metrics, suggesting that the pre-trained\nAV-HuBERT model provides more informative visual cues for target speech\nextraction. Furthermore, through a comparative study, we confirm that the\nproposed Mask-And-Recover strategy is significantly effective.\n","authors":["Wenxuan Wu","Xueyuan Chen","Xixin Wu","Haizhou Li","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2403.16078v1.pdf","comment":"Accepted by IJCNN 2024"}]},"2024-03-23T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2309.09469v2","updated":"2024-03-23T04:41:23Z","published":"2023-09-18T04:03:05Z","title":"Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks","summary":"  Brain-inspired spiking neural networks (SNNs) have demonstrated great\npotential for temporal signal processing. However, their performance in speech\nprocessing remains limited due to the lack of an effective auditory front-end.\nTo address this limitation, we introduce Spiking-LEAF, a learnable auditory\nfront-end meticulously designed for SNN-based speech processing. Spiking-LEAF\ncombines a learnable filter bank with a novel two-compartment spiking neuron\nmodel called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure\nof inner hair cells (IHC) and they leverage segregated dendritic and somatic\ncompartments to effectively capture multi-scale temporal dynamics of speech\nsignals. Additionally, the IHC-LIF neurons incorporate the lateral feedback\nmechanism along with spike regularization loss to enhance spike encoding\nefficiency. On keyword spotting and speaker identification tasks, the proposed\nSpiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional\nreal-valued acoustic features in terms of classification accuracy, noise\nrobustness, and encoding efficiency.\n","authors":["Zeyang Song","Jibin Wu","Malu Zhang","Mike Zheng Shou","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2309.09469v2.pdf","comment":"Accepted by ICASSP2024"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.01130v2","updated":"2024-03-23T10:42:06Z","published":"2024-03-02T08:19:58Z","title":"Arbitrary Discrete Fourier Analysis and Its Application in Replayed\n  Speech Detection","summary":"  In this paper, a group of finite sequences and its variants were proposed to\nuse in conducting signal analysis; we called the developed signal analysis\nmethods arbitrary discrete Fourier analysis (ADFA), Mel-scale discrete Fourier\nanalysis (MDFA) and constant Q analysis (CQA). The effectiveness of three\nsignal analysis methods were then validated by testing their performance on a\nreplayed speech detection benchmark (i.e., the ASVspoof 2019 Physical Access)\nalong with a state-of-the-art model. Comparable performance to the best\nreported systems were shown by the experimental results with three signal\nanalysis methods. Furthermore, the CQA method shown its efficiency with less\ncomputation time in compared to the convention method constant Q transform\n(CQT), which is commonly used in spoofed and fake speech detection and music\nprocessing.\n","authors":["Shih-Kuang Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01130v2.pdf","comment":"https://github.com/shihkuanglee/ADFA"},{"id":"http://arxiv.org/abs/2309.09469v2","updated":"2024-03-23T04:41:23Z","published":"2023-09-18T04:03:05Z","title":"Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks","summary":"  Brain-inspired spiking neural networks (SNNs) have demonstrated great\npotential for temporal signal processing. However, their performance in speech\nprocessing remains limited due to the lack of an effective auditory front-end.\nTo address this limitation, we introduce Spiking-LEAF, a learnable auditory\nfront-end meticulously designed for SNN-based speech processing. Spiking-LEAF\ncombines a learnable filter bank with a novel two-compartment spiking neuron\nmodel called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure\nof inner hair cells (IHC) and they leverage segregated dendritic and somatic\ncompartments to effectively capture multi-scale temporal dynamics of speech\nsignals. Additionally, the IHC-LIF neurons incorporate the lateral feedback\nmechanism along with spike regularization loss to enhance spike encoding\nefficiency. On keyword spotting and speaker identification tasks, the proposed\nSpiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional\nreal-valued acoustic features in terms of classification accuracy, noise\nrobustness, and encoding efficiency.\n","authors":["Zeyang Song","Jibin Wu","Malu Zhang","Mike Zheng Shou","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2309.09469v2.pdf","comment":"Accepted by ICASSP2024"}]},"2024-03-26T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.16760v2","updated":"2024-03-26T15:17:51Z","published":"2024-03-25T13:39:33Z","title":"As Good As A Coin Toss: Human detection of AI-generated images, videos,\n  audio, and audiovisual stimuli","summary":"  As synthetic media becomes progressively more realistic and barriers to using\nit continue to lower, the technology has been increasingly utilized for\nmalicious purposes, from financial fraud to nonconsensual pornography. Today,\nthe principal defense against being misled by synthetic media relies on the\nability of the human observer to visually and auditorily discern between real\nand fake. However, it remains unclear just how vulnerable people actually are\nto deceptive synthetic media in the course of their day to day lives. We\nconducted a perceptual study with 1276 participants to assess how accurate\npeople were at distinguishing synthetic images, audio only, video only, and\naudiovisual stimuli from authentic. To reflect the circumstances under which\npeople would likely encounter synthetic media in the wild, testing conditions\nand stimuli emulated a typical online platform, while all synthetic media used\nin the survey was sourced from publicly accessible generative AI technology.\n  We find that overall, participants struggled to meaningfully discern between\nsynthetic and authentic content. We also find that detection performance\nworsens when the stimuli contains synthetic content as compared to authentic\ncontent, images featuring human faces as compared to non face objects, a single\nmodality as compared to multimodal stimuli, mixed authenticity as compared to\nbeing fully synthetic for audiovisual stimuli, and features foreign languages\nas compared to languages the observer is fluent in. Finally, we also find that\nprior knowledge of synthetic media does not meaningfully impact their detection\nperformance. Collectively, these results indicate that people are highly\nsusceptible to being tricked by synthetic media in their daily lives and that\nhuman perceptual detection capabilities can no longer be relied upon as an\neffective counterdefense.\n","authors":["Di Cooke","Abigail Edwards","Sophia Barkoff","Kathryn Kelly"],"pdf_url":"https://arxiv.org/pdf/2403.16760v2.pdf","comment":"For study pre-registration, see https://osf.io/fnhr3"},{"id":"http://arxiv.org/abs/2403.10518v2","updated":"2024-03-26T04:24:13Z","published":"2024-03-15T17:59:33Z","title":"Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation\n  Guided by the Characteristic Dance Primitives","summary":"  We propose Lodge, a network capable of generating extremely long dance\nsequences conditioned on given music. We design Lodge as a two-stage coarse to\nfine diffusion architecture, and propose the characteristic dance primitives\nthat possess significant expressiveness as intermediate representations between\ntwo diffusion models. The first stage is global diffusion, which focuses on\ncomprehending the coarse-level music-dance correlation and production\ncharacteristic dance primitives. In contrast, the second-stage is the local\ndiffusion, which parallelly generates detailed motion sequences under the\nguidance of the dance primitives and choreographic rules. In addition, we\npropose a Foot Refine Block to optimize the contact between the feet and the\nground, enhancing the physical realism of the motion. Our approach can\nparallelly generate dance sequences of extremely long length, striking a\nbalance between global choreographic patterns and local motion quality and\nexpressiveness. Extensive experiments validate the efficacy of our method.\n","authors":["Ronghui Li","YuXiang Zhang","Yachao Zhang","Hongwen Zhang","Jie Guo","Yan Zhang","Yebin Liu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.10518v2.pdf","comment":"Accepted by CVPR2024, Project page:\n  https://li-ronghui.github.io/lodge"},{"id":"http://arxiv.org/abs/2402.04866v2","updated":"2024-03-26T16:57:46Z","published":"2024-02-01T21:16:40Z","title":"Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones","summary":"  Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several important real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex-valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.\n","authors":["Francesca Ronchini","Luca Comanducci","Mirco Pezzoli","Fabio Antonacci","Augusto Sarti"],"pdf_url":"https://arxiv.org/pdf/2402.04866v2.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17864v1","updated":"2024-03-26T16:51:21Z","published":"2024-03-26T16:51:21Z","title":"Synthesizing Soundscapes: Leveraging Text-to-Audio Models for\n  Environmental Sound Classification","summary":"  In the past few years, text-to-audio models have emerged as a significant\nadvancement in automatic audio generation. Although they represent impressive\ntechnological progress, the effectiveness of their use in the development of\naudio applications remains uncertain. This paper aims to investigate these\naspects, specifically focusing on the task of classification of environmental\nsounds. This study analyzes the performance of two different environmental\nclassification systems when data generated from text-to-audio models is used\nfor training. Two cases are considered: a) when the training dataset is\naugmented by data coming from two different text-to-audio models; and b) when\nthe training dataset consists solely of synthetic audio generated. In both\ncases, the performance of the classification task is tested on real data.\nResults indicate that text-to-audio models are effective for dataset\naugmentation, whereas the performance of the models drops when relying on only\ngenerated audio.\n","authors":["Francesca Ronchini","Luca Comanducci","Fabio Antonacci"],"pdf_url":"https://arxiv.org/pdf/2403.17864v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2311.07703v2","updated":"2024-03-26T15:31:34Z","published":"2023-11-13T19:41:34Z","title":"Measuring Entrainment in Spontaneous Code-switched Speech","summary":"  It is well-known that speakers who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans, finding that (1)\npatterns of written and spoken entrainment in monolingual settings largely\ngeneralize to code-switched settings, and (2) some patterns of entrainment on\ncode-switching in dialogue agent-generated text generalize to spontaneous\ncode-switched speech. Our findings give rise to important implications for the\npotentially \"universal\" nature of entrainment as a communication phenomenon,\nand potential applications in inclusive and interactive speech technology.\n","authors":["Debasmita Bhattacharya","Siying Ding","Alayna Nguyen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2311.07703v2.pdf","comment":"Edits: camera-ready manuscript for NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17562v1","updated":"2024-03-26T10:10:56Z","published":"2024-03-26T10:10:56Z","title":"Deep functional multiple index models with an application to SER","summary":"  Speech Emotion Recognition (SER) plays a crucial role in advancing\nhuman-computer interaction and speech processing capabilities. We introduce a\nnovel deep-learning architecture designed specifically for the functional data\nmodel known as the multiple-index functional model. Our key innovation lies in\nintegrating adaptive basis layers and an automated data transformation search\nwithin the deep learning framework. Simulations for this new model show good\nperformances. This allows us to extract features tailored for chunk-level SER,\nbased on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the\neffectiveness of our approach on the benchmark IEMOCAP database, achieving good\nperformance compared to existing methods.\n","authors":["Matthieu Saumard","Abir El Haj","Thibault Napoleon"],"pdf_url":"https://arxiv.org/pdf/2403.17562v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.17529v1","updated":"2024-03-26T09:35:16Z","published":"2024-03-26T09:35:16Z","title":"Detection of Deepfake Environmental Audio","summary":"  With the ever-rising quality of deep generative models, it is increasingly\nimportant to be able to discern whether the audio data at hand have been\nrecorded or synthesized. Although the detection of fake speech signals has been\nstudied extensively, this is not the case for the detection of fake\nenvironmental audio.\n  We propose a simple and efficient pipeline for detecting fake environmental\nsounds based on the CLAP audio embedding. We evaluate this detector using audio\ndata from the 2023 DCASE challenge task on Foley sound synthesis.\n  Our experiments show that fake sounds generated by 44 state-of-the-art\nsynthesizers can be detected on average with 98% accuracy. We show that using\nan audio embedding learned on environmental audio is beneficial over a standard\nVGGish one as it provides a 10% increase in detection performance. Informal\nlistening to Incorrect Negative examples demonstrates audible features of fake\nsounds missed by the detector such as distortion and implausible background\nnoise.\n","authors":["Hafsa Ouajdi","Oussama Hadder","Modan Tailleur","Mathieu Lagrange","Laurie M. Heller"],"pdf_url":"https://arxiv.org/pdf/2403.17529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17514v1","updated":"2024-03-26T09:16:21Z","published":"2024-03-26T09:16:21Z","title":"Speaker Distance Estimation in Enclosures from Single-Channel Audio","summary":"  Distance estimation from audio plays a crucial role in various applications,\nsuch as acoustic scene analysis, sound source localization, and room modeling.\nMost studies predominantly center on employing a classification approach, where\ndistances are discretized into distinct categories, enabling smoother model\ntraining and achieving higher accuracy but imposing restrictions on the\nprecision of the obtained sound source position. Towards this direction, in\nthis paper we propose a novel approach for continuous distance estimation from\naudio signals using a convolutional recurrent neural network with an attention\nmodule. The attention mechanism enables the model to focus on relevant temporal\nand spectral features, enhancing its ability to capture fine-grained\ndistance-related information. To evaluate the effectiveness of our proposed\nmethod, we conduct extensive experiments using audio recordings in controlled\nenvironments with three levels of realism (synthetic room impulse response,\nmeasured response with convolved speech, and real recordings) on four datasets\n(our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23). Experimental\nresults show that the model achieves an absolute error of 0.11 meters in a\nnoiseless synthetic scenario. Moreover, the results showed an absolute error of\nabout 1.30 meters in the hybrid scenario. The algorithm's performance in the\nreal scenario, where unpredictable environmental factors and noise are\nprevalent, yields an absolute error of approximately 0.50 meters. For\nreproducible research purposes we make model, code, and synthetic datasets\navailable at https://github.com/michaelneri/audio-distance-estimation.\n","authors":["Michael Neri","Archontis Politis","Daniel Krause","Marco Carli","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2403.17514v1.pdf","comment":"Accepted for publication in IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing"},{"id":"http://arxiv.org/abs/2403.17508v1","updated":"2024-03-26T09:09:59Z","published":"2024-03-26T09:09:59Z","title":"Correlation of Fréchet Audio Distance With Human Perception of\n  Environmental Audio Is Embedding Dependant","summary":"  This paper explores whether considering alternative domain-specific\nembeddings to calculate the Fr\\'echet Audio Distance (FAD) metric can help the\nFAD to correlate better with perceptual ratings of environmental sounds. We\nused embeddings from VGGish, PANNs, MS-CLAP, L-CLAP, and MERT, which are\ntailored for either music or environmental sound evaluation. The FAD scores\nwere calculated for sounds from the DCASE 2023 Task 7 dataset. Using perceptual\ndata from the same task, we find that PANNs-WGM-LogMel produces the best\ncorrelation between FAD scores and perceptual ratings of both audio quality and\nperceived fit with a Spearman correlation higher than 0.5. We also find that\nmusic-specific embeddings resulted in significantly lower results.\nInterestingly, VGGish, the embedding used for the original Fr\\'echet\ncalculation, yielded a correlation below 0.1. These results underscore the\ncritical importance of the choice of embedding for the FAD metric design.\n","authors":["Modan Tailleur","Junwon Lee","Mathieu Lagrange","Keunwoo Choi","Laurie M. Heller","Keisuke Imoto","Yuki Okamoto"],"pdf_url":"https://arxiv.org/pdf/2403.17508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17420v1","updated":"2024-03-26T06:27:50Z","published":"2024-03-26T06:27:50Z","title":"Learning to Visually Localize Sound Sources from Mixtures without Prior\n  Source Knowledge","summary":"  The goal of the multi-sound source localization task is to localize sound\nsources from the mixture individually. While recent multi-sound source\nlocalization methods have shown improved performance, they face challenges due\nto their reliance on prior information about the number of objects to be\nseparated. In this paper, to overcome this limitation, we present a novel\nmulti-sound source localization method that can perform localization without\nprior knowledge of the number of sound sources. To achieve this goal, we\npropose an iterative object identification (IOI) module, which can recognize\nsound-making objects in an iterative manner. After finding the regions of\nsound-making objects, we devise object similarity-aware clustering (OSC) loss\nto guide the IOI module to effectively combine regions of the same object but\nalso distinguish between different objects and backgrounds. It enables our\nmethod to perform accurate localization of sound-making objects without any\nprior knowledge. Extensive experimental results on the MUSIC and VGGSound\nbenchmarks show the significant performance improvements of the proposed method\nover the existing methods for both single and multi-source. Our code is\navailable at: https://github.com/VisualAIKHU/NoPrior_MultiSSL\n","authors":["Dongjin Kim","Sung Jin Um","Sangmin Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17420v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17402v1","updated":"2024-03-26T05:41:39Z","published":"2024-03-26T05:41:39Z","title":"Infrastructure-less Localization from Indoor Environmental Sounds Based\n  on Spectral Decomposition and Spatial Likelihood Model","summary":"  Human and/or asset tracking using an attached sensor units helps understand\ntheir activities. Most common indoor localization methods for human tracking\ntechnologies require expensive infrastructures, deployment and maintenance. To\novercome this problem, environmental sounds have been used for\ninfrastructure-free localization. While they achieve room-level classification,\nthey suffer from two problems: low signal-to-noise-ratio (SNR) condition and\nnon-uniqueness of sound over the coverage area. A microphone localization\nmethod was proposed using supervised spectral decomposition and spatial\nlikelihood to solve these problems. The proposed method was evaluated with\nactual recordings in an experimental room with a size of 12 x 30 m. The results\nshowed that the proposed method with supervised NMF was robust under low-SNR\ncondition compared to a simple feature (mel frequency cepstrum coefficient:\nMFCC). Additionally, the proposed method could be easily integrated with prior\ndistribution, which is available from other Bayesian localizations. The\nproposed method can be used to evaluate the spatial likelihood from\nenvironmental sounds.\n","authors":["Satoki Ogiso","Yoshiaki Bando","Takeshi Kurata","Takashi Okuma"],"pdf_url":"https://arxiv.org/pdf/2403.17402v1.pdf","comment":"6 pages, 6 figures, accepted to IEEE/SICE SII 2023"},{"id":"http://arxiv.org/abs/2403.17378v1","updated":"2024-03-26T04:53:15Z","published":"2024-03-26T04:53:15Z","title":"Low-Latency Neural Speech Phase Prediction based on Parallel Estimation\n  Architecture and Anti-Wrapping Losses for Speech Generation Tasks","summary":"  This paper presents a novel neural speech phase prediction model which\npredicts wrapped phase spectra directly from amplitude spectra. The proposed\nmodel is a cascade of a residual convolutional network and a parallel\nestimation architecture. The parallel estimation architecture is a core module\nfor direct wrapped phase prediction. This architecture consists of two parallel\nlinear convolutional layers and a phase calculation formula, imitating the\nprocess of calculating the phase spectra from the real and imaginary parts of\ncomplex spectra and strictly restricting the predicted phase values to the\nprincipal value interval. To avoid the error expansion issue caused by phase\nwrapping, we design anti-wrapping training losses defined between the predicted\nwrapped phase spectra and natural ones by activating the instantaneous phase\nerror, group delay error and instantaneous angular frequency error using an\nanti-wrapping function. We mathematically demonstrate that the anti-wrapping\nfunction should possess three properties, namely parity, periodicity and\nmonotonicity. We also achieve low-latency streamable phase prediction by\ncombining causal convolutions and knowledge distillation training strategies.\nFor both analysis-synthesis and specific speech generation tasks, experimental\nresults show that our proposed neural speech phase prediction model outperforms\nthe iterative phase estimation algorithms and neural network-based phase\nprediction methods in terms of phase prediction precision, efficiency and\nrobustness. Compared with HiFi-GAN-based waveform reconstruction method, our\nproposed model also shows outstanding efficiency advantages while ensuring the\nquality of synthesized speech. To the best of our knowledge, we are the first\nto directly predict speech phase spectra from amplitude spectra only via neural\nnetworks.\n","authors":["Yang Ai","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2403.17378v1.pdf","comment":"Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing. arXiv admin note: substantial text overlap with arXiv:2211.15974"},{"id":"http://arxiv.org/abs/2403.17327v1","updated":"2024-03-26T02:21:36Z","published":"2024-03-26T02:21:36Z","title":"Accuracy enhancement method for speech emotion recognition from\n  spectrogram using temporal frequency correlation and positional information\n  learning through knowledge transfer","summary":"  In this paper, we propose a method to improve the accuracy of speech emotion\nrecognition (SER) by using vision transformer (ViT) to attend to the\ncorrelation of frequency (y-axis) with time (x-axis) in spectrogram and\ntransferring positional information between ViT through knowledge transfer. The\nproposed method has the following originality i) We use vertically segmented\npatches of log-Mel spectrogram to analyze the correlation of frequencies over\ntime. This type of patch allows us to correlate the most relevant frequencies\nfor a particular emotion with the time they were uttered. ii) We propose the\nuse of image coordinate encoding, an absolute positional encoding suitable for\nViT. By normalizing the x, y coordinates of the image to -1 to 1 and\nconcatenating them to the image, we can effectively provide valid absolute\npositional information for ViT. iii) Through feature map matching, the locality\nand location information of the teacher network is effectively transmitted to\nthe student network. Teacher network is a ViT that contains locality of\nconvolutional stem and absolute position information through image coordinate\nencoding, and student network is a structure that lacks positional encoding in\nthe basic ViT structure. In feature map matching stage, we train through the\nmean absolute error (L1 loss) to minimize the difference between the feature\nmaps of the two networks. To validate the proposed method, three emotion\ndatasets (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into\nlog-Mel spectrograms for comparison experiments. The experimental results show\nthat the proposed method significantly outperforms the state-of-the-art methods\nin terms of weighted accuracy while requiring significantly fewer floating\npoint operations (FLOPs). Overall, the proposed method offers an promising\nsolution for SER by providing improved efficiency and performance.\n","authors":["Jeong-Yoon Kim","Seung-Ho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.17327v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2403.14438v2","updated":"2024-03-26T11:02:32Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wagner","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.10518v2","updated":"2024-03-26T04:24:13Z","published":"2024-03-15T17:59:33Z","title":"Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation\n  Guided by the Characteristic Dance Primitives","summary":"  We propose Lodge, a network capable of generating extremely long dance\nsequences conditioned on given music. We design Lodge as a two-stage coarse to\nfine diffusion architecture, and propose the characteristic dance primitives\nthat possess significant expressiveness as intermediate representations between\ntwo diffusion models. The first stage is global diffusion, which focuses on\ncomprehending the coarse-level music-dance correlation and production\ncharacteristic dance primitives. In contrast, the second-stage is the local\ndiffusion, which parallelly generates detailed motion sequences under the\nguidance of the dance primitives and choreographic rules. In addition, we\npropose a Foot Refine Block to optimize the contact between the feet and the\nground, enhancing the physical realism of the motion. Our approach can\nparallelly generate dance sequences of extremely long length, striking a\nbalance between global choreographic patterns and local motion quality and\nexpressiveness. Extensive experiments validate the efficacy of our method.\n","authors":["Ronghui Li","YuXiang Zhang","Yachao Zhang","Hongwen Zhang","Jie Guo","Yan Zhang","Yebin Liu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.10518v2.pdf","comment":"Accepted by CVPR2024, Project page:\n  https://li-ronghui.github.io/lodge"},{"id":"http://arxiv.org/abs/2402.04866v2","updated":"2024-03-26T16:57:46Z","published":"2024-02-01T21:16:40Z","title":"Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones","summary":"  Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several important real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex-valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.\n","authors":["Francesca Ronchini","Luca Comanducci","Mirco Pezzoli","Fabio Antonacci","Augusto Sarti"],"pdf_url":"https://arxiv.org/pdf/2402.04866v2.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17864v1","updated":"2024-03-26T16:51:21Z","published":"2024-03-26T16:51:21Z","title":"Synthesizing Soundscapes: Leveraging Text-to-Audio Models for\n  Environmental Sound Classification","summary":"  In the past few years, text-to-audio models have emerged as a significant\nadvancement in automatic audio generation. Although they represent impressive\ntechnological progress, the effectiveness of their use in the development of\naudio applications remains uncertain. This paper aims to investigate these\naspects, specifically focusing on the task of classification of environmental\nsounds. This study analyzes the performance of two different environmental\nclassification systems when data generated from text-to-audio models is used\nfor training. Two cases are considered: a) when the training dataset is\naugmented by data coming from two different text-to-audio models; and b) when\nthe training dataset consists solely of synthetic audio generated. In both\ncases, the performance of the classification task is tested on real data.\nResults indicate that text-to-audio models are effective for dataset\naugmentation, whereas the performance of the models drops when relying on only\ngenerated audio.\n","authors":["Francesca Ronchini","Luca Comanducci","Fabio Antonacci"],"pdf_url":"https://arxiv.org/pdf/2403.17864v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2311.07703v2","updated":"2024-03-26T15:31:34Z","published":"2023-11-13T19:41:34Z","title":"Measuring Entrainment in Spontaneous Code-switched Speech","summary":"  It is well-known that speakers who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans, finding that (1)\npatterns of written and spoken entrainment in monolingual settings largely\ngeneralize to code-switched settings, and (2) some patterns of entrainment on\ncode-switching in dialogue agent-generated text generalize to spontaneous\ncode-switched speech. Our findings give rise to important implications for the\npotentially \"universal\" nature of entrainment as a communication phenomenon,\nand potential applications in inclusive and interactive speech technology.\n","authors":["Debasmita Bhattacharya","Siying Ding","Alayna Nguyen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2311.07703v2.pdf","comment":"Edits: camera-ready manuscript for NAACL 2024"},{"id":"http://arxiv.org/abs/2403.16760v2","updated":"2024-03-26T15:17:51Z","published":"2024-03-25T13:39:33Z","title":"As Good As A Coin Toss: Human detection of AI-generated images, videos,\n  audio, and audiovisual stimuli","summary":"  As synthetic media becomes progressively more realistic and barriers to using\nit continue to lower, the technology has been increasingly utilized for\nmalicious purposes, from financial fraud to nonconsensual pornography. Today,\nthe principal defense against being misled by synthetic media relies on the\nability of the human observer to visually and auditorily discern between real\nand fake. However, it remains unclear just how vulnerable people actually are\nto deceptive synthetic media in the course of their day to day lives. We\nconducted a perceptual study with 1276 participants to assess how accurate\npeople were at distinguishing synthetic images, audio only, video only, and\naudiovisual stimuli from authentic. To reflect the circumstances under which\npeople would likely encounter synthetic media in the wild, testing conditions\nand stimuli emulated a typical online platform, while all synthetic media used\nin the survey was sourced from publicly accessible generative AI technology.\n  We find that overall, participants struggled to meaningfully discern between\nsynthetic and authentic content. We also find that detection performance\nworsens when the stimuli contains synthetic content as compared to authentic\ncontent, images featuring human faces as compared to non face objects, a single\nmodality as compared to multimodal stimuli, mixed authenticity as compared to\nbeing fully synthetic for audiovisual stimuli, and features foreign languages\nas compared to languages the observer is fluent in. Finally, we also find that\nprior knowledge of synthetic media does not meaningfully impact their detection\nperformance. Collectively, these results indicate that people are highly\nsusceptible to being tricked by synthetic media in their daily lives and that\nhuman perceptual detection capabilities can no longer be relied upon as an\neffective counterdefense.\n","authors":["Di Cooke","Abigail Edwards","Sophia Barkoff","Kathryn Kelly"],"pdf_url":"https://arxiv.org/pdf/2403.16760v2.pdf","comment":"For study pre-registration, see https://osf.io/fnhr3"},{"id":"http://arxiv.org/abs/2312.02512v2","updated":"2024-03-26T13:21:28Z","published":"2023-12-05T05:36:44Z","title":"AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation","summary":"  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.\n","authors":["Jeongsoo Choi","Se Jin Park","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2312.02512v2.pdf","comment":"CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av"},{"id":"http://arxiv.org/abs/2403.17562v1","updated":"2024-03-26T10:10:56Z","published":"2024-03-26T10:10:56Z","title":"Deep functional multiple index models with an application to SER","summary":"  Speech Emotion Recognition (SER) plays a crucial role in advancing\nhuman-computer interaction and speech processing capabilities. We introduce a\nnovel deep-learning architecture designed specifically for the functional data\nmodel known as the multiple-index functional model. Our key innovation lies in\nintegrating adaptive basis layers and an automated data transformation search\nwithin the deep learning framework. Simulations for this new model show good\nperformances. This allows us to extract features tailored for chunk-level SER,\nbased on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the\neffectiveness of our approach on the benchmark IEMOCAP database, achieving good\nperformance compared to existing methods.\n","authors":["Matthieu Saumard","Abir El Haj","Thibault Napoleon"],"pdf_url":"https://arxiv.org/pdf/2403.17562v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.17529v1","updated":"2024-03-26T09:35:16Z","published":"2024-03-26T09:35:16Z","title":"Detection of Deepfake Environmental Audio","summary":"  With the ever-rising quality of deep generative models, it is increasingly\nimportant to be able to discern whether the audio data at hand have been\nrecorded or synthesized. Although the detection of fake speech signals has been\nstudied extensively, this is not the case for the detection of fake\nenvironmental audio.\n  We propose a simple and efficient pipeline for detecting fake environmental\nsounds based on the CLAP audio embedding. We evaluate this detector using audio\ndata from the 2023 DCASE challenge task on Foley sound synthesis.\n  Our experiments show that fake sounds generated by 44 state-of-the-art\nsynthesizers can be detected on average with 98% accuracy. We show that using\nan audio embedding learned on environmental audio is beneficial over a standard\nVGGish one as it provides a 10% increase in detection performance. Informal\nlistening to Incorrect Negative examples demonstrates audible features of fake\nsounds missed by the detector such as distortion and implausible background\nnoise.\n","authors":["Hafsa Ouajdi","Oussama Hadder","Modan Tailleur","Mathieu Lagrange","Laurie M. Heller"],"pdf_url":"https://arxiv.org/pdf/2403.17529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17514v1","updated":"2024-03-26T09:16:21Z","published":"2024-03-26T09:16:21Z","title":"Speaker Distance Estimation in Enclosures from Single-Channel Audio","summary":"  Distance estimation from audio plays a crucial role in various applications,\nsuch as acoustic scene analysis, sound source localization, and room modeling.\nMost studies predominantly center on employing a classification approach, where\ndistances are discretized into distinct categories, enabling smoother model\ntraining and achieving higher accuracy but imposing restrictions on the\nprecision of the obtained sound source position. Towards this direction, in\nthis paper we propose a novel approach for continuous distance estimation from\naudio signals using a convolutional recurrent neural network with an attention\nmodule. The attention mechanism enables the model to focus on relevant temporal\nand spectral features, enhancing its ability to capture fine-grained\ndistance-related information. To evaluate the effectiveness of our proposed\nmethod, we conduct extensive experiments using audio recordings in controlled\nenvironments with three levels of realism (synthetic room impulse response,\nmeasured response with convolved speech, and real recordings) on four datasets\n(our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23). Experimental\nresults show that the model achieves an absolute error of 0.11 meters in a\nnoiseless synthetic scenario. Moreover, the results showed an absolute error of\nabout 1.30 meters in the hybrid scenario. The algorithm's performance in the\nreal scenario, where unpredictable environmental factors and noise are\nprevalent, yields an absolute error of approximately 0.50 meters. For\nreproducible research purposes we make model, code, and synthetic datasets\navailable at https://github.com/michaelneri/audio-distance-estimation.\n","authors":["Michael Neri","Archontis Politis","Daniel Krause","Marco Carli","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2403.17514v1.pdf","comment":"Accepted for publication in IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing"},{"id":"http://arxiv.org/abs/2403.17508v1","updated":"2024-03-26T09:09:59Z","published":"2024-03-26T09:09:59Z","title":"Correlation of Fréchet Audio Distance With Human Perception of\n  Environmental Audio Is Embedding Dependant","summary":"  This paper explores whether considering alternative domain-specific\nembeddings to calculate the Fr\\'echet Audio Distance (FAD) metric can help the\nFAD to correlate better with perceptual ratings of environmental sounds. We\nused embeddings from VGGish, PANNs, MS-CLAP, L-CLAP, and MERT, which are\ntailored for either music or environmental sound evaluation. The FAD scores\nwere calculated for sounds from the DCASE 2023 Task 7 dataset. Using perceptual\ndata from the same task, we find that PANNs-WGM-LogMel produces the best\ncorrelation between FAD scores and perceptual ratings of both audio quality and\nperceived fit with a Spearman correlation higher than 0.5. We also find that\nmusic-specific embeddings resulted in significantly lower results.\nInterestingly, VGGish, the embedding used for the original Fr\\'echet\ncalculation, yielded a correlation below 0.1. These results underscore the\ncritical importance of the choice of embedding for the FAD metric design.\n","authors":["Modan Tailleur","Junwon Lee","Mathieu Lagrange","Keunwoo Choi","Laurie M. Heller","Keisuke Imoto","Yuki Okamoto"],"pdf_url":"https://arxiv.org/pdf/2403.17508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17420v1","updated":"2024-03-26T06:27:50Z","published":"2024-03-26T06:27:50Z","title":"Learning to Visually Localize Sound Sources from Mixtures without Prior\n  Source Knowledge","summary":"  The goal of the multi-sound source localization task is to localize sound\nsources from the mixture individually. While recent multi-sound source\nlocalization methods have shown improved performance, they face challenges due\nto their reliance on prior information about the number of objects to be\nseparated. In this paper, to overcome this limitation, we present a novel\nmulti-sound source localization method that can perform localization without\nprior knowledge of the number of sound sources. To achieve this goal, we\npropose an iterative object identification (IOI) module, which can recognize\nsound-making objects in an iterative manner. After finding the regions of\nsound-making objects, we devise object similarity-aware clustering (OSC) loss\nto guide the IOI module to effectively combine regions of the same object but\nalso distinguish between different objects and backgrounds. It enables our\nmethod to perform accurate localization of sound-making objects without any\nprior knowledge. Extensive experimental results on the MUSIC and VGGSound\nbenchmarks show the significant performance improvements of the proposed method\nover the existing methods for both single and multi-source. Our code is\navailable at: https://github.com/VisualAIKHU/NoPrior_MultiSSL\n","authors":["Dongjin Kim","Sung Jin Um","Sangmin Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17420v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17402v1","updated":"2024-03-26T05:41:39Z","published":"2024-03-26T05:41:39Z","title":"Infrastructure-less Localization from Indoor Environmental Sounds Based\n  on Spectral Decomposition and Spatial Likelihood Model","summary":"  Human and/or asset tracking using an attached sensor units helps understand\ntheir activities. Most common indoor localization methods for human tracking\ntechnologies require expensive infrastructures, deployment and maintenance. To\novercome this problem, environmental sounds have been used for\ninfrastructure-free localization. While they achieve room-level classification,\nthey suffer from two problems: low signal-to-noise-ratio (SNR) condition and\nnon-uniqueness of sound over the coverage area. A microphone localization\nmethod was proposed using supervised spectral decomposition and spatial\nlikelihood to solve these problems. The proposed method was evaluated with\nactual recordings in an experimental room with a size of 12 x 30 m. The results\nshowed that the proposed method with supervised NMF was robust under low-SNR\ncondition compared to a simple feature (mel frequency cepstrum coefficient:\nMFCC). Additionally, the proposed method could be easily integrated with prior\ndistribution, which is available from other Bayesian localizations. The\nproposed method can be used to evaluate the spatial likelihood from\nenvironmental sounds.\n","authors":["Satoki Ogiso","Yoshiaki Bando","Takeshi Kurata","Takashi Okuma"],"pdf_url":"https://arxiv.org/pdf/2403.17402v1.pdf","comment":"6 pages, 6 figures, accepted to IEEE/SICE SII 2023"},{"id":"http://arxiv.org/abs/2403.17378v1","updated":"2024-03-26T04:53:15Z","published":"2024-03-26T04:53:15Z","title":"Low-Latency Neural Speech Phase Prediction based on Parallel Estimation\n  Architecture and Anti-Wrapping Losses for Speech Generation Tasks","summary":"  This paper presents a novel neural speech phase prediction model which\npredicts wrapped phase spectra directly from amplitude spectra. The proposed\nmodel is a cascade of a residual convolutional network and a parallel\nestimation architecture. The parallel estimation architecture is a core module\nfor direct wrapped phase prediction. This architecture consists of two parallel\nlinear convolutional layers and a phase calculation formula, imitating the\nprocess of calculating the phase spectra from the real and imaginary parts of\ncomplex spectra and strictly restricting the predicted phase values to the\nprincipal value interval. To avoid the error expansion issue caused by phase\nwrapping, we design anti-wrapping training losses defined between the predicted\nwrapped phase spectra and natural ones by activating the instantaneous phase\nerror, group delay error and instantaneous angular frequency error using an\nanti-wrapping function. We mathematically demonstrate that the anti-wrapping\nfunction should possess three properties, namely parity, periodicity and\nmonotonicity. We also achieve low-latency streamable phase prediction by\ncombining causal convolutions and knowledge distillation training strategies.\nFor both analysis-synthesis and specific speech generation tasks, experimental\nresults show that our proposed neural speech phase prediction model outperforms\nthe iterative phase estimation algorithms and neural network-based phase\nprediction methods in terms of phase prediction precision, efficiency and\nrobustness. Compared with HiFi-GAN-based waveform reconstruction method, our\nproposed model also shows outstanding efficiency advantages while ensuring the\nquality of synthesized speech. To the best of our knowledge, we are the first\nto directly predict speech phase spectra from amplitude spectra only via neural\nnetworks.\n","authors":["Yang Ai","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2403.17378v1.pdf","comment":"Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing. arXiv admin note: substantial text overlap with arXiv:2211.15974"},{"id":"http://arxiv.org/abs/2403.17327v1","updated":"2024-03-26T02:21:36Z","published":"2024-03-26T02:21:36Z","title":"Accuracy enhancement method for speech emotion recognition from\n  spectrogram using temporal frequency correlation and positional information\n  learning through knowledge transfer","summary":"  In this paper, we propose a method to improve the accuracy of speech emotion\nrecognition (SER) by using vision transformer (ViT) to attend to the\ncorrelation of frequency (y-axis) with time (x-axis) in spectrogram and\ntransferring positional information between ViT through knowledge transfer. The\nproposed method has the following originality i) We use vertically segmented\npatches of log-Mel spectrogram to analyze the correlation of frequencies over\ntime. This type of patch allows us to correlate the most relevant frequencies\nfor a particular emotion with the time they were uttered. ii) We propose the\nuse of image coordinate encoding, an absolute positional encoding suitable for\nViT. By normalizing the x, y coordinates of the image to -1 to 1 and\nconcatenating them to the image, we can effectively provide valid absolute\npositional information for ViT. iii) Through feature map matching, the locality\nand location information of the teacher network is effectively transmitted to\nthe student network. Teacher network is a ViT that contains locality of\nconvolutional stem and absolute position information through image coordinate\nencoding, and student network is a structure that lacks positional encoding in\nthe basic ViT structure. In feature map matching stage, we train through the\nmean absolute error (L1 loss) to minimize the difference between the feature\nmaps of the two networks. To validate the proposed method, three emotion\ndatasets (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into\nlog-Mel spectrograms for comparison experiments. The experimental results show\nthat the proposed method significantly outperforms the state-of-the-art methods\nin terms of weighted accuracy while requiring significantly fewer floating\npoint operations (FLOPs). Overall, the proposed method offers an promising\nsolution for SER by providing improved efficiency and performance.\n","authors":["Jeong-Yoon Kim","Seung-Ho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.17327v1.pdf","comment":null}]}}